{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311624b8",
   "metadata": {},
   "source": [
    "# üöÄ XGBoost with 10-Fold CV - Fertilizer Prediction\n",
    "\n",
    "> **Objective**: Build a robust XGBoost model with 10-fold cross-validation to predict optimal fertilizers for agricultural conditions.\n",
    "> \n",
    "> **Target Variable**: `Fertilizer Name` (multi-class classification)\n",
    "> \n",
    "> **Strategy**: Complete pipeline from data loading to ensemble prediction with advanced feature engineering\n",
    "> \n",
    "> **Main Metric**: MAP@3 (Mean Average Precision at 3) - Kaggle competition requirement\n",
    "> \n",
    "> **‚ú® KEY FEATURES**:\n",
    "> - üìä **Complete pipeline**: Data loading, EDA, feature engineering, encoding\n",
    "> - üî¨ **Advanced feature engineering**: NPK ratios, environmental indices, categorical binning\n",
    "> - üîç **Stratified 10-Fold CV**: Maintains class distribution across folds\n",
    "> - ‚öñÔ∏è **Class weight balancing**: Handles imbalanced fertilizer classes\n",
    "> - üéØ **Ensemble prediction**: Combines 10 models for robust results\n",
    "> - ‚úÇÔ∏è **Early stopping**: Prevents overfitting with validation monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Competition Overview\n",
    "\n",
    "**Kaggle Playground Series S5E6: Fertilizer Prediction Challenge**\n",
    "\n",
    "**Problem**: Select the best fertilizer for different weather, soil conditions, and crops\n",
    "**Type**: Multi-class classification\n",
    "**Evaluation**: MAP@3 (Mean Average Precision @ 3)\n",
    "\n",
    "**Dataset Features**:\n",
    "- üå°Ô∏è Environmental: Temperature, Humidity, Soil Moisture\n",
    "- üß™ Chemical: Nitrogen, Phosphorus, Potassium levels\n",
    "- üå± Agricultural: Soil Type, Crop Type\n",
    "- üéØ Target: Fertilizer Name (22 different fertilizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb332602",
   "metadata": {},
   "source": [
    "## üìö 1. Import Libraries\n",
    "\n",
    "**Essential libraries for data manipulation, machine learning, and visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4c06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning - Scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import EarlyStopping\n",
    "\n",
    "# Model persistence and metadata\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(513)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb56f28",
   "metadata": {},
   "source": [
    "## üìè MAP@k Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "463195f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapk(actual, predicted, k=3):\n",
    "    \"\"\"Compute mean average precision at k (MAP@k).\"\"\"\n",
    "    def apk(a, p, k):\n",
    "        score = 0.0\n",
    "        for i in range(min(k, len(p))):\n",
    "            if p[i] == a:\n",
    "                score += 1.0 / (i + 1)\n",
    "                break  # only the first correct prediction counts\n",
    "        return score\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee416ea2",
   "metadata": {},
   "source": [
    "## üìÇ 2. Data Loading\n",
    "\n",
    "**Load datasets and prepare for modeling (EDA already completed in separate notebook)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b348a655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading datasets...\n",
      "‚úÖ Data loaded successfully:\n",
      "  ‚Ä¢ Training set: (750000, 10)\n",
      "  ‚Ä¢ Test set: (250000, 9)\n",
      "  ‚Ä¢ Sample submission: (250000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "data_path = '../data'\n",
    "train_path = os.path.join(data_path, 'train.csv')\n",
    "test_path = os.path.join(data_path, 'test.csv')\n",
    "sample_submission_path = os.path.join(data_path, 'sample_submission.csv')\n",
    "\n",
    "# Load datasets\n",
    "print(\"üìÇ Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully:\")\n",
    "print(f\"  ‚Ä¢ Training set: {train_df.shape}\")\n",
    "print(f\"  ‚Ä¢ Test set: {test_df.shape}\")\n",
    "print(f\"  ‚Ä¢ Sample submission: {sample_submission.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a7dcac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data separation completed:\n",
      "  ‚Ä¢ Training features: (750000, 9)\n",
      "  ‚Ä¢ Training target: (750000,)\n",
      "  ‚Ä¢ Test features: (250000, 9)\n",
      "  ‚Ä¢ Target classes: 7\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target variable\n",
    "target_column = 'Fertilizer Name'\n",
    "\n",
    "# Split training data\n",
    "X_raw = train_df.drop(columns=[target_column])\n",
    "y_raw = train_df[target_column]\n",
    "X_test_raw = test_df.copy()\n",
    "\n",
    "print(\"‚úÖ Data separation completed:\")\n",
    "print(f\"  ‚Ä¢ Training features: {X_raw.shape}\")\n",
    "print(f\"  ‚Ä¢ Training target: {y_raw.shape}\")\n",
    "print(f\"  ‚Ä¢ Test features: {X_test_raw.shape}\")\n",
    "print(f\"  ‚Ä¢ Target classes: {y_raw.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aee804",
   "metadata": {},
   "source": [
    "## üî¨ 3. Feature Engineering\n",
    "\n",
    "**Apply advanced feature engineering based on agricultural domain knowledge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c3d8682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying feature engineering...\n",
      "‚úÖ Feature engineering completed:\n",
      "  ‚Ä¢ Original features: 9\n",
      "  ‚Ä¢ After feature engineering: 25\n",
      "  ‚Ä¢ New features added: 16\n",
      "\n",
      "üÜï New engineered features (16):\n",
      "   1. N_P_ratio\n",
      "   2. N_K_ratio\n",
      "   3. P_K_ratio\n",
      "   4. Total_NPK\n",
      "   5. NPK_Balance\n",
      "   6. Temp_Hum_index\n",
      "   7. Moist_Balance\n",
      "   8. Environ_Stress\n",
      "   9. Temp_Moist_inter\n",
      "  10. Dominant_NPK\n",
      "  11. Temp_Cat\n",
      "  12. Hum_Cat\n",
      "  13. N_Level\n",
      "  14. K_Level\n",
      "  15. P_Level\n",
      "  16. Soil_Crop_Combo\n"
     ]
    }
   ],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create engineered features based on agricultural domain knowledge\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with agricultural features\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with additional engineered features\n",
    "    \"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # NPK Ratios (crucial for agricultural decisions)\n",
    "    df_eng['N_P_ratio'] = df_eng['Nitrogen'] / (df_eng['Phosphorous'] + 0.001)\n",
    "    df_eng['N_K_ratio'] = df_eng['Nitrogen'] / (df_eng['Potassium'] + 0.001)\n",
    "    df_eng['P_K_ratio'] = df_eng['Phosphorous'] / (df_eng['Potassium'] + 0.001)\n",
    "    \n",
    "    # Total NPK and NPK Balance\n",
    "    df_eng['Total_NPK'] = df_eng['Nitrogen'] + df_eng['Phosphorous'] + df_eng['Potassium']\n",
    "    npk_mean = df_eng[['Nitrogen', 'Phosphorous', 'Potassium']].mean(axis=1)\n",
    "    df_eng['NPK_Balance'] = df_eng[['Nitrogen', 'Phosphorous', 'Potassium']].std(axis=1) / (npk_mean + 0.001)\n",
    "    \n",
    "    # Environmental indices\n",
    "    df_eng['Temp_Hum_index'] = df_eng['Temparature'] * df_eng['Humidity'] / 100\n",
    "    df_eng['Moist_Balance'] = df_eng['Moisture'] - df_eng['Humidity']\n",
    "    df_eng['Environ_Stress'] = np.sqrt((df_eng['Temparature'] - 25)**2 + (df_eng['Humidity'] - 65)**2)\n",
    "    df_eng['Temp_Moist_inter'] = df_eng['Temparature'] * df_eng['Moisture'] / 100\n",
    "    \n",
    "    # Dominant nutrient\n",
    "    npk_cols = ['Nitrogen', 'Phosphorous', 'Potassium']\n",
    "    df_eng['Dominant_NPK'] = df_eng[npk_cols].idxmax(axis=1)\n",
    "    \n",
    "    # Categorical binning\n",
    "    df_eng['Temp_Cat'] = pd.cut(df_eng['Temparature'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    df_eng['Hum_Cat'] = pd.cut(df_eng['Humidity'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    df_eng['N_Level'] = pd.cut(df_eng['Nitrogen'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    df_eng['K_Level'] = pd.cut(df_eng['Potassium'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    df_eng['P_Level'] = pd.cut(df_eng['Phosphorous'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    # Soil-Crop interaction\n",
    "    df_eng['Soil_Crop_Combo'] = df_eng['Soil Type'].astype(str) + '_' + df_eng['Crop Type'].astype(str)\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"üîß Applying feature engineering...\")\n",
    "X_train_featured = create_features(X_raw)\n",
    "X_test_featured = create_features(X_test_raw)\n",
    "\n",
    "print(f\"‚úÖ Feature engineering completed:\")\n",
    "print(f\"  ‚Ä¢ Original features: {X_raw.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ After feature engineering: {X_train_featured.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ New features added: {X_train_featured.shape[1] - X_raw.shape[1]}\")\n",
    "\n",
    "# Display new feature names\n",
    "original_features = set(X_raw.columns)\n",
    "new_features = [col for col in X_train_featured.columns if col not in original_features]\n",
    "print(f\"\\nüÜï New engineered features ({len(new_features)}):\")\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fdd129",
   "metadata": {},
   "source": [
    "## üî¢ 4. Label Encoding\n",
    "\n",
    "**Encode categorical variables for machine learning compatibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a39158f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¢ Encoding categorical features...\n",
      "Categorical columns found: ['Soil Type', 'Crop Type', 'Dominant_NPK', 'Temp_Cat', 'Hum_Cat', 'N_Level', 'K_Level', 'P_Level', 'Soil_Crop_Combo']\n",
      "  ‚Ä¢ Encoding: Soil Type\n",
      "    - Classes: 5 | ['Black', 'Clayey', 'Loamy', 'Red', 'Sandy']\n",
      "  ‚Ä¢ Encoding: Crop Type\n",
      "    - Classes: 11 | ['Barley', 'Cotton', 'Ground Nuts', 'Maize', 'Millets']...\n",
      "  ‚Ä¢ Encoding: Dominant_NPK\n",
      "    - Classes: 3 | ['Nitrogen', 'Phosphorous', 'Potassium']\n",
      "  ‚Ä¢ Encoding: Temp_Cat\n",
      "    - Classes: 3 | ['High', 'Low', 'Medium']\n",
      "  ‚Ä¢ Encoding: Hum_Cat\n",
      "    - Classes: 3 | ['High', 'Low', 'Medium']\n",
      "  ‚Ä¢ Encoding: N_Level\n",
      "    - Classes: 3 | ['High', 'Low', 'Medium']\n",
      "  ‚Ä¢ Encoding: K_Level\n",
      "    - Classes: 3 | ['High', 'Low', 'Medium']\n",
      "  ‚Ä¢ Encoding: P_Level\n",
      "    - Classes: 3 | ['High', 'Low', 'Medium']\n",
      "  ‚Ä¢ Encoding: Soil_Crop_Combo\n",
      "    - Classes: 55 | ['Black_Barley', 'Black_Cotton', 'Black_Ground Nuts', 'Black_Maize', 'Black_Millets']...\n",
      "\n",
      "üéØ Encoding target variable: Fertilizer Name\n",
      "  ‚Ä¢ Target classes: 7\n",
      "  ‚Ä¢ Class mapping preview: {'10-26-26': 0, '14-35-14': 1, '17-17-17': 2, '20-20': 3, '28-28': 4}\n",
      "\n",
      "‚úÖ Encoding completed:\n",
      "  ‚Ä¢ Training features: (750000, 25)\n",
      "  ‚Ä¢ Test features: (250000, 25)\n",
      "  ‚Ä¢ Encoded target: (750000,)\n",
      "  ‚Ä¢ Encoders stored: 10\n"
     ]
    }
   ],
   "source": [
    "def encode_categorical_features(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Encode categorical features using LabelEncoder\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        X_test: Test features  \n",
    "        y_train: Training target\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (X_train_encoded, X_test_encoded, y_encoded, encoders_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize encoders dictionary\n",
    "    encoders = {}\n",
    "    \n",
    "    # Create copies to avoid modifying originals\n",
    "    X_train_enc = X_train.copy()\n",
    "    X_test_enc = X_test.copy()\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"üî¢ Encoding categorical features...\")\n",
    "    print(f\"Categorical columns found: {categorical_cols}\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for col in categorical_cols:\n",
    "        print(f\"  ‚Ä¢ Encoding: {col}\")\n",
    "        \n",
    "        # Create encoder\n",
    "        encoder = LabelEncoder()\n",
    "        \n",
    "        # Fit on combined training and test data to ensure consistency\n",
    "        combined_values = pd.concat([X_train[col], X_test[col]]).astype(str)\n",
    "        encoder.fit(combined_values)\n",
    "        \n",
    "        # Transform both datasets\n",
    "        X_train_enc[col] = encoder.transform(X_train[col].astype(str))\n",
    "        X_test_enc[col] = encoder.transform(X_test[col].astype(str))\n",
    "        \n",
    "        # Store encoder\n",
    "        encoders[col] = encoder\n",
    "        \n",
    "        print(f\"    - Classes: {len(encoder.classes_)} | {list(encoder.classes_[:5])}{'...' if len(encoder.classes_) > 5 else ''}\")\n",
    "    \n",
    "    # Encode target variable\n",
    "    print(f\"\\nüéØ Encoding target variable: {target_column}\")\n",
    "    target_encoder = LabelEncoder()\n",
    "    y_encoded = target_encoder.fit_transform(y_train)\n",
    "    encoders['target'] = target_encoder\n",
    "    \n",
    "    print(f\"  ‚Ä¢ Target classes: {len(target_encoder.classes_)}\")\n",
    "    print(f\"  ‚Ä¢ Class mapping preview: {dict(zip(target_encoder.classes_[:5], range(5)))}\")\n",
    "    \n",
    "    return X_train_enc, X_test_enc, y_encoded, encoders\n",
    "\n",
    "# Apply encoding\n",
    "X_train_encoded, X_test_encoded, y_encoded, label_encoders = encode_categorical_features(\n",
    "    X_train_featured, X_test_featured, y_raw\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Encoding completed:\")\n",
    "print(f\"  ‚Ä¢ Training features: {X_train_encoded.shape}\")\n",
    "print(f\"  ‚Ä¢ Test features: {X_test_encoded.shape}\")\n",
    "print(f\"  ‚Ä¢ Encoded target: {y_encoded.shape}\")\n",
    "print(f\"  ‚Ä¢ Encoders stored: {len(label_encoders)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f42c8d",
   "metadata": {},
   "source": [
    "## üéØ 5. Feature Selection\n",
    "\n",
    "**Select features for model training with easy toggle options**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc0844e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Validating features against processed dataset...\n",
      "üìä Available columns in dataset: ['id', 'Temparature', 'Humidity', 'Moisture', 'Soil Type', 'Crop Type', 'Nitrogen', 'Potassium', 'Phosphorous', 'N_P_ratio', 'N_K_ratio', 'P_K_ratio', 'Total_NPK', 'NPK_Balance', 'Temp_Hum_index', 'Moist_Balance', 'Environ_Stress', 'Temp_Moist_inter', 'Dominant_NPK', 'Temp_Cat', 'Hum_Cat', 'N_Level', 'K_Level', 'P_Level', 'Soil_Crop_Combo']\n",
      "\n",
      "üìã SELECTED FEATURES (8 total):\n",
      "\n",
      "üå°Ô∏è Original Climate (3):\n",
      "   1. Temparature\n",
      "   2. Humidity\n",
      "   3. Moisture\n",
      "\n",
      "üß™ Original NPK (3):\n",
      "   1. Nitrogen\n",
      "   2. Potassium\n",
      "   3. Phosphorous\n",
      "\n",
      "üî¢ Encoded Categories (2):\n",
      "   1. Soil Type\n",
      "   2. Crop Type\n",
      "\n",
      "üöÄ Ready for model training with 8 features!\n",
      "\n",
      "‚úÖ Final dataset shapes:\n",
      "  ‚Ä¢ Training: (750000, 8)\n",
      "  ‚Ä¢ Test: (250000, 8)\n",
      "  ‚Ä¢ Target: (750000,)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FEATURE SELECTION FOR THE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "features_to_use = [\n",
    "    # üå°Ô∏è ORIGINAL CLIMATE VARIABLES\n",
    "    'Temparature',\n",
    "    'Humidity', \n",
    "    'Moisture',\n",
    "    \n",
    "    # üß™ CHEMICAL VARIABLES (NPK)\n",
    "    'Nitrogen',\n",
    "    'Potassium', \n",
    "    'Phosphorous',\n",
    "    \n",
    "    # üìä ENGINEERED FEATURES - NPK RATIOS (from create_features)\n",
    "    # 'N_P_ratio',\n",
    "    # 'N_K_ratio',\n",
    "    # 'P_K_ratio',\n",
    "    # 'Total_NPK',\n",
    "    # 'NPK_Balance',\n",
    "    \n",
    "    # üå°Ô∏è ENGINEERED FEATURES - CLIMATE INDICES (from create_features)\n",
    "    # 'Temp_Hum_index',\n",
    "    # 'Moist_Balance',\n",
    "    # 'Environ_Stress',\n",
    "    # 'Temp_Moist_inter',\n",
    "    \n",
    "    # üè∑Ô∏è ENGINEERED FEATURES - CATEGORICAL LEVELS (from create_features, encoded)\n",
    "    # 'Temp_Cat',\n",
    "    # 'Hum_Cat',\n",
    "    # 'N_Level',\n",
    "    # 'K_Level',\n",
    "    # 'P_Level',\n",
    "\n",
    "    # üîó ENGINEERED FEATURES - COMBINATIONS (from create_features)\n",
    "    # 'Soil_Crop_Combo',\n",
    "    # 'Dominant_NPK',\n",
    "    \n",
    "    # üî¢ ENCODED CATEGORICAL FEATURES (from preprocessing)\n",
    "    'Soil Type',      # ‚úÖ Encoded during preprocessing\n",
    "    'Crop Type',      # ‚úÖ Encoded during preprocessing\n",
    "]\n",
    "\n",
    "# Validate available features against the actual processed dataset\n",
    "print(f\"üîç Validating features against processed dataset...\")\n",
    "print(f\"üìä Available columns in dataset: {list(X_train_encoded.columns)}\")\n",
    "\n",
    "available_features = []\n",
    "missing_features = []\n",
    "\n",
    "for feature in features_to_use:\n",
    "    if feature in X_train_encoded.columns:\n",
    "        available_features.append(feature)\n",
    "    else:\n",
    "        missing_features.append(feature)\n",
    "\n",
    "# Update final feature list to only include available features\n",
    "features_to_use = available_features\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\n‚ö†Ô∏è Missing features (will be skipped): {missing_features}\")\n",
    "\n",
    "# Display selected features by category\n",
    "print(f\"\\nüìã SELECTED FEATURES ({len(features_to_use)} total):\")\n",
    "\n",
    "# Group features by category for better readability\n",
    "climate_original = [f for f in features_to_use if f in ['Temparature', 'Humidity', 'Moisture']]\n",
    "npk_original = [f for f in features_to_use if f in ['Nitrogen', 'Potassium', 'Phosphorous']]\n",
    "npk_ratios = [f for f in features_to_use if any(x in f for x in ['_ratio', 'Total_NPK', 'NPK_Balance'])]\n",
    "climate_engineered = [f for f in features_to_use if any(x in f for x in ['Temp_Hum', 'Moist_Balance', 'Environ_Stress', 'Temp_Moist'])]\n",
    "categorical_levels = [f for f in features_to_use if any(x in f for x in ['_Cat', '_Level'])]\n",
    "combinations = [f for f in features_to_use if any(x in f for x in ['Combo', 'Dominant'])]\n",
    "encoded_original = [f for f in features_to_use if f in ['Soil Type', 'Crop Type']]\n",
    "\n",
    "feature_groups = [\n",
    "    (\"üå°Ô∏è Original Climate\", climate_original),\n",
    "    (\"üß™ Original NPK\", npk_original),\n",
    "    (\"üìä NPK Ratios\", npk_ratios),\n",
    "    (\"üå°Ô∏è Climate Indices\", climate_engineered),\n",
    "    (\"üè∑Ô∏è Categorical Levels\", categorical_levels),\n",
    "    (\"üîó Combinations\", combinations),\n",
    "    (\"üî¢ Encoded Categories\", encoded_original)\n",
    "]\n",
    "\n",
    "for group_name, group_features in feature_groups:\n",
    "    if group_features:\n",
    "        print(f\"\\n{group_name} ({len(group_features)}):\")\n",
    "        for i, feature in enumerate(group_features, 1):\n",
    "            print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready for model training with {len(features_to_use)} features!\")\n",
    "\n",
    "# Create final training datasets\n",
    "X_final = X_train_encoded[features_to_use].copy()\n",
    "X_test_final = X_test_encoded[features_to_use].copy()\n",
    "\n",
    "print(f\"\\n‚úÖ Final dataset shapes:\")\n",
    "print(f\"  ‚Ä¢ Training: {X_final.shape}\")\n",
    "print(f\"  ‚Ä¢ Test: {X_test_final.shape}\")\n",
    "print(f\"  ‚Ä¢ Target: {y_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5d67f",
   "metadata": {},
   "source": [
    "## üîÑ 6. Cross-Validation Setup\n",
    "\n",
    "**Configure stratified 10-fold cross-validation to maintain class distribution across folds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ea40e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ CROSS-VALIDATION CONFIGURATION:\n",
      "  ‚Ä¢ Number of folds: 10\n",
      "  ‚Ä¢ Strategy: Stratified (maintains class proportions)\n",
      "  ‚Ä¢ Shuffle: True\n",
      "  ‚Ä¢ Random state: 513\n",
      "\n",
      "üìä Class distribution analysis:\n",
      "  ‚Ä¢ Total classes: 7\n",
      "  ‚Ä¢ Total samples: 750000\n",
      "  ‚Ä¢ Samples per fold: ~75000\n",
      "  ‚Ä¢ Minimum class size: 92317\n",
      "  ‚úÖ All classes have sufficient samples for 10-fold CV\n",
      "\n",
      "üîç Fold size preview:\n",
      "  Fold 1: Train=675000, Val=75000\n",
      "  Fold 2: Train=675000, Val=75000\n",
      "  Fold 3: Train=675000, Val=75000\n",
      "  ...\n",
      "  Fold 10: Train=675000, Val=75000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STRATIFIED 10-FOLD CROSS-VALIDATION CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Cross-validation parameters\n",
    "N_SPLITS = 10  # 10-fold cross-validation for robust evaluation\n",
    "RANDOM_STATE = 513\n",
    "SHUFFLE = True\n",
    "\n",
    "# Initialize StratifiedKFold to maintain class distribution\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=N_SPLITS, \n",
    "    shuffle=SHUFFLE, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"üîÑ CROSS-VALIDATION CONFIGURATION:\")\n",
    "print(f\"  ‚Ä¢ Number of folds: {N_SPLITS}\")\n",
    "print(f\"  ‚Ä¢ Strategy: Stratified (maintains class proportions)\")\n",
    "print(f\"  ‚Ä¢ Shuffle: {SHUFFLE}\")\n",
    "print(f\"  ‚Ä¢ Random state: {RANDOM_STATE}\")\n",
    "\n",
    "# Analyze class distribution for stratification\n",
    "print(f\"\\nüìä Class distribution analysis:\")\n",
    "unique_classes, class_counts = np.unique(y_encoded, return_counts=True)\n",
    "print(f\"  ‚Ä¢ Total classes: {len(unique_classes)}\")\n",
    "print(f\"  ‚Ä¢ Total samples: {len(y_encoded)}\")\n",
    "print(f\"  ‚Ä¢ Samples per fold: ~{len(y_encoded) // N_SPLITS}\")\n",
    "\n",
    "# Check minimum class size for stratification\n",
    "min_class_count = min(class_counts)\n",
    "print(f\"  ‚Ä¢ Minimum class size: {min_class_count}\")\n",
    "if min_class_count < N_SPLITS:\n",
    "    print(f\"  ‚ö†Ô∏è Warning: Smallest class has {min_class_count} samples, less than {N_SPLITS} folds\")\n",
    "    print(f\"    Some folds may not contain all classes\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ All classes have sufficient samples for {N_SPLITS}-fold CV\")\n",
    "\n",
    "# Preview fold splits\n",
    "print(f\"\\nüîç Fold size preview:\")\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_final, y_encoded)):\n",
    "    if fold_idx < 3:  # Show first 3 folds\n",
    "        print(f\"  Fold {fold_idx + 1}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
    "    elif fold_idx == 3:\n",
    "        print(\"  ...\")\n",
    "    elif fold_idx == N_SPLITS - 1:  # Show last fold\n",
    "        print(f\"  Fold {fold_idx + 1}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5df129",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è 7. XGBoost Configuration\n",
    "\n",
    "**Set up XGBoost hyperparameters optimized for the fertilizer prediction task**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4db2902d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Class weight calculation:\n",
      "  ‚Ä¢ Balanced class weights computed for 7 classes\n",
      "  ‚Ä¢ Weight range: 0.936 - 1.161\n",
      "\n",
      "üöÄ XGBOOST CONFIGURATION:\n",
      "  ‚Ä¢ Objective: multi:softprob\n",
      "  ‚Ä¢ Number of classes: 7\n",
      "  ‚Ä¢ Max depth: 8\n",
      "  ‚Ä¢ Learning rate: 0.1\n",
      "  ‚Ä¢ Max estimators: 2000\n",
      "  ‚Ä¢ Early stopping: 100 rounds\n",
      "  ‚Ä¢ Evaluation metric: mlogloss\n",
      "  ‚Ä¢ Regularization: L1=0.1, L2=1.0\n",
      "  ‚Ä¢ Class balancing: Enabled\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# XGBOOST HYPERPARAMETER CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_encoded),\n",
    "    y=y_encoded\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_encoded), class_weights))\n",
    "\n",
    "print(\"‚öñÔ∏è Class weight calculation:\")\n",
    "print(f\"  ‚Ä¢ Balanced class weights computed for {len(class_weight_dict)} classes\")\n",
    "print(f\"  ‚Ä¢ Weight range: {min(class_weights):.3f} - {max(class_weights):.3f}\")\n",
    "\n",
    "# XGBoost hyperparameters (optimized for multi-class classification)\n",
    "xgb_params = {\n",
    "    # Multi-class objective\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': len(label_encoders['target'].classes_),\n",
    "    'eval_metric': 'mlogloss',\n",
    "    \n",
    "    # Tree structure\n",
    "    'max_depth': 8,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    \n",
    "    # Learning parameters\n",
    "    'learning_rate': 0.1,\n",
    "    'n_estimators': 2000,  # High number with early stopping\n",
    "    \n",
    "    # Regularization\n",
    "    'reg_alpha': 0.1,  # L1 regularization\n",
    "    'reg_lambda': 1.0,  # L2 regularization\n",
    "    'gamma': 0.1,      # Minimum split loss\n",
    "    \n",
    "    # Performance\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0,\n",
    "    \n",
    "    # Early stopping will be handled separately\n",
    "    'early_stopping_rounds': 100\n",
    "}\n",
    "\n",
    "# Early stopping configuration\n",
    "es = 100\n",
    "eval_metric = 'mlogloss'\n",
    "\n",
    "print(f\"\\nüöÄ XGBOOST CONFIGURATION:\")\n",
    "print(f\"  ‚Ä¢ Objective: {xgb_params['objective']}\")\n",
    "print(f\"  ‚Ä¢ Number of classes: {xgb_params['num_class']}\")\n",
    "print(f\"  ‚Ä¢ Max depth: {xgb_params['max_depth']}\")\n",
    "print(f\"  ‚Ä¢ Learning rate: {xgb_params['learning_rate']}\")\n",
    "print(f\"  ‚Ä¢ Max estimators: {xgb_params['n_estimators']}\")\n",
    "print(f\"  ‚Ä¢ Early stopping: {xgb_params['early_stopping_rounds']} rounds\")\n",
    "print(f\"  ‚Ä¢ Evaluation metric: {xgb_params['eval_metric']}\")\n",
    "print(f\"  ‚Ä¢ Regularization: L1={xgb_params['reg_alpha']}, L2={xgb_params['reg_lambda']}\")\n",
    "print(f\"  ‚Ä¢ Class balancing: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d735d",
   "metadata": {},
   "source": [
    "## üèãÔ∏è 8. Model Training with 10-Fold Cross-Validation\n",
    "\n",
    "**Train XGBoost models using stratified 10-fold cross-validation with early stopping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66cc36f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting model training...\n",
      "üèãÔ∏è Starting 10-Fold Cross-Validation Training...\n",
      "‚è∞ Training started at: 22:31:36\n",
      "\n",
      "üìÅ FOLD 1/10\n",
      "  ‚Ä¢ Train samples: 675000\n",
      "  ‚Ä¢ Validation samples: 75000\n",
      "  ‚úÖ Fold completed in 526.4s\n",
      "  üìä Accuracy: 0.2033 | MAP@3: 0.3380\n",
      "  üîÑ Best iteration: 374\n",
      "\n",
      "üìÅ FOLD 2/10\n",
      "  ‚Ä¢ Train samples: 675000\n",
      "  ‚Ä¢ Validation samples: 75000\n",
      "  ‚úÖ Fold completed in 473.7s\n",
      "  üìä Accuracy: 0.2014 | MAP@3: 0.3365\n",
      "  üîÑ Best iteration: 329\n",
      "\n",
      "üìÅ FOLD 3/10\n",
      "  ‚Ä¢ Train samples: 675000\n",
      "  ‚Ä¢ Validation samples: 75000\n",
      "  ‚úÖ Fold completed in 504.8s\n",
      "  üìä Accuracy: 0.1994 | MAP@3: 0.3349\n",
      "  üîÑ Best iteration: 356\n",
      "\n",
      "üìÅ FOLD 4/10\n",
      "  ‚Ä¢ Train samples: 675000\n",
      "  ‚Ä¢ Validation samples: 75000\n",
      "  ‚úÖ Fold completed in 470.5s\n",
      "  üìä Accuracy: 0.1975 | MAP@3: 0.3326\n",
      "  üîÑ Best iteration: 321\n",
      "\n",
      "üìÅ FOLD 5/10\n",
      "  ‚Ä¢ Train samples: 675000\n",
      "  ‚Ä¢ Validation samples: 75000\n",
      "  ‚úÖ Fold completed in 468.6s\n",
      "  üìä Accuracy: 0.1983 | MAP@3: 0.3331\n",
      "  üîÑ Best iteration: 322\n",
      "\n",
      "üìÅ FOLD 6/10\n",
      "  ‚Ä¢ Train samples: 675000\n",
      "  ‚Ä¢ Validation samples: 75000\n",
      "  ‚úÖ Fold completed in 423.3s\n",
      "  üìä Accuracy: 0.2000 | MAP@3: 0.3355\n",
      "  üîÑ Best iteration: 285\n",
      "\n",
      "üìÅ FOLD 7/10\n",
      "  ‚Ä¢ Train samples: 675000\n",
      "  ‚Ä¢ Validation samples: 75000\n",
      "  ‚úÖ Fold completed in 453.3s\n",
      "  üìä Accuracy: 0.1989 | MAP@3: 0.3340\n",
      "  üîÑ Best iteration: 307\n",
      "\n",
      "üìÅ FOLD 8/10\n",
      "  ‚Ä¢ Train samples: 675000\n",
      "  ‚Ä¢ Validation samples: 75000\n",
      "  ‚úÖ Fold completed in 466.4s\n",
      "  üìä Accuracy: 0.2001 | MAP@3: 0.3357\n",
      "  üîÑ Best iteration: 320\n",
      "\n",
      "üìÅ FOLD 9/10\n",
      "  ‚Ä¢ Train samples: 675000\n",
      "  ‚Ä¢ Validation samples: 75000\n",
      "  ‚úÖ Fold completed in 466.0s\n",
      "  üìä Accuracy: 0.2016 | MAP@3: 0.3362\n",
      "  üîÑ Best iteration: 319\n",
      "\n",
      "üìÅ FOLD 10/10\n",
      "  ‚Ä¢ Train samples: 675000\n",
      "  ‚Ä¢ Validation samples: 75000\n",
      "  ‚úÖ Fold completed in 456.4s\n",
      "  üìä Accuracy: 0.1969 | MAP@3: 0.3326\n",
      "  üîÑ Best iteration: 311\n",
      "\n",
      "üéâ CROSS-VALIDATION TRAINING COMPLETED!\n",
      "‚è∞ Training finished at: 23:50:08\n",
      "‚è±Ô∏è Total training time: 4711.8s (78.5min)\n",
      "üìä Overall Accuracy: 0.1998\n",
      "üìä Overall MAP@3: 0.3349\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 10-FOLD CROSS-VALIDATION TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def train_xgboost_cv(X, y, features, cv_splitter, params):\n",
    "    \"\"\"\n",
    "    Train XGBoost models using cross-validation\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target vector (encoded)\n",
    "        features: List of feature names to use\n",
    "        cv_splitter: Cross-validation splitter (StratifiedKFold)\n",
    "        params: XGBoost parameters\n",
    "        early_stopping_rounds: Early stopping patience\n",
    "        \n",
    "    Returns:\n",
    "        Dict with trained models, predictions, and metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize storage\n",
    "    models = {}\n",
    "    oof_predictions = np.zeros((len(X), params['num_class']))  # Out-of-fold predictions\n",
    "    cv_scores = []\n",
    "    feature_importance_list = []\n",
    "    \n",
    "    print(f\"üèãÔ∏è Starting {N_SPLITS}-Fold Cross-Validation Training...\")\n",
    "    print(f\"‚è∞ Training started at: {time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv_splitter.split(X, y)):\n",
    "        \n",
    "        fold_start_time = time.time()\n",
    "        print(f\"\\nüìÅ FOLD {fold_idx + 1}/{N_SPLITS}\")\n",
    "        print(f\"  ‚Ä¢ Train samples: {len(train_idx)}\")\n",
    "        print(f\"  ‚Ä¢ Validation samples: {len(val_idx)}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_fold = X.iloc[train_idx][features]\n",
    "        X_val_fold = X.iloc[val_idx][features]\n",
    "        y_train_fold = y[train_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        # Calculate sample weights for this fold\n",
    "        fold_class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y_train_fold),\n",
    "            y=y_train_fold\n",
    "        )\n",
    "        fold_class_weight_dict = dict(zip(np.unique(y_train_fold), fold_class_weights))\n",
    "        sample_weights = np.array([fold_class_weight_dict.get(label, 1.0) for label in y_train_fold])\n",
    "        \n",
    "        # Initialize model\n",
    "        model = XGBClassifier(**params)\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            sample_weight=sample_weights,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Predict validation set\n",
    "        val_pred_proba = model.predict_proba(X_val_fold)\n",
    "        val_pred_classes = model.predict(X_val_fold)\n",
    "        \n",
    "        # Store out-of-fold predictions\n",
    "        oof_predictions[val_idx] = val_pred_proba\n",
    "        \n",
    "        # Calculate fold metrics\n",
    "        fold_accuracy = accuracy_score(y_val_fold, val_pred_classes)\n",
    "        \n",
    "        # Calculate MAP@3 for this fold\n",
    "        # Get top 3 predictions for each sample\n",
    "        val_top3_indices = np.argsort(val_pred_proba, axis=1)[:, -3:][:, ::-1]\n",
    "        \n",
    "        # Convert to lists for mapk function\n",
    "        actual_list = y_val_fold.tolist() if hasattr(y_val_fold, 'tolist') else list(y_val_fold)\n",
    "        predicted_list = val_top3_indices.tolist()\n",
    "        \n",
    "        # Calculate MAP@3 using the correct format\n",
    "        fold_map3 = mapk(actual_list, predicted_list, k=3)\n",
    "        \n",
    "        # Store results\n",
    "        cv_scores.append({\n",
    "            'fold': fold_idx + 1,\n",
    "            'accuracy': fold_accuracy,\n",
    "            'map3': fold_map3,\n",
    "            'best_iteration': model.best_iteration,\n",
    "            'train_samples': len(train_idx),\n",
    "            'val_samples': len(val_idx),\n",
    "            'training_time': time.time() - fold_start_time\n",
    "        })\n",
    "        \n",
    "        # Store model and feature importance\n",
    "        models[f'fold_{fold_idx + 1}'] = model\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'importance': model.feature_importances_,\n",
    "                'fold': fold_idx + 1\n",
    "            })\n",
    "            feature_importance_list.append(importance_df)\n",
    "        \n",
    "        fold_time = time.time() - fold_start_time\n",
    "        print(f\"  ‚úÖ Fold completed in {fold_time:.1f}s\")\n",
    "        print(f\"  üìä Accuracy: {fold_accuracy:.4f} | MAP@3: {fold_map3:.4f}\")\n",
    "        print(f\"  üîÑ Best iteration: {model.best_iteration}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    oof_pred_classes = np.argmax(oof_predictions, axis=1)\n",
    "    overall_accuracy = accuracy_score(y, oof_pred_classes)\n",
    "    \n",
    "    # Calculate overall MAP@3\n",
    "    # Get top 3 predictions for each sample\n",
    "    oof_top3_indices = np.argsort(oof_predictions, axis=1)[:, -3:][:, ::-1]\n",
    "    \n",
    "    # Convert to lists for mapk function\n",
    "    actual_list = y.tolist() if hasattr(y, 'tolist') else list(y)\n",
    "    predicted_list = oof_top3_indices.tolist()\n",
    "    \n",
    "    # Calculate MAP@3 using the correct format\n",
    "    overall_map3 = mapk(actual_list, predicted_list, k=3)\n",
    "    \n",
    "    # Combine feature importance across folds\n",
    "    if feature_importance_list:\n",
    "        feature_importance_df = pd.concat(feature_importance_list, ignore_index=True)\n",
    "        feature_importance_summary = feature_importance_df.groupby('feature')['importance'].agg(['mean', 'std']).reset_index()\n",
    "        feature_importance_summary = feature_importance_summary.sort_values('mean', ascending=False)\n",
    "    else:\n",
    "        feature_importance_summary = None\n",
    "    \n",
    "    return {\n",
    "        'models': models,\n",
    "        'oof_predictions': oof_predictions,\n",
    "        'cv_scores': cv_scores,\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'overall_map3': overall_map3,\n",
    "        'feature_importance': feature_importance_summary\n",
    "    }\n",
    "\n",
    "# Execute cross-validation training\n",
    "print(\"üöÄ Starting model training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "training_results = train_xgboost_cv(\n",
    "    X=X_final,\n",
    "    y=y_encoded,\n",
    "    features=features_to_use,\n",
    "    cv_splitter=skf,\n",
    "    params=xgb_params,\n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüéâ CROSS-VALIDATION TRAINING COMPLETED!\")\n",
    "print(f\"‚è∞ Training finished at: {time.strftime('%H:%M:%S')}\")\n",
    "print(f\"‚è±Ô∏è Total training time: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "print(f\"üìä Overall Accuracy: {training_results['overall_accuracy']:.4f}\")\n",
    "print(f\"üìä Overall MAP@3: {training_results['overall_map3']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7da959",
   "metadata": {},
   "source": [
    "## üìä 9. Model Evaluation\n",
    "\n",
    "**Complete performance analysis and cross-validation metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "244fa935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä CROSS-VALIDATION RESULTS\n",
      "============================================================\n",
      "üéØ FINAL METRICS:\n",
      "  üìà Cross-Validation Accuracy: 0.1998 ¬± 0.0020\n",
      "  üìà Cross-Validation MAP@3:    0.3349 ¬± 0.0018\n",
      "  üìà Out-of-Fold Accuracy:      0.1998\n",
      "  üìà Out-of-Fold MAP@3:         0.3349\n",
      "\n",
      "üîç STABILITY ANALYSIS:\n",
      "  üìä Coefficient of variation (Accuracy): 0.010\n",
      "  üìä Coefficient of variation (MAP@3):    0.005\n",
      "  ‚úÖ Stable model (Accuracy CV < 0.05)\n",
      "  ‚úÖ Stable model (MAP@3 CV < 0.05)\n",
      "\n",
      "‚è±Ô∏è TRAINING TIMES:\n",
      "  üìä Average time per fold: 470.8s\n",
      "  üìä Total time: 4711.8s (78.5min)\n",
      "\n",
      "üìã DETAILED RESULTS BY FOLD:\n",
      "Fold  Accuracy   MAP@3    Best_Iter  Time(s)\n",
      "--------------------------------------------------\n",
      " 1    0.2033   0.3380      374    526.4\n",
      " 2    0.2014   0.3365      329    473.6\n",
      " 3    0.1994   0.3349      356    504.6\n",
      " 4    0.1975   0.3326      321    470.4\n",
      " 5    0.1983   0.3331      322    468.5\n",
      " 6    0.2000   0.3355      285    423.2\n",
      " 7    0.1989   0.3340      307    453.3\n",
      " 8    0.2001   0.3357      320    466.3\n",
      " 9    0.2016   0.3362      319    465.9\n",
      "10    0.1969   0.3326      311    456.3\n",
      "--------------------------------------------------\n",
      "Mean  0.1998   0.3349      324    470.8\n",
      "\n",
      "üîç TOP 10 MOST IMPORTANT FEATURES:\n",
      "Rank  Feature               Importance\n",
      "----------------------------------------\n",
      " 1.   Phosphorous            0.1313\n",
      " 2.   Moisture               0.1290\n",
      " 3.   Nitrogen               0.1288\n",
      " 4.   Crop Type              0.1279\n",
      " 5.   Potassium              0.1241\n",
      " 6.   Humidity               0.1207\n",
      " 7.   Soil Type              0.1195\n",
      " 8.   Temparature            0.1187\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CROSS-VALIDATION RESULTS EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üìä CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract results from training\n",
    "cv_results_df = pd.DataFrame(training_results['cv_scores'])\n",
    "\n",
    "# Calculate statistics\n",
    "accuracy_mean = cv_results_df['accuracy'].mean()\n",
    "accuracy_std = cv_results_df['accuracy'].std()\n",
    "map3_mean = cv_results_df['map3'].mean()\n",
    "map3_std = cv_results_df['map3'].std()\n",
    "\n",
    "print(f\"üéØ FINAL METRICS:\")\n",
    "print(f\"  üìà Cross-Validation Accuracy: {accuracy_mean:.4f} ¬± {accuracy_std:.4f}\")\n",
    "print(f\"  üìà Cross-Validation MAP@3:    {map3_mean:.4f} ¬± {map3_std:.4f}\")\n",
    "print(f\"  üìà Out-of-Fold Accuracy:      {training_results['overall_accuracy']:.4f}\")\n",
    "print(f\"  üìà Out-of-Fold MAP@3:         {training_results['overall_map3']:.4f}\")\n",
    "\n",
    "# Stability evaluation\n",
    "accuracy_cv = accuracy_std / accuracy_mean if accuracy_mean > 0 else 0\n",
    "map3_cv = map3_std / map3_mean if map3_mean > 0 else 0\n",
    "\n",
    "print(f\"\\nüîç STABILITY ANALYSIS:\")\n",
    "print(f\"  üìä Coefficient of variation (Accuracy): {accuracy_cv:.3f}\")\n",
    "print(f\"  üìä Coefficient of variation (MAP@3):    {map3_cv:.3f}\")\n",
    "print(f\"  {'‚úÖ Stable model' if accuracy_cv < 0.05 else '‚ö†Ô∏è Variable model'} (Accuracy CV < 0.05)\")\n",
    "print(f\"  {'‚úÖ Stable model' if map3_cv < 0.05 else '‚ö†Ô∏è Variable model'} (MAP@3 CV < 0.05)\")\n",
    "\n",
    "# Training time analysis\n",
    "avg_fold_time = cv_results_df['training_time'].mean()\n",
    "print(f\"\\n‚è±Ô∏è TRAINING TIMES:\")\n",
    "print(f\"  üìä Average time per fold: {avg_fold_time:.1f}s\")\n",
    "print(f\"  üìä Total time: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "\n",
    "# Detailed results by fold\n",
    "print(f\"\\nüìã DETAILED RESULTS BY FOLD:\")\n",
    "print(\"Fold  Accuracy   MAP@3    Best_Iter  Time(s)\")\n",
    "print(\"-\" * 50)\n",
    "for _, row in cv_results_df.iterrows():\n",
    "    print(f\"{row['fold']:2.0f}    {row['accuracy']:.4f}   {row['map3']:.4f}     {row['best_iteration']:4.0f}   {row['training_time']:6.1f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Mean  {accuracy_mean:.4f}   {map3_mean:.4f}     {cv_results_df['best_iteration'].mean():4.0f}   {avg_fold_time:6.1f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "if training_results['feature_importance'] is not None:\n",
    "    print(f\"\\nüîç TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "    print(\"Rank  Feature               Importance\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (_, row) in enumerate(training_results['feature_importance'].head(10).iterrows()):\n",
    "        print(f\"{i+1:2d}.   {row['feature']:20} {row['mean']:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524143e",
   "metadata": {},
   "source": [
    "## üéØ 10. Test Predictions Generation\n",
    "\n",
    "**Final predictions using ensemble of trained models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c09ef9c",
   "metadata": {},
   "source": [
    "## üíæ 11. Model Persistence and Results\n",
    "\n",
    "**Save trained models, metrics, and create submission file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfaeaef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ MODEL DIRECTORY:\n",
      "  ../models/XGB/10CV/XGB_10CV_MAP@3-033492\n",
      "\n",
      "üìù FILES TO CREATE:\n",
      "  hparams        : XGB_10CV_MAP@3-033492_hparams.json\n",
      "  metrics        : XGB_10CV_MAP@3-033492_metrics.json\n",
      "  metrics_pkl    : XGB_10CV_MAP@3-033492_metrics.pkl\n",
      "  model_pkl      : XGB_10CV_MAP@3-033492_model.pkl\n",
      "  feature_import : XGB_10CV_MAP@3-033492_feature_importance.csv\n",
      "  submission     : XGB_10CV_MAP@3-033492_submission.csv\n",
      "  submission_info: XGB_10CV_MAP@3-033492_submission_info.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# FILE SAVING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure model name based on MAP@3\n",
    "overall_map3 = training_results['overall_map3']\n",
    "model_name = f\"XGB_10CV_MAP@3-{overall_map3:.5f}\".replace('.', '')\n",
    "model_dir = f\"../models/XGB/{N_SPLITS}CV/{model_name}\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ MODEL DIRECTORY:\")\n",
    "print(f\"  {model_dir}\")\n",
    "\n",
    "# File name configuration\n",
    "base_filename = model_name\n",
    "files_to_create = {\n",
    "    'hparams': f\"{base_filename}_hparams.json\",\n",
    "    'metrics': f\"{base_filename}_metrics.json\",\n",
    "    'metrics_pkl': f\"{base_filename}_metrics.pkl\",\n",
    "    'model_pkl': f\"{base_filename}_model.pkl\",\n",
    "    'feature_import': f\"{base_filename}_feature_importance.csv\",\n",
    "    'submission': f\"{base_filename}_submission.csv\",\n",
    "    'submission_info': f\"{base_filename}_submission_info.json\"\n",
    "}\n",
    "\n",
    "print(f\"\\nüìù FILES TO CREATE:\")\n",
    "for file_type, filename in files_to_create.items():\n",
    "    print(f\"  {file_type:15}: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf5945",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trained_models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# SAVE HYPERPARAMETERS\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# General model information\u001b[39;00m\n\u001b[32m      6\u001b[39m hparams_data = {\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mXGBClassifier\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmodel_abbreviation\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mXGB\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcv_strategy\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_SPLITS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-Fold Stratified Cross Validation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moptimization_method\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mManual hyperparameter tuning\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mensemble_method\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mAverage of fold predictions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m \n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# Fixed hyperparameters used\u001b[39;00m\n\u001b[32m     14\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhyperparameters\u001b[39m\u001b[33m\"\u001b[39m: xgb_params,\n\u001b[32m     15\u001b[39m \n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# General configuration\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfeatures_selected\u001b[39m\u001b[33m\"\u001b[39m: features_to_use,\n\u001b[32m     18\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnum_features\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(features_to_use),\n\u001b[32m     19\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mclass_weights_used\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     20\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrandom_state\u001b[39m\u001b[33m\"\u001b[39m: RANDOM_STATE,\n\u001b[32m     21\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcv_splits\u001b[39m\u001b[33m\"\u001b[39m: N_SPLITS,\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtotal_models\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrained_models\u001b[49m),\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mearly_stopping_rounds\u001b[39m\u001b[33m\"\u001b[39m: EARLY_STOPPING_ROUNDS\n\u001b[32m     24\u001b[39m }\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Save general hyperparameters\u001b[39;00m\n\u001b[32m     27\u001b[39m hparams_file = os.path.join(model_dir, files_to_create[\u001b[33m'\u001b[39m\u001b[33mhparams\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'trained_models' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SAVE HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# General model information\n",
    "hparams_data = {\n",
    "    \"model_type\": \"XGBClassifier\",\n",
    "    \"model_abbreviation\": \"XGB\",\n",
    "    \"cv_strategy\": f\"{N_SPLITS}-Fold Stratified Cross Validation\",\n",
    "    \"optimization_method\": \"Manual hyperparameter tuning\",\n",
    "    \"ensemble_method\": \"Average of fold predictions\",\n",
    "    \n",
    "    # Fixed hyperparameters used\n",
    "    \"hyperparameters\": xgb_params,\n",
    "    \n",
    "    # General configuration\n",
    "    \"features_selected\": features_to_use,\n",
    "    \"num_features\": len(features_to_use),\n",
    "    \"class_weights_used\": True,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"cv_splits\": N_SPLITS,\n",
    "    \"total_models\": training_results['models'],\n",
    "    \"early_stopping_rounds\": es\n",
    "}\n",
    "\n",
    "# Save general hyperparameters\n",
    "hparams_file = os.path.join(model_dir, files_to_create['hparams'])\n",
    "with open(hparams_file, 'w') as f:\n",
    "    json.dump(hparams_data, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Hyperparameters saved:\")\n",
    "print(f\"  üìÑ General: {files_to_create['hparams']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697983d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE METRICS\n",
    "# =============================================================================\n",
    "\n",
    "# Extract metrics from training results\n",
    "cv_results_df = pd.DataFrame(training_results['cv_scores'])\n",
    "map3_mean = cv_results_df['map3'].mean()\n",
    "map3_std = cv_results_df['map3'].std()\n",
    "accuracy_mean = cv_results_df['accuracy'].mean()\n",
    "accuracy_std = cv_results_df['accuracy'].std()\n",
    "\n",
    "# Main metrics for JSON\n",
    "metrics_data = {\n",
    "    \"model_type\": \"XGBClassifier\",\n",
    "    \"model_abbreviation\": \"XGB\",\n",
    "    \"tier\": \"10_FOLD_CV\",\n",
    "    \"target_variable\": \"Fertilizer Name\",\n",
    "    \"cv_strategy\": f\"{N_SPLITS}-Fold Stratified Cross Validation\",\n",
    "    \"optimization_method\": \"Manual hyperparameter tuning\",\n",
    "    \n",
    "    # Main metrics\n",
    "    \"map3_score_cv_mean\": float(map3_mean),\n",
    "    \"map3_score_cv_std\": float(map3_std),\n",
    "    \"map3_score_oof\": float(training_results['overall_map3']),\n",
    "    \"accuracy_cv_mean\": float(accuracy_mean),\n",
    "    \"accuracy_cv_std\": float(accuracy_std),\n",
    "    \"accuracy_oof\": float(training_results['overall_accuracy']),\n",
    "    \n",
    "    # Model information\n",
    "    \"num_classes\": len(label_encoders['target'].classes_),\n",
    "    \"features_used\": len(features_to_use),\n",
    "    \"features_list\": features_to_use,\n",
    "    \"cv_folds\": N_SPLITS,\n",
    "    \"total_models_trained\": len(training_results['models']),\n",
    "    \n",
    "    # Metrics by fold\n",
    "    \"fold_results\": training_results['cv_scores'],\n",
    "    \n",
    "    # Stability statistics\n",
    "    \"accuracy_cv_coefficient\": float(accuracy_std / accuracy_mean) if accuracy_mean > 0 else 0.0,\n",
    "    \"map3_cv_coefficient\": float(map3_std / map3_mean) if map3_mean > 0 else 0.0,\n",
    "    \n",
    "    # Times\n",
    "    \"training_time_total\": float(total_time),\n",
    "    \"training_time_per_fold_avg\": float(cv_results_df['training_time'].mean()),\n",
    "    \n",
    "    # Hyperparameters used\n",
    "    \"hyperparameters\": xgb_params,\n",
    "    \n",
    "    # Metadata\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"kaggle_competition\": \"playground-series-s5e6\"\n",
    "}\n",
    "\n",
    "# Save JSON metrics\n",
    "metrics_file = os.path.join(model_dir, files_to_create['metrics'])\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(metrics_data, f, indent=2)\n",
    "\n",
    "# Complete metrics for PKL (includes complex objects)\n",
    "metrics_pkl_data = {\n",
    "    **metrics_data,\n",
    "    \"oof_predictions\": training_results['oof_predictions'],\n",
    "    \"trained_models\": training_results['models'],\n",
    "    \"feature_importance\": training_results['feature_importance'],\n",
    "    \"label_encoders\": label_encoders\n",
    "}\n",
    "\n",
    "# Save PKL metrics\n",
    "metrics_pkl_file = os.path.join(model_dir, files_to_create['metrics_pkl'])\n",
    "joblib.dump(metrics_pkl_data, metrics_pkl_file, compress=3)\n",
    "\n",
    "print(f\"‚úÖ Metrics saved:\")\n",
    "print(f\"  üìÑ JSON: {files_to_create['metrics']}\")\n",
    "print(f\"  üìÑ PKL: {files_to_create['metrics_pkl']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE TRAINED MODELS AND CREATE SUBMISSION\n",
    "# =============================================================================\n",
    "\n",
    "# Save feature importance\n",
    "if training_results['feature_importance'] is not None:\n",
    "    feature_importance_file = os.path.join(model_dir, files_to_create['feature_import'])\n",
    "    training_results['feature_importance'].to_csv(feature_importance_file, index=False)\n",
    "    print(f\"‚úÖ Feature importance saved: {files_to_create['feature_import']}\")\n",
    "\n",
    "# Save the ensemble of trained models\n",
    "model_data = {\n",
    "    \"ensemble_models\": training_results['models'],\n",
    "    \"model_type\": \"XGBClassifier\",\n",
    "    \"cv_folds\": N_SPLITS,\n",
    "    \"features_used\": features_to_use,\n",
    "    \"hyperparameters\": xgb_params,\n",
    "    \"label_encoders\": label_encoders,\n",
    "    \"training_info\": {\n",
    "        \"map3_cv_mean\": float(map3_mean),\n",
    "        \"map3_oof\": float(training_results['overall_map3']),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save models\n",
    "model_file = os.path.join(model_dir, files_to_create['model_pkl'])\n",
    "joblib.dump(model_data, model_file, compress=3)\n",
    "print(f\"‚úÖ Models saved: {files_to_create['model_pkl']}\")\n",
    "\n",
    "# =============================================================================\n",
    "# GENERATE TEST PREDICTIONS AND CREATE SUBMISSION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüîÆ Generating test predictions...\")\n",
    "\n",
    "# Generate ensemble predictions\n",
    "test_predictions_all = []\n",
    "for fold_name, model in training_results['models'].items():\n",
    "    pred_proba = model.predict_proba(X_test_final)\n",
    "    test_predictions_all.append(pred_proba)\n",
    "\n",
    "# Average predictions (ensemble)\n",
    "test_predictions_ensemble = np.mean(test_predictions_all, axis=0)\n",
    "\n",
    "# Get top 3 predictions for each sample\n",
    "test_top3_indices = np.argsort(test_predictions_ensemble, axis=1)[:, -3:][:, ::-1]\n",
    "\n",
    "# Convert indices to fertilizer names\n",
    "test_top3_names = []\n",
    "for i in range(len(test_top3_indices)):\n",
    "    top3_for_sample = []\n",
    "    for j in range(3):\n",
    "        class_idx = test_top3_indices[i, j]\n",
    "        class_name = label_encoders['target'].inverse_transform([class_idx])[0]\n",
    "        top3_for_sample.append(class_name)\n",
    "    test_top3_names.append(top3_for_sample)\n",
    "\n",
    "# Create submission predictions (space-separated top 3)\n",
    "submission_predictions = []\n",
    "for top3_names in test_top3_names:\n",
    "    prediction_string = ' '.join(top3_names)\n",
    "    submission_predictions.append(prediction_string)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(test_predictions_ensemble)),\n",
    "    'Fertilizer Name': submission_predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_file = os.path.join(model_dir, files_to_create['submission'])\n",
    "submission.to_csv(submission_file, index=False)\n",
    "\n",
    "# Submission information\n",
    "submission_info = {\n",
    "    \"model_type\": \"XGBClassifier\",\n",
    "    \"model_abbreviation\": \"XGB\", \n",
    "    \"cv_strategy\": f\"{N_SPLITS}-Fold Stratified Cross Validation\",\n",
    "    \"map3_score_cv_mean\": float(map3_mean),\n",
    "    \"map3_score_oof\": float(training_results['overall_map3']),\n",
    "    \"submission_file\": files_to_create['submission'],\n",
    "    \"num_predictions\": len(submission),\n",
    "    \"format\": \"MAP@3 - Top 3 fertilizer names separated by spaces\",\n",
    "    \"target_variable\": \"Fertilizer Name\",\n",
    "    \"ensemble_models\": len(training_results['models']),\n",
    "    \"features_used\": len(features_to_use),\n",
    "    \"total_training_time_minutes\": float(total_time / 60),\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"kaggle_competition\": \"playground-series-s5e6\"\n",
    "}\n",
    "\n",
    "# Save submission information\n",
    "submission_info_file = os.path.join(model_dir, files_to_create['submission_info'])\n",
    "with open(submission_info_file, 'w') as f:\n",
    "    json.dump(submission_info, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Submission created: {files_to_create['submission']}\")\n",
    "print(f\"‚úÖ Submission info saved: {files_to_create['submission_info']}\")\n",
    "print(f\"üìä Submission shape: {submission.shape}\")\n",
    "print(f\"üéØ Sample predictions:\")\n",
    "for i in range(min(3, len(submission))):\n",
    "    print(f\"  {i+1}. {submission.iloc[i, 1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ba37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUMMARY OF SAVED FILES\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nüíæ FINAL SUMMARY - SAVED FILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"üìÅ DIRECTORY: {model_dir}\")\n",
    "print(f\"\\nüìÑ CREATED FILES:\")\n",
    "\n",
    "# Verify and show all created files\n",
    "for file_type, filename in files_to_create.items():\n",
    "    file_path = os.path.join(model_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        if file_size > 1024*1024:  # > 1MB\n",
    "            size_str = f\"{file_size/(1024*1024):.1f} MB\"\n",
    "        elif file_size > 1024:  # > 1KB\n",
    "            size_str = f\"{file_size/1024:.1f} KB\"\n",
    "        else:\n",
    "            size_str = f\"{file_size} bytes\"\n",
    "        \n",
    "        print(f\"  ‚úÖ {filename:40} ({size_str})\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå {filename:40} (NOT CREATED)\")\n",
    "\n",
    "print(f\"\\nüéØ MAIN METRICS:\")\n",
    "print(f\"  üìä MAP@3 (CV Mean):    {map3_mean:.5f} ¬± {map3_std:.5f}\")\n",
    "print(f\"  üìä MAP@3 (OOF):        {training_results['overall_map3']:.5f}\")\n",
    "print(f\"  üìä Accuracy (OOF):     {training_results['overall_accuracy']:.5f}\")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è MODEL CONFIGURATION:\")\n",
    "print(f\"  ü§ñ Models:            {len(training_results['models'])} (ensemble)\")\n",
    "print(f\"  üìä Features:          {len(features_to_use)}\")\n",
    "print(f\"  ‚è±Ô∏è Total time:        {total_time/60:.1f} minutes\")\n",
    "\n",
    "# Show some of the hyperparameters used\n",
    "print(f\"\\nüèÜ MAIN HYPERPARAMETERS:\")\n",
    "main_params = ['max_depth', 'learning_rate', 'n_estimators', 'reg_alpha', 'reg_lambda']\n",
    "for param in main_params:\n",
    "    if param in xgb_params:\n",
    "        print(f\"  {param:15}: {xgb_params[param]}\")\n",
    "\n",
    "print(f\"\\nüéâ ALL FILES SAVED SUCCESSFULLY\")\n",
    "print(f\"üìÇ Location: {os.path.abspath(model_dir)}\")\n",
    "print(f\"\\n‚ú® XGBoost 10-FOLD CV MODEL COMPLETED\")\n",
    "print(f\"üöÄ Final MAP@3 score: {training_results['overall_map3']:.5f}\")\n",
    "print(f\"üéØ Submission ready: {files_to_create['submission']}\")\n",
    "print(f\"\\nüìà WORKFLOW SUMMARY:\")\n",
    "print(f\"  1. ‚úÖ Data loaded and processed\")\n",
    "print(f\"  2. ‚úÖ Feature engineering applied\")\n",
    "print(f\"  3. ‚úÖ Categorical encoding completed\")\n",
    "print(f\"  4. ‚úÖ 10-fold CV training completed\")\n",
    "print(f\"  5. ‚úÖ Model evaluation finished\")\n",
    "print(f\"  6. ‚úÖ Test predictions generated\")\n",
    "print(f\"  7. ‚úÖ Files saved and submission created\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
