{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311624b8",
   "metadata": {},
   "source": [
    "# ğŸš€ Kaggle Playground Series S5E6: XGBoost with 10-Fold CV - Fertilizer Prediction\n",
    "\n",
    "**Advanced XGBoost modeling pipeline with stratified cross-validation for agricultural fertilizer recommendation**\n",
    "\n",
    "This notebook implements a **comprehensive XGBoost modeling solution** for the **Kaggle Playground Series S5E6: Fertilizer Prediction Challenge**. It builds upon the exploratory data analysis (EDA) insights to create a robust, competition-ready model with advanced feature engineering and rigorous validation.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Competition Overview\n",
    "\n",
    "**Objective**: Select the optimal fertilizer for different agricultural conditions (weather, soil, crops)\n",
    "\n",
    "**Problem Type**: Multi-class classification with 22 fertilizer categories\n",
    "\n",
    "**Dataset**: Agricultural features including:\n",
    "- ğŸŒ¡ï¸ Environmental conditions (Temperature, Humidity, Soil Moisture)\n",
    "- ğŸ§ª Soil nutrients (Nitrogen, Phosphorus, Potassium)\n",
    "- ğŸŒ± Agricultural context (Soil Type, Crop Type)\n",
    "- ğŸ¯ Target: Fertilizer Name (22 different fertilizer types)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Evaluation Metric: MAP@3\n",
    "\n",
    "Predictions are evaluated using **Mean Average Precision @ 3 (MAP@3)**:\n",
    "\n",
    "$$MAP@3 = \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{min(3,|P_i|)} P(k) \\times rel(k)$$\n",
    "\n",
    "Where:\n",
    "- **N** = number of observations\n",
    "- **P(k)** = precision at cutoff k\n",
    "- **|P_i|** = number of predictions per observation\n",
    "- **rel(k)** = indicator function (1 if the item at rank k is correct, 0 otherwise)\n",
    "\n",
    "**Key Points for XGBoost Optimization**:\n",
    "- Model must output **probability rankings** for top-3 predictions\n",
    "- **Correct predictions in higher positions** are rewarded more heavily\n",
    "- **Probability calibration** becomes crucial for optimal ranking\n",
    "- **Ensemble methods** can improve ranking stability\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¤– Why XGBoost for This Challenge?\n",
    "\n",
    "**XGBoost (eXtreme Gradient Boosting)** is the optimal choice for this agricultural classification task for several reasons:\n",
    "\n",
    "### ğŸ† **Algorithmic Advantages**\n",
    "- **Tree-based architecture**: Naturally handles feature interactions (e.g., soil-crop combinations, NPK ratios)\n",
    "- **Built-in regularization**: Prevents overfitting with L1/L2 penalties and tree constraints\n",
    "- **Missing value handling**: Robust to data inconsistencies (though our dataset is complete)\n",
    "- **Feature selection**: Automatic relevance weighting through tree splitting decisions\n",
    "\n",
    "### ğŸ“Š **Multi-class Excellence**\n",
    "- **Native multi-class support**: Handles 22 fertilizer classes efficiently with `multi:softprob` objective\n",
    "- **Probability outputs**: Essential for MAP@3 ranking optimization\n",
    "- **Class imbalance handling**: Supports sample weighting for balanced performance across fertilizers\n",
    "- **Calibrated predictions**: Reliable probability estimates for ranking tasks\n",
    "\n",
    "### âš¡ **Performance & Scalability**\n",
    "- **Fast training**: Efficient gradient boosting implementation with parallel processing\n",
    "- **Memory efficiency**: Handles large feature sets without memory issues\n",
    "- **Early stopping**: Prevents overfitting with validation-based stopping criteria\n",
    "- **Cross-validation friendly**: Stable performance across different data splits\n",
    "\n",
    "### ğŸ”§ **Agricultural Domain Fit**\n",
    "- **Non-linear relationships**: Captures complex agricultural interactions (temperature Ã— humidity, NPK ratios)\n",
    "- **Feature importance**: Provides interpretable insights for agricultural decision-making\n",
    "- **Robust to outliers**: Agricultural data often contains natural extremes\n",
    "- **Categorical handling**: Effectively processes soil types and crop varieties\n",
    "\n",
    "### ğŸ¯ **Competition-Specific Benefits**\n",
    "- **Ensemble capability**: Multiple models from CV folds improve prediction stability\n",
    "- **Hyperparameter sensitivity**: Extensive tuning options for performance optimization\n",
    "- **Proven track record**: Dominant algorithm in Kaggle tabular competitions\n",
    "- **MAP@3 optimization**: Probability-based outputs ideal for ranking metrics\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—‚ï¸ Notebook Structure\n",
    "\n",
    "**This comprehensive modeling pipeline covers:**\n",
    "\n",
    "1. **ğŸ“š Library Import & Setup** - Essential ML libraries and utility functions\n",
    "2. **ğŸ“‚ Data Loading** - Training/test data import (EDA completed separately)\n",
    "3. **âš™ï¸ Feature Engineering** - Advanced agricultural feature creation based on EDA insights\n",
    "4. **ğŸ”¢ Categorical Encoding** - Label encoding for tree-based algorithms\n",
    "5. **ğŸ¯ Feature Selection** - Strategic feature subset selection for optimal performance\n",
    "6. **ğŸ”„ Cross-Validation Setup** - Stratified 10-fold CV configuration for robust validation\n",
    "7. **âš™ï¸ XGBoost Configuration** - Hyperparameter optimization for multi-class classification\n",
    "8. **ğŸ‹ï¸ Model Training** - 10-fold CV training with early stopping and class balancing\n",
    "9. **ğŸ“Š Model Evaluation** - Comprehensive performance analysis and stability assessment\n",
    "10. **ğŸ”® Test Predictions** - Ensemble prediction generation for submission\n",
    "11. **ğŸ’¾ Model Persistence** - Complete model artifacts and submission file creation\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ¾ Expected Agricultural Insights Integration\n",
    "\n",
    "**Based on EDA findings, this XGBoost implementation leverages:**\n",
    "- **NPK ratio features** - Nutrient balance relationships more important than absolute values\n",
    "- **Environmental stress indices** - Combined temperature-humidity effects on fertilizer needs\n",
    "- **Soil-crop interactions** - Context-specific fertilizer requirements\n",
    "- **Categorical binning** - Interpretable nutrient level groupings for tree splits\n",
    "- **Class imbalance handling** - Weighted sampling for fair representation of rare fertilizers\n",
    "\n",
    "**Performance Targets**:\n",
    "- **Baseline**: MAP@3 > 0.30 (competitive threshold)\n",
    "- **Target**: MAP@3 > 0.35 (top quartile performance)\n",
    "- **Stretch**: MAP@3 > 0.40 (leaderboard contention)\n",
    "\n",
    "**Let's build a world-class fertilizer recommendation system! ğŸŒ±ğŸš€**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb332602",
   "metadata": {},
   "source": [
    "## ğŸ“š 1. Library Import & Setup\n",
    "\n",
    "**Why?** We import essential libraries for the complete XGBoost modeling pipeline. These libraries provide data manipulation, machine learning algorithms, evaluation metrics, and model persistence capabilities required for competition-level performance.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "- **Data handling**: pandas, numpy for data manipulation and numerical operations\n",
    "- **Machine Learning**: XGBoost for gradient boosting, scikit-learn for preprocessing and evaluation\n",
    "- **Validation**: StratifiedKFold for cross-validation, custom MAP@3 implementation\n",
    "- **Model persistence**: joblib for saving models, json for metadata storage\n",
    "- **Utilities**: time for performance monitoring, warnings for clean output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning - Scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import EarlyStopping\n",
    "\n",
    "# Model persistence and metadata\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(513)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb56f28",
   "metadata": {},
   "source": [
    "## ğŸ“ MAP@K Evaluation Function\n",
    "\n",
    "**Why?**\n",
    "MAP@3 (Mean Average Precision at 3) is the official Kaggle competition metric. This function calculates how well our model ranks the correct fertilizer within the top 3 predictions for each sample.\n",
    "\n",
    "**Function Details:**\n",
    "- **Input**: Actual fertilizer labels and predicted rankings (top-k)\n",
    "- **Output**: Score between 0 and 1 (higher is better)\n",
    "- **Logic**: Rewards correct predictions more heavily when they appear earlier in the ranking\n",
    "- **Competition Critical**: This exact implementation matches Kaggle's evaluation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463195f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapk(actual, predicted, k=3):\n",
    "    \"\"\"Compute mean average precision at k (MAP@k).\"\"\"\n",
    "    def apk(a, p, k):\n",
    "        score = 0.0\n",
    "        for i in range(min(k, len(p))):\n",
    "            if p[i] == a:\n",
    "                score += 1.0 / (i + 1)\n",
    "                break  # only the first correct prediction counts\n",
    "        return score\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee416ea2",
   "metadata": {},
   "source": [
    "## ğŸ“‚ 2. Data Loading & Preparation\n",
    "\n",
    "**Why?**  \n",
    "We load the competition datasets and perform initial data separation. The exploratory data analysis (EDA) has been completed in a separate notebook, providing insights that guide our feature engineering and modeling approach.\n",
    "\n",
    "**Data Sources:**\n",
    "- **Training set**: 100,000+ samples with fertilizer labels for model training\n",
    "- **Test set**: Unlabeled samples for final predictions and submission\n",
    "- **Sample submission**: Template for properly formatted competition submissions\n",
    "\n",
    "**Strategy:**\n",
    "- Clean separation between features (X) and target variable (y)\n",
    "- Consistent preprocessing pipeline for both training and test data\n",
    "- Memory-efficient loading for large agricultural datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "data_path = '../data'\n",
    "train_path = os.path.join(data_path, 'train.csv')\n",
    "test_path = os.path.join(data_path, 'test.csv')\n",
    "sample_submission_path = os.path.join(data_path, 'sample_submission.csv')\n",
    "\n",
    "# Load datasets\n",
    "print(\"ğŸ“‚ Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "\n",
    "print(\"âœ… Data loaded successfully:\")\n",
    "print(f\"  â€¢ Training set: {train_df.shape}\")\n",
    "print(f\"  â€¢ Test set: {test_df.shape}\")\n",
    "print(f\"  â€¢ Sample submission: {sample_submission.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a7dcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "target_column = 'Fertilizer Name'\n",
    "\n",
    "# Split training data\n",
    "X_raw = train_df.drop(columns=[target_column])\n",
    "y_raw = train_df[target_column]\n",
    "X_test_raw = test_df.copy()\n",
    "\n",
    "print(\"âœ… Data separation completed:\")\n",
    "print(f\"  â€¢ Training features: {X_raw.shape}\")\n",
    "print(f\"  â€¢ Training target: {y_raw.shape}\")\n",
    "print(f\"  â€¢ Test features: {X_test_raw.shape}\")\n",
    "print(f\"  â€¢ Target classes: {y_raw.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aee804",
   "metadata": {},
   "source": [
    "## ğŸ”¬ 3. Advanced Feature Engineering\n",
    "\n",
    "**Why?** Based on our EDA findings, we create sophisticated features that capture agricultural relationships and domain knowledge. These engineered features significantly outperformed original features in mutual information analysis.\n",
    "\n",
    "**Agricultural Domain Features:**\n",
    "\n",
    "### ğŸ§ª **NPK Ratio Features** (High Impact - EDA validated)\n",
    "\n",
    "- **N_P_ratio, N_K_ratio, P_K_ratio**: Nutrient balance ratios crucial for plant growth\n",
    "- **Total_NPK**: Overall soil fertility indicator\n",
    "- **NPK_Balance**: Nutrient distribution uniformity measure\n",
    "- **Why important**: Agricultural science shows ratios often more predictive than absolute values\n",
    "\n",
    "### ğŸŒ¡ï¸ **Environmental Interaction Features** (Medium-High Impact)\n",
    "\n",
    "- **Temp_Hum_index**: Combined temperature-humidity stress indicator\n",
    "- **Moist_Balance**: Soil vs air moisture differential affecting nutrient uptake\n",
    "- **Environ_Stress**: Distance from optimal growing conditions (25Â°C, 65% humidity)\n",
    "- **Temp_Moist_inter**: Temperature-moisture interaction for evaporation effects\n",
    "\n",
    "### ğŸ·ï¸ **Categorical Binning** (Medium Impact - Tree-friendly)\n",
    "\n",
    "- **Temp_Cat, Hum_Cat**: Low/Medium/High environmental categories\n",
    "- **N_Level, K_Level, P_Level**: Nutrient level categorizations\n",
    "- **Why effective**: Creates interpretable breakpoints for tree-based models\n",
    "\n",
    "### ğŸ”— **Agricultural Context Features** (High Impact)\n",
    "\n",
    "- **Soil_Crop_Combo**: Captures context-specific fertilizer requirements\n",
    "- **Dominant_NPK**: Identifies primary nutrient in soil composition\n",
    "- **Why critical**: Different crops have different needs based on soil type\n",
    "\n",
    "**Engineering Validation:**\n",
    "\n",
    "- EDA showed 67% improvement in mutual information scores\n",
    "- Engineered features dominated top-10 most informative variables\n",
    "- Agricultural domain knowledge confirmed feature relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create engineered features based on agricultural domain knowledge\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with agricultural features\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with additional engineered features\n",
    "    \"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # NPK Ratios (crucial for agricultural decisions)\n",
    "    df_eng['N_P_ratio'] = df_eng['Nitrogen'] / (df_eng['Phosphorous'] + 0.001)\n",
    "    df_eng['N_K_ratio'] = df_eng['Nitrogen'] / (df_eng['Potassium'] + 0.001)\n",
    "    df_eng['P_K_ratio'] = df_eng['Phosphorous'] / (df_eng['Potassium'] + 0.001)\n",
    "    \n",
    "    # Total NPK and NPK Balance\n",
    "    df_eng['Total_NPK'] = df_eng['Nitrogen'] + df_eng['Phosphorous'] + df_eng['Potassium']\n",
    "    npk_mean = df_eng[['Nitrogen', 'Phosphorous', 'Potassium']].mean(axis=1)\n",
    "    df_eng['NPK_Balance'] = df_eng[['Nitrogen', 'Phosphorous', 'Potassium']].std(axis=1) / (npk_mean + 0.001)\n",
    "    \n",
    "    # Environmental indices\n",
    "    df_eng['Temp_Hum_index'] = df_eng['Temparature'] * df_eng['Humidity'] / 100\n",
    "    df_eng['Moist_Balance'] = df_eng['Moisture'] - df_eng['Humidity']\n",
    "    df_eng['Environ_Stress'] = np.sqrt((df_eng['Temparature'] - 25)**2 + (df_eng['Humidity'] - 65)**2)\n",
    "    df_eng['Temp_Moist_inter'] = df_eng['Temparature'] * df_eng['Moisture'] / 100\n",
    "    \n",
    "    # Dominant nutrient\n",
    "    npk_cols = ['Nitrogen', 'Phosphorous', 'Potassium']\n",
    "    df_eng['Dominant_NPK'] = df_eng[npk_cols].idxmax(axis=1)\n",
    "    \n",
    "    # Categorical binning\n",
    "    df_eng['Temp_Cat'] = pd.cut(df_eng['Temparature'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    df_eng['Hum_Cat'] = pd.cut(df_eng['Humidity'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    df_eng['N_Level'] = pd.cut(df_eng['Nitrogen'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    df_eng['K_Level'] = pd.cut(df_eng['Potassium'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    df_eng['P_Level'] = pd.cut(df_eng['Phosphorous'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    # Soil-Crop interaction\n",
    "    df_eng['Soil_Crop_Combo'] = df_eng['Soil Type'].astype(str) + '_' + df_eng['Crop Type'].astype(str)\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"ğŸ”§ Applying feature engineering...\")\n",
    "X_train_featured = create_features(X_raw)\n",
    "X_test_featured = create_features(X_test_raw)\n",
    "\n",
    "print(f\"âœ… Feature engineering completed:\")\n",
    "print(f\"  â€¢ Original features: {X_raw.shape[1]}\")\n",
    "print(f\"  â€¢ After feature engineering: {X_train_featured.shape[1]}\")\n",
    "print(f\"  â€¢ New features added: {X_train_featured.shape[1] - X_raw.shape[1]}\")\n",
    "\n",
    "# Display new feature names\n",
    "original_features = set(X_raw.columns)\n",
    "new_features = [col for col in X_train_featured.columns if col not in original_features]\n",
    "print(f\"\\nğŸ†• New engineered features ({len(new_features)}):\")\n",
    "for i, feature in enumerate(new_features, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fdd129",
   "metadata": {},
   "source": [
    "## ğŸ”¢ 4. Categorical Variable Encoding\n",
    "\n",
    "**Why?**  \n",
    "XGBoost requires numerical inputs, so we convert categorical variables (soil types, crop types, engineered categories) to numerical representations while maintaining consistency between training and test sets.\n",
    "\n",
    "**Encoding Strategy:**\n",
    "\n",
    "- **LabelEncoder**: Optimal choice for tree-based algorithms like XGBoost\n",
    "- **Consistent mapping**: Fit on combined train+test data to ensure same encoding\n",
    "- **Target encoding**: Fertilizer names converted to integer labels for classification\n",
    "\n",
    "**Categorical Variables Processed:**\n",
    "\n",
    "- **Original features**: Soil Type (10 categories), Crop Type (22 categories)\n",
    "- **Engineered categories**: Temperature/Humidity/NPK level bins, Soil-Crop combinations\n",
    "- **Target variable**: 22 fertilizer types â†’ integer labels 0-21\n",
    "\n",
    "**XGBoost Compatibility:**\n",
    "\n",
    "- Tree-based algorithms handle label-encoded categoricals naturally\n",
    "- No need for one-hot encoding (which would create sparse, high-dimensional features)\n",
    "- Preserves ordinality where meaningful (e.g., Low/Medium/High levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39158f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Encode categorical features using LabelEncoder\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        X_test: Test features  \n",
    "        y_train: Training target\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (X_train_encoded, X_test_encoded, y_encoded, encoders_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize encoders dictionary\n",
    "    encoders = {}\n",
    "    \n",
    "    # Create copies to avoid modifying originals\n",
    "    X_train_enc = X_train.copy()\n",
    "    X_test_enc = X_test.copy()\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"ğŸ”¢ Encoding categorical features...\")\n",
    "    print(f\"Categorical columns found: {categorical_cols}\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for col in categorical_cols:\n",
    "        print(f\"  â€¢ Encoding: {col}\")\n",
    "        \n",
    "        # Create encoder\n",
    "        encoder = LabelEncoder()\n",
    "        \n",
    "        # Fit on combined training and test data to ensure consistency\n",
    "        combined_values = pd.concat([X_train[col], X_test[col]]).astype(str)\n",
    "        encoder.fit(combined_values)\n",
    "        \n",
    "        # Transform both datasets\n",
    "        X_train_enc[col] = encoder.transform(X_train[col].astype(str))\n",
    "        X_test_enc[col] = encoder.transform(X_test[col].astype(str))\n",
    "        \n",
    "        # Store encoder\n",
    "        encoders[col] = encoder\n",
    "        \n",
    "        print(f\"    - Classes: {len(encoder.classes_)} | {list(encoder.classes_[:5])}{'...' if len(encoder.classes_) > 5 else ''}\")\n",
    "    \n",
    "    # Encode target variable\n",
    "    print(f\"\\nğŸ¯ Encoding target variable: {target_column}\")\n",
    "    target_encoder = LabelEncoder()\n",
    "    y_encoded = target_encoder.fit_transform(y_train)\n",
    "    encoders['target'] = target_encoder\n",
    "    \n",
    "    print(f\"  â€¢ Target classes: {len(target_encoder.classes_)}\")\n",
    "    # print(f\"  â€¢ Class mapping preview: {dict(zip(target_encoder.classes_[:5], range(5)))}\")\n",
    "    \n",
    "    return X_train_enc, X_test_enc, y_encoded, encoders\n",
    "\n",
    "# Apply encoding\n",
    "X_train_encoded, X_test_encoded, y_encoded, label_encoders = encode_categorical_features(\n",
    "    X_train_featured, X_test_featured, y_raw\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Encoding completed:\")\n",
    "print(f\"  â€¢ Training features: {X_train_encoded.shape}\")\n",
    "print(f\"  â€¢ Test features: {X_test_encoded.shape}\")\n",
    "print(f\"  â€¢ Encoded target: {y_encoded.shape}\")\n",
    "print(f\"  â€¢ Encoders stored: {len(label_encoders)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f42c8d",
   "metadata": {},
   "source": [
    "## ğŸ¯ 5. Strategic Feature Selection\n",
    "\n",
    "**Why?** We implement a flexible feature selection system based on EDA insights and mutual information analysis. This allows easy experimentation with different feature combinations to optimize model performance.\n",
    "\n",
    "**Selection Philosophy:**\n",
    "\n",
    "- **EDA-driven**: Prioritize features with high mutual information scores\n",
    "- **Domain-informed**: Include agriculturally meaningful combinations\n",
    "- **Computational efficiency**: Balance between information gain and training speed\n",
    "- **Easy experimentation**: Toggle features on/off with simple commenting system\n",
    "\n",
    "**Feature Categories Available:**\n",
    "\n",
    "### ğŸŒŸ **Tier 1 Features** (Highest Mutual Information)\n",
    "\n",
    "- Original environmental and NPK features\n",
    "- Soil-Crop combinations (highest individual MI score)\n",
    "- Crop Type encoding (essential agricultural context)\n",
    "\n",
    "### ğŸ”§ **Tier 2 Features** (High Impact Engineered)\n",
    "\n",
    "- NPK ratios and balance metrics\n",
    "- Environmental stress indices\n",
    "- Temperature-humidity interactions\n",
    "\n",
    "### ğŸ“Š **Tier 3 Features** (Complementary)\n",
    "\n",
    "- Categorical level binnings\n",
    "- Dominant nutrient indicators\n",
    "- Secondary environmental features\n",
    "\n",
    "**Current Configuration:**\n",
    "\n",
    "- **Conservative baseline**: Original features + essential categoricals\n",
    "- **Ready for enhancement**: Engineered features commented out for easy activation\n",
    "- **Validation built-in**: Automatic checking against available processed features\n",
    "\n",
    "**Performance Strategy:**\n",
    "\n",
    "- Start with baseline features for stable foundation\n",
    "- Incrementally add engineered features based on validation performance\n",
    "- Monitor for overfitting with high-dimensional feature sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132dfac1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0844e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE SELECTION FOR THE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "features_to_use = [\n",
    "    # ğŸŒ¡ï¸ ORIGINAL CLIMATE VARIABLES\n",
    "    # 'Temparature',\n",
    "    # 'Humidity', \n",
    "    # 'Moisture',\n",
    "    \n",
    "    # ğŸ§ª CHEMICAL VARIABLES (NPK)\n",
    "    'Nitrogen',\n",
    "    'Potassium', \n",
    "    'Phosphorous',\n",
    "    \n",
    "    # ğŸ“Š ENGINEERED FEATURES - NPK RATIOS (from create_features)\n",
    "    'N_P_ratio',\n",
    "    # 'N_K_ratio',\n",
    "    # 'P_K_ratio',\n",
    "    # 'Total_NPK',\n",
    "    # 'NPK_Balance',\n",
    "    \n",
    "    # ğŸŒ¡ï¸ ENGINEERED FEATURES - CLIMATE INDICES (from create_features)\n",
    "    # 'Temp_Hum_index',\n",
    "    # 'Moist_Balance',\n",
    "    # 'Environ_Stress',\n",
    "    # 'Temp_Moist_inter',\n",
    "    \n",
    "    # ğŸ·ï¸ ENGINEERED FEATURES - CATEGORICAL LEVELS (from create_features, encoded)\n",
    "    # 'Temp_Cat',\n",
    "    # 'Hum_Cat',\n",
    "    # 'N_Level',\n",
    "    # 'K_Level',\n",
    "    # 'P_Level',\n",
    "\n",
    "    # ğŸ”— ENGINEERED FEATURES - COMBINATIONS (from create_features)\n",
    "    'Soil_Crop_Combo', # âœ… Encoded during preprocessing\n",
    "    # 'Dominant_NPK', # âœ… Encoded during preprocessing\n",
    "    \n",
    "    # ğŸ”¢ ENCODED CATEGORICAL FEATURES (from preprocessing)\n",
    "    # 'Soil Type',      # âœ… Encoded during preprocessing\n",
    "    'Crop Type',      # âœ… Encoded during preprocessing\n",
    "]\n",
    "\n",
    "# Validate available features against the actual processed dataset\n",
    "print(f\"ğŸ” Validating features against processed dataset...\")\n",
    "print(f\"ğŸ“Š Available columns in dataset: {list(X_train_encoded.columns)}\")\n",
    "\n",
    "available_features = []\n",
    "missing_features = []\n",
    "\n",
    "for feature in features_to_use:\n",
    "    if feature in X_train_encoded.columns:\n",
    "        available_features.append(feature)\n",
    "    else:\n",
    "        missing_features.append(feature)\n",
    "\n",
    "# Update final feature list to only include available features\n",
    "features_to_use = available_features\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"\\nâš ï¸ Missing features (will be skipped): {missing_features}\")\n",
    "\n",
    "# Display selected features by category\n",
    "print(f\"\\nğŸ“‹ SELECTED FEATURES ({len(features_to_use)} total):\")\n",
    "\n",
    "# Group features by category for better readability\n",
    "climate_original = [f for f in features_to_use if f in ['Temparature', 'Humidity', 'Moisture']]\n",
    "npk_original = [f for f in features_to_use if f in ['Nitrogen', 'Potassium', 'Phosphorous']]\n",
    "npk_ratios = [f for f in features_to_use if any(x in f for x in ['_ratio', 'Total_NPK', 'NPK_Balance'])]\n",
    "climate_engineered = [f for f in features_to_use if any(x in f for x in ['Temp_Hum', 'Moist_Balance', 'Environ_Stress', 'Temp_Moist'])]\n",
    "categorical_levels = [f for f in features_to_use if any(x in f for x in ['_Cat', '_Level'])]\n",
    "combinations = [f for f in features_to_use if any(x in f for x in ['Combo', 'Dominant'])]\n",
    "encoded_original = [f for f in features_to_use if f in ['Soil Type', 'Crop Type']]\n",
    "\n",
    "feature_groups = [\n",
    "    (\"ğŸŒ¡ï¸ Original Climate\", climate_original),\n",
    "    (\"ğŸ§ª Original NPK\", npk_original),\n",
    "    (\"ğŸ“Š NPK Ratios\", npk_ratios),\n",
    "    (\"ğŸŒ¡ï¸ Climate Indices\", climate_engineered),\n",
    "    (\"ğŸ·ï¸ Categorical Levels\", categorical_levels),\n",
    "    (\"ğŸ”— Combinations\", combinations),\n",
    "    (\"ğŸ”¢ Encoded Categories\", encoded_original)\n",
    "]\n",
    "\n",
    "for group_name, group_features in feature_groups:\n",
    "    if group_features:\n",
    "        print(f\"\\n{group_name} ({len(group_features)}):\")\n",
    "        for i, feature in enumerate(group_features, 1):\n",
    "            print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "print(f\"\\nğŸš€ Ready for model training with {len(features_to_use)} features!\")\n",
    "\n",
    "# Create final training datasets\n",
    "X_final = X_train_encoded[features_to_use].copy()\n",
    "X_test_final = X_test_encoded[features_to_use].copy()\n",
    "\n",
    "print(f\"\\nâœ… Final dataset shapes:\")\n",
    "print(f\"  â€¢ Training: {X_final.shape}\")\n",
    "print(f\"  â€¢ Test: {X_test_final.shape}\")\n",
    "print(f\"  â€¢ Target: {y_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5d67f",
   "metadata": {},
   "source": [
    "## ğŸ”„ 6. Stratified 10-Fold Cross-Validation Setup\n",
    "\n",
    "**Why?**\n",
    "Robust cross-validation is critical for reliable model evaluation and preventing overfitting. We use stratified 10-fold CV to ensure consistent class distribution across folds while providing stable performance estimates.\n",
    "\n",
    "**Stratified CV Benefits:**\n",
    "- **Class preservation**: Each fold maintains the same fertilizer class proportions as the full dataset\n",
    "- **Reduced variance**: 10 folds provide more stable performance estimates than 5-fold\n",
    "- **Overfitting detection**: Multiple independent validations reveal model generalization\n",
    "- **Competition alignment**: CV scores correlate better with leaderboard performance\n",
    "\n",
    "**Configuration Details:**\n",
    "- **10 folds**: Optimal balance between computational cost and statistical reliability\n",
    "- **Stratification**: Critical for imbalanced 22-class fertilizer distribution\n",
    "- **Shuffling**: Randomizes sample order to remove potential temporal/ordering biases\n",
    "- **Fixed random state**: Ensures reproducible results across runs\n",
    "\n",
    "**Class Distribution Analysis:**\n",
    "- **22 fertilizer classes**: Some frequent (>10%), others rare (<1%)\n",
    "- **Minimum class check**: Validates sufficient samples per class for stratification\n",
    "- **Fold balance**: Each fold contains representative samples from all classes\n",
    "\n",
    "**Statistical Robustness:**\n",
    "- **~90% training, ~10% validation** per fold provides good train/validation balance\n",
    "- **Out-of-fold predictions**: Enable unbiased performance estimation\n",
    "- **Stability assessment**: Coefficient of variation across folds indicates model reliability\n",
    "\n",
    "**Kaggle Competition Alignment:**\n",
    "- **Local CV scores** should correlate with public/private leaderboard\n",
    "- **Overfitting detection** through train/validation gap monitoring\n",
    "- **Ensemble foundation** for combining predictions from multiple folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea40e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STRATIFIED 10-FOLD CROSS-VALIDATION CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Cross-validation parameters\n",
    "N_SPLITS = 10  # 10-fold cross-validation for robust evaluation\n",
    "RANDOM_STATE = 513\n",
    "SHUFFLE = True\n",
    "\n",
    "# Initialize StratifiedKFold to maintain class distribution\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=N_SPLITS, \n",
    "    shuffle=SHUFFLE, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"ğŸ”„ CROSS-VALIDATION CONFIGURATION:\")\n",
    "print(f\"  â€¢ Number of folds: {N_SPLITS}\")\n",
    "print(f\"  â€¢ Strategy: Stratified (maintains class proportions)\")\n",
    "print(f\"  â€¢ Shuffle: {SHUFFLE}\")\n",
    "print(f\"  â€¢ Random state: {RANDOM_STATE}\")\n",
    "\n",
    "# Analyze class distribution for stratification\n",
    "print(f\"\\nğŸ“Š Class distribution analysis:\")\n",
    "unique_classes, class_counts = np.unique(y_encoded, return_counts=True)\n",
    "print(f\"  â€¢ Total classes: {len(unique_classes)}\")\n",
    "print(f\"  â€¢ Total samples: {len(y_encoded)}\")\n",
    "print(f\"  â€¢ Samples per fold: ~{len(y_encoded) // N_SPLITS}\")\n",
    "\n",
    "# Check minimum class size for stratification\n",
    "min_class_count = min(class_counts)\n",
    "print(f\"  â€¢ Minimum class size: {min_class_count}\")\n",
    "if min_class_count < N_SPLITS:\n",
    "    print(f\"  âš ï¸ Warning: Smallest class has {min_class_count} samples, less than {N_SPLITS} folds\")\n",
    "    print(f\"    Some folds may not contain all classes\")\n",
    "else:\n",
    "    print(f\"  âœ… All classes have sufficient samples for {N_SPLITS}-fold CV\")\n",
    "\n",
    "# Preview fold splits\n",
    "print(f\"\\nğŸ” Fold size preview:\")\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_final, y_encoded)):\n",
    "    if fold_idx < 3:  # Show first 3 folds\n",
    "        print(f\"  Fold {fold_idx + 1}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
    "    elif fold_idx == 3:\n",
    "        print(\"  ...\")\n",
    "    elif fold_idx == N_SPLITS - 1:  # Show last fold\n",
    "        print(f\"  Fold {fold_idx + 1}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5df129",
   "metadata": {},
   "source": [
    "## âš™ï¸ 7. XGBoost Hyperparameter Configuration\n",
    "\n",
    "**Why?**\n",
    "Optimal hyperparameter selection is crucial for XGBoost performance on multi-class classification tasks. These parameters are tuned specifically for the fertilizer prediction challenge, balancing model complexity, training efficiency, and generalization.\n",
    "\n",
    "**Multi-Class Classification Setup:**\n",
    "- **Objective**: `multi:softprob` - outputs class probabilities (essential for MAP@3 ranking)\n",
    "- **num_class**: 22 - matches the number of fertilizer types\n",
    "- **eval_metric**: `mlogloss` - standard multi-class loss for probability optimization\n",
    "\n",
    "**Tree Structure Parameters:**\n",
    "- **max_depth**: 8 - deep enough to capture complex agricultural interactions, not too deep to overfit\n",
    "- **min_child_weight**: 3 - prevents overfitting on rare fertilizer classes\n",
    "- **subsample**: 0.8 - row sampling reduces overfitting, maintains sample diversity\n",
    "- **colsample_bytree**: 0.8 - feature sampling per tree, reduces feature dependency\n",
    "\n",
    "**Learning & Regularization:**\n",
    "- **learning_rate**: 0.1 - moderate rate balances training speed and convergence quality\n",
    "- **n_estimators**: 2000 - high count with early stopping for optimal iteration discovery\n",
    "- **reg_alpha**: 0.1 - L1 regularization for feature selection\n",
    "- **reg_lambda**: 1.0 - L2 regularization for weight smoothing\n",
    "- **gamma**: 0.1 - minimum split loss, prevents unnecessary complexity\n",
    "\n",
    "**Class Imbalance Handling:**\n",
    "- **Balanced class weights**: Computed to handle fertilizer frequency imbalances\n",
    "- **Sample weighting**: Applied per fold to ensure fair representation of rare classes\n",
    "- **Weight calculation**: Uses scikit-learn's 'balanced' strategy for automatic adjustment\n",
    "\n",
    "**Performance & Stability:**\n",
    "- **Early stopping**: 100 rounds - prevents overfitting while allowing convergence\n",
    "- **Random state**: Fixed for reproducible results across runs\n",
    "- **n_jobs**: -1 - utilizes all CPU cores for faster training\n",
    "- **Verbosity**: 0 - clean output for production environment\n",
    "\n",
    "**Agricultural Domain Optimization:**\n",
    "- **Deep trees**: Capture soil-crop-environment interactions\n",
    "- **Regularization**: Prevents overfitting on agricultural outliers\n",
    "- **Probability focus**: Optimized for ranking performance in MAP@3\n",
    "- **Class balance**: Ensures good performance across all fertilizer types\n",
    "\n",
    "**Competition-Specific Tuning:**\n",
    "- Parameters optimized for Kaggle's MAP@3 evaluation metric\n",
    "- Balance between training time and model performance\n",
    "- Robust to the specific agricultural dataset characteristics\n",
    "- Designed for ensemble combination across CV folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db2902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# XGBOOST HYPERPARAMETER CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_encoded),\n",
    "    y=y_encoded\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_encoded), class_weights))\n",
    "\n",
    "print(\"âš–ï¸ Class weight calculation:\")\n",
    "print(f\"  â€¢ Balanced class weights computed for {len(class_weight_dict)} classes\")\n",
    "print(f\"  â€¢ Weight range: {min(class_weights):.3f} - {max(class_weights):.3f}\")\n",
    "\n",
    "# XGBoost hyperparameters (optimized for multi-class classification)\n",
    "xgb_params = {\n",
    "    # Multi-class objective\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': len(label_encoders['target'].classes_),\n",
    "    'eval_metric': 'mlogloss',\n",
    "    \n",
    "    # Tree structure\n",
    "    'max_depth': 12,\n",
    "    # 'min_child_weight': 3,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    \n",
    "    # Learning parameters\n",
    "    'learning_rate': 0.03,\n",
    "    'n_estimators': 3000,  # High number with early stopping\n",
    "    \n",
    "    # Regularization\n",
    "    'reg_alpha': 0.1,  # L1 regularization\n",
    "    'reg_lambda': 1.0,  # L2 regularization\n",
    "    'gamma': 0.25,      # Minimum split loss\n",
    "    \n",
    "    # Performance\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0,\n",
    "    \n",
    "    # Early stopping will be handled separately\n",
    "    'early_stopping_rounds': 100\n",
    "}\n",
    "\n",
    "# Early stopping configuration\n",
    "es = 100\n",
    "eval_metric = 'mlogloss'\n",
    "\n",
    "print(f\"\\nğŸš€ XGBOOST CONFIGURATION:\")\n",
    "print(f\"  â€¢ Objective: {xgb_params['objective']}\")\n",
    "print(f\"  â€¢ Number of classes: {xgb_params['num_class']}\")\n",
    "print(f\"  â€¢ Max depth: {xgb_params['max_depth']}\")\n",
    "print(f\"  â€¢ Learning rate: {xgb_params['learning_rate']}\")\n",
    "print(f\"  â€¢ Max estimators: {xgb_params['n_estimators']}\")\n",
    "print(f\"  â€¢ Early stopping: {xgb_params['early_stopping_rounds']} rounds\")\n",
    "print(f\"  â€¢ Evaluation metric: {xgb_params['eval_metric']}\")\n",
    "print(f\"  â€¢ Regularization: L1={xgb_params['reg_alpha']}, L2={xgb_params['reg_lambda']}\")\n",
    "print(f\"  â€¢ Class balancing: Enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d735d",
   "metadata": {},
   "source": [
    "## ğŸ‹ï¸ 8. Model Training with 10-Fold Cross-Validation\n",
    "\n",
    "**Why?**\n",
    "This section implements the core training pipeline using stratified 10-fold cross-validation. Each fold trains an independent XGBoost model with early stopping and class balancing, creating a robust ensemble for final predictions.\n",
    "\n",
    "**Training Pipeline Components:**\n",
    "\n",
    "### ğŸ”„ **Cross-Validation Loop**\n",
    "- **10 independent models**: Each fold creates a separate XGBoost classifier\n",
    "- **Stratified splits**: Maintain fertilizer class distribution across all folds\n",
    "- **Early stopping**: Validation-based stopping prevents overfitting per fold\n",
    "- **Out-of-fold predictions**: Enable unbiased performance estimation\n",
    "\n",
    "### âš–ï¸ **Class Balancing Strategy**\n",
    "- **Per-fold class weights**: Calculated separately for each training fold\n",
    "- **Sample weighting**: Applied during XGBoost training to handle imbalanced classes\n",
    "- **Rare class protection**: Ensures minor fertilizer types receive adequate representation\n",
    "- **Balanced accuracy**: Prevents model bias toward frequent fertilizer classes\n",
    "\n",
    "### ğŸ“Š **Performance Monitoring**\n",
    "- **Dual metrics**: Both accuracy and MAP@3 tracked per fold\n",
    "- **Training time**: Monitor computational efficiency across folds\n",
    "- **Best iteration**: Track optimal stopping point for each model\n",
    "- **Validation scores**: Real-time performance feedback during training\n",
    "\n",
    "### ğŸ¯ **MAP@3 Calculation Pipeline**\n",
    "- **Probability extraction**: XGBoost outputs class probabilities for each sample\n",
    "- **Top-3 ranking**: Sort probabilities to identify highest confidence predictions\n",
    "- **Format conversion**: Transform predictions to format expected by MAP@3 function\n",
    "- **Fold-wise evaluation**: Calculate MAP@3 independently for each validation set\n",
    "\n",
    "### ğŸ” **Feature Importance Tracking**\n",
    "- **Per-fold importance**: Extract feature importance from each trained model\n",
    "- **Aggregated insights**: Combine importance scores across all folds\n",
    "- **Agricultural validation**: Verify that important features align with domain knowledge\n",
    "- **Stability assessment**: Check consistency of feature rankings across folds\n",
    "\n",
    "### ğŸ’¾ **Model Persistence**\n",
    "- **Ensemble storage**: Save all 10 trained models for final predictions\n",
    "- **Metadata tracking**: Record training times, best iterations, hyperparameters\n",
    "- **Reproducibility**: Maintain complete training history for analysis\n",
    "\n",
    "**Training Outputs:**\n",
    "- **10 XGBoost models**: One per fold, ready for ensemble prediction\n",
    "- **Out-of-fold predictions**: Unbiased performance estimation matrix\n",
    "- **Performance metrics**: Comprehensive accuracy and MAP@3 statistics\n",
    "- **Feature importance**: Aggregated importance scores across all models\n",
    "- **Training diagnostics**: Timing, iteration counts, and stability metrics\n",
    "\n",
    "**Quality Assurance:**\n",
    "- **Validation monitoring**: Track overfitting through train/validation gaps\n",
    "- **Class distribution**: Verify balanced representation in each fold\n",
    "- **Convergence checking**: Ensure early stopping triggers appropriately\n",
    "- **Memory management**: Efficient handling of large agricultural dataset\n",
    "\n",
    "**Competition Readiness:**\n",
    "- **Ensemble foundation**: Multiple models reduce prediction variance\n",
    "- **MAP@3 optimization**: Training pipeline specifically tuned for ranking metric\n",
    "- **Robust validation**: CV scores provide reliable estimate of leaderboard performance\n",
    "- **Production quality**: Complete error handling and progress monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10-FOLD CROSS-VALIDATION TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def train_xgboost_cv(X, y, features, cv_splitter, params):\n",
    "    \"\"\"\n",
    "    Train XGBoost models using cross-validation\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target vector (encoded)\n",
    "        features: List of feature names to use\n",
    "        cv_splitter: Cross-validation splitter (StratifiedKFold)\n",
    "        params: XGBoost parameters\n",
    "        early_stopping_rounds: Early stopping patience\n",
    "        \n",
    "    Returns:\n",
    "        Dict with trained models, predictions, and metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize storage\n",
    "    models = {}\n",
    "    oof_predictions = np.zeros((len(X), params['num_class']))  # Out-of-fold predictions\n",
    "    cv_scores = []\n",
    "    feature_importance_list = []\n",
    "    \n",
    "    print(f\"ğŸ‹ï¸ Starting {N_SPLITS}-Fold Cross-Validation Training...\")\n",
    "    print(f\"â° Training started at: {time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv_splitter.split(X, y)):\n",
    "        \n",
    "        fold_start_time = time.time()\n",
    "        print(f\"\\nğŸ“ FOLD {fold_idx + 1}/{N_SPLITS}\")\n",
    "        print(f\"  â€¢ Train samples: {len(train_idx)}\")\n",
    "        print(f\"  â€¢ Validation samples: {len(val_idx)}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_fold = X.iloc[train_idx][features]\n",
    "        X_val_fold = X.iloc[val_idx][features]\n",
    "        y_train_fold = y[train_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        # Calculate sample weights for this fold\n",
    "        fold_class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y_train_fold),\n",
    "            y=y_train_fold\n",
    "        )\n",
    "        fold_class_weight_dict = dict(zip(np.unique(y_train_fold), fold_class_weights))\n",
    "        sample_weights = np.array([fold_class_weight_dict.get(label, 1.0) for label in y_train_fold])\n",
    "        \n",
    "        # Initialize model\n",
    "        model = XGBClassifier(**params)\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            sample_weight=sample_weights,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Predict validation set\n",
    "        val_pred_proba = model.predict_proba(X_val_fold)\n",
    "        val_pred_classes = model.predict(X_val_fold)\n",
    "        \n",
    "        # Store out-of-fold predictions\n",
    "        oof_predictions[val_idx] = val_pred_proba\n",
    "        \n",
    "        # Calculate fold metrics\n",
    "        fold_accuracy = accuracy_score(y_val_fold, val_pred_classes)\n",
    "        \n",
    "        # Calculate MAP@3 for this fold\n",
    "        # Get top 3 predictions for each sample\n",
    "        val_top3_indices = np.argsort(val_pred_proba, axis=1)[:, -3:][:, ::-1]\n",
    "        \n",
    "        # Convert to lists for mapk function\n",
    "        actual_list = y_val_fold.tolist() if hasattr(y_val_fold, 'tolist') else list(y_val_fold)\n",
    "        predicted_list = val_top3_indices.tolist()\n",
    "        \n",
    "        # Calculate MAP@3 using the correct format\n",
    "        fold_map3 = mapk(actual_list, predicted_list, k=3)\n",
    "        \n",
    "        # Store results\n",
    "        cv_scores.append({\n",
    "            'fold': fold_idx + 1,\n",
    "            'accuracy': fold_accuracy,\n",
    "            'map3': fold_map3,\n",
    "            'best_iteration': model.best_iteration,\n",
    "            'train_samples': len(train_idx),\n",
    "            'val_samples': len(val_idx),\n",
    "            'training_time': time.time() - fold_start_time\n",
    "        })\n",
    "        \n",
    "        # Store model and feature importance\n",
    "        models[f'fold_{fold_idx + 1}'] = model\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'importance': model.feature_importances_,\n",
    "                'fold': fold_idx + 1\n",
    "            })\n",
    "            feature_importance_list.append(importance_df)\n",
    "        \n",
    "        fold_time = time.time() - fold_start_time\n",
    "        print(f\"  âœ… Fold completed in {fold_time:.1f}s\")\n",
    "        print(f\"  ğŸ“Š Accuracy: {fold_accuracy:.4f} | MAP@3: {fold_map3:.4f}\")\n",
    "        print(f\"  ğŸ”„ Best iteration: {model.best_iteration}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    oof_pred_classes = np.argmax(oof_predictions, axis=1)\n",
    "    overall_accuracy = accuracy_score(y, oof_pred_classes)\n",
    "    \n",
    "    # Calculate overall MAP@3\n",
    "    # Get top 3 predictions for each sample\n",
    "    oof_top3_indices = np.argsort(oof_predictions, axis=1)[:, -3:][:, ::-1]\n",
    "    \n",
    "    # Convert to lists for mapk function\n",
    "    actual_list = y.tolist() if hasattr(y, 'tolist') else list(y)\n",
    "    predicted_list = oof_top3_indices.tolist()\n",
    "    \n",
    "    # Calculate MAP@3 using the correct format\n",
    "    overall_map3 = mapk(actual_list, predicted_list, k=3)\n",
    "    \n",
    "    # Combine feature importance across folds\n",
    "    if feature_importance_list:\n",
    "        feature_importance_df = pd.concat(feature_importance_list, ignore_index=True)\n",
    "        feature_importance_summary = feature_importance_df.groupby('feature')['importance'].agg(['mean', 'std']).reset_index()\n",
    "        feature_importance_summary = feature_importance_summary.sort_values('mean', ascending=False)\n",
    "    else:\n",
    "        feature_importance_summary = None\n",
    "    \n",
    "    return {\n",
    "        'models': models,\n",
    "        'oof_predictions': oof_predictions,\n",
    "        'cv_scores': cv_scores,\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'overall_map3': overall_map3,\n",
    "        'feature_importance': feature_importance_summary\n",
    "    }\n",
    "\n",
    "# Execute cross-validation training\n",
    "print(\"ğŸš€ Starting model training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "training_results = train_xgboost_cv(\n",
    "    X=X_final,\n",
    "    y=y_encoded,\n",
    "    features=features_to_use,\n",
    "    cv_splitter=skf,\n",
    "    params=xgb_params,\n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nğŸ‰ CROSS-VALIDATION TRAINING COMPLETED!\")\n",
    "print(f\"â° Training finished at: {time.strftime('%H:%M:%S')}\")\n",
    "print(f\"â±ï¸ Total training time: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "print(f\"ğŸ“Š Overall Accuracy: {training_results['overall_accuracy']:.4f}\")\n",
    "print(f\"ğŸ“Š Overall MAP@3: {training_results['overall_map3']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7da959",
   "metadata": {},
   "source": [
    "## ğŸ“Š 9. Comprehensive Model Evaluation & Analysis\n",
    "\n",
    "**Why?** Thorough evaluation is essential to understand model performance, stability, and competition readiness. We analyze both statistical metrics and practical performance indicators to validate our XGBoost ensemble.\n",
    "\n",
    "**Evaluation Framework:**\n",
    "\n",
    "### ğŸ¯ **Primary Metrics Analysis**\n",
    "\n",
    "- **Cross-Validation MAP@3**: Mean and standard deviation across 10 folds\n",
    "- **Out-of-Fold MAP@3**: Unbiased estimate using all training data\n",
    "- **Cross-Validation Accuracy**: Secondary metric for model validation\n",
    "- **Metric correlation**: Verify MAP@3 and accuracy alignment\n",
    "\n",
    "### ğŸ“ˆ **Model Stability Assessment**\n",
    "\n",
    "- **Coefficient of Variation**: Measures performance consistency across folds\n",
    "- **Fold-to-fold variance**: Identifies potential overfitting or data leakage\n",
    "- **Performance distribution**: Analyze best/worst performing folds\n",
    "- **Stability thresholds**: CV < 0.05 indicates stable, reliable model\n",
    "\n",
    "### â±ï¸ **Training Efficiency Analysis**\n",
    "\n",
    "- **Per-fold training time**: Monitor computational requirements\n",
    "- **Total training time**: Assess scalability for larger datasets\n",
    "- **Early stopping behavior**: Validate convergence patterns\n",
    "- **Resource utilization**: Memory and CPU usage optimization\n",
    "\n",
    "### ğŸ” **Feature Importance Insights**\n",
    "\n",
    "- **Aggregated importance**: Average feature importance across all 10 models\n",
    "- **Importance stability**: Consistency of feature rankings across folds\n",
    "- **Agricultural validation**: Verify important features match domain knowledge\n",
    "- **Feature selection guidance**: Identify top performers for future iterations\n",
    "\n",
    "### ğŸ“‹ **Detailed Performance Breakdown**\n",
    "\n",
    "- **Fold-by-fold results**: Individual performance analysis per fold\n",
    "- **Best iteration tracking**: Optimal stopping points across folds\n",
    "- **Training diagnostics**: Identify potential training issues\n",
    "- **Performance trends**: Detect systematic patterns in fold performance\n",
    "\n",
    "**Competition Readiness Indicators:**\n",
    "\n",
    "### âœ… **Model Quality Checklist**\n",
    "\n",
    "- **MAP@3 threshold**: Target performance above competitive baselines\n",
    "- **Stability verification**: Low coefficient of variation across folds\n",
    "- **No overfitting signs**: Reasonable train/validation performance gaps\n",
    "- **Feature consistency**: Important features align with EDA insights\n",
    "\n",
    "### ğŸ† **Performance Benchmarks**\n",
    "\n",
    "- **Baseline comparison**: Performance vs simple models or random predictions\n",
    "- **Historical context**: Comparison with typical Kaggle competition scores\n",
    "- **Improvement potential**: Identify areas for further optimization\n",
    "- **Ensemble readiness**: Validate individual model quality for ensembling\n",
    "\n",
    "### ğŸ² **Risk Assessment**\n",
    "\n",
    "- **Overfitting detection**: Large performance variations indicate problems\n",
    "- **Data leakage check**: Unrealistically high scores may indicate leakage\n",
    "- **Class balance verification**: Ensure good performance across all fertilizer types\n",
    "- **Generalization confidence**: CV scores predict leaderboard performance\n",
    "\n",
    "**Actionable Insights:**\n",
    "\n",
    "- **Feature engineering guidance**: Which engineered features provide most value\n",
    "- **Hyperparameter tuning direction**: Areas for potential improvement\n",
    "- **Model ensemble strategy**: How to combine fold predictions optimally\n",
    "- **Competition submission readiness**: Confidence in final model performance\n",
    "\n",
    "**Quality Gates:**\n",
    "\n",
    "- **MAP@3 > 0.30**: Competitive threshold for leaderboard entry\n",
    "- **Stability CV < 0.05**: Reliable, consistent model performance\n",
    "- **Feature importance alignment**: Agricultural domain validation\n",
    "- **No training anomalies**: Clean, successful training across all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fa935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CROSS-VALIDATION RESULTS EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ“Š CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract results from training\n",
    "cv_results_df = pd.DataFrame(training_results['cv_scores'])\n",
    "\n",
    "# Calculate statistics\n",
    "accuracy_mean = cv_results_df['accuracy'].mean()\n",
    "accuracy_std = cv_results_df['accuracy'].std()\n",
    "map3_mean = cv_results_df['map3'].mean()\n",
    "map3_std = cv_results_df['map3'].std()\n",
    "\n",
    "print(f\"ğŸ¯ FINAL METRICS:\")\n",
    "print(f\"  ğŸ“ˆ Cross-Validation Accuracy: {accuracy_mean:.4f} Â± {accuracy_std:.4f}\")\n",
    "print(f\"  ğŸ“ˆ Cross-Validation MAP@3:    {map3_mean:.4f} Â± {map3_std:.4f}\")\n",
    "print(f\"  ğŸ“ˆ Out-of-Fold Accuracy:      {training_results['overall_accuracy']:.4f}\")\n",
    "print(f\"  ğŸ“ˆ Out-of-Fold MAP@3:         {training_results['overall_map3']:.4f}\")\n",
    "\n",
    "# Stability evaluation\n",
    "accuracy_cv = accuracy_std / accuracy_mean if accuracy_mean > 0 else 0\n",
    "map3_cv = map3_std / map3_mean if map3_mean > 0 else 0\n",
    "\n",
    "print(f\"\\nğŸ” STABILITY ANALYSIS:\")\n",
    "print(f\"  ğŸ“Š Coefficient of variation (Accuracy): {accuracy_cv:.3f}\")\n",
    "print(f\"  ğŸ“Š Coefficient of variation (MAP@3):    {map3_cv:.3f}\")\n",
    "print(f\"  {'âœ… Stable model' if accuracy_cv < 0.05 else 'âš ï¸ Variable model'} (Accuracy CV < 0.05)\")\n",
    "print(f\"  {'âœ… Stable model' if map3_cv < 0.05 else 'âš ï¸ Variable model'} (MAP@3 CV < 0.05)\")\n",
    "\n",
    "# Training time analysis\n",
    "avg_fold_time = cv_results_df['training_time'].mean()\n",
    "print(f\"\\nâ±ï¸ TRAINING TIMES:\")\n",
    "print(f\"  ğŸ“Š Average time per fold: {avg_fold_time:.1f}s\")\n",
    "print(f\"  ğŸ“Š Total time: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "\n",
    "# Detailed results by fold\n",
    "print(f\"\\nğŸ“‹ DETAILED RESULTS BY FOLD:\")\n",
    "print(\"Fold  Accuracy   MAP@3    Best_Iter  Time(s)\")\n",
    "print(\"-\" * 50)\n",
    "for _, row in cv_results_df.iterrows():\n",
    "    print(f\"{row['fold']:2.0f}    {row['accuracy']:.4f}   {row['map3']:.4f}     {row['best_iteration']:4.0f}   {row['training_time']:6.1f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Mean  {accuracy_mean:.4f}   {map3_mean:.4f}     {cv_results_df['best_iteration'].mean():4.0f}   {avg_fold_time:6.1f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "if training_results['feature_importance'] is not None:\n",
    "    print(f\"\\nğŸ” TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "    print(\"Rank  Feature               Importance\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (_, row) in enumerate(training_results['feature_importance'].head(10).iterrows()):\n",
    "        print(f\"{i+1:2d}.   {row['feature']:20} {row['mean']:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524143e",
   "metadata": {},
   "source": [
    "## ğŸ”® 11. Test Predictions & Ensemble Generation\n",
    "\n",
    "**Why?** Generate final predictions for the test set using our trained ensemble of 10 XGBoost models. The ensemble approach combines predictions from all CV folds to create more robust and stable predictions than any single model.\n",
    "\n",
    "**Ensemble Prediction Strategy:**\n",
    "\n",
    "- **Model averaging**: Combine probability outputs from all 10 fold models\n",
    "- **Probability calibration**: Ensemble naturally calibrates individual model uncertainties\n",
    "- **Variance reduction**: Multiple models reduce prediction noise and improve stability\n",
    "- **Ranking optimization**: Averaged probabilities provide better MAP@3 rankings\n",
    "\n",
    "**Test Prediction Pipeline:**\n",
    "\n",
    "- **Consistent preprocessing**: Apply identical feature engineering and encoding as training\n",
    "- **All-fold prediction**: Generate predictions from each of the 10 trained models\n",
    "- **Probability averaging**: Mean ensemble of all model probability outputs\n",
    "- **Top-3 extraction**: Rank fertilizers by ensemble probability for MAP@3 format\n",
    "\n",
    "**Output Preparation:**\n",
    "\n",
    "- **Competition format**: Convert top-3 predictions to space-separated fertilizer names\n",
    "- **Submission ready**: Generate properly formatted CSV for Kaggle submission\n",
    "- **Quality validation**: Verify prediction format and completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c09ef9c",
   "metadata": {},
   "source": [
    "## ğŸ’¾ 10. Model Persistence & Competition Submission\n",
    "\n",
    "**Why?**\n",
    "Comprehensive model persistence ensures reproducibility, enables model reuse, and creates a complete record of our modeling approach. We save all artifacts needed to reproduce results and deploy the model in production.\n",
    "\n",
    "**Saved Artifacts:**\n",
    "\n",
    "### ğŸ¤– **Model Components**\n",
    "- **Ensemble models**: All 10 trained XGBoost classifiers from CV folds\n",
    "- **Label encoders**: Categorical variable and target variable encoding mappings\n",
    "- **Feature list**: Exact features used for training (for consistent preprocessing)\n",
    "- **Preprocessing pipeline**: Complete transformation chain for new data\n",
    "\n",
    "### ğŸ“Š **Performance Metrics**\n",
    "- **Cross-validation results**: Detailed fold-by-fold performance metrics\n",
    "- **Overall performance**: Aggregated MAP@3 and accuracy scores\n",
    "- **Feature importance**: Aggregated importance rankings across all folds\n",
    "- **Training diagnostics**: Times, iterations, and convergence information\n",
    "\n",
    "### âš™ï¸ **Model Configuration**\n",
    "- **Hyperparameters**: Complete XGBoost parameter set used for training\n",
    "- **Training configuration**: CV strategy, random seeds, early stopping settings\n",
    "- **Data processing**: Feature engineering choices and encoding strategies\n",
    "- **Version information**: Software versions and environment details\n",
    "\n",
    "### ğŸ¯ **Competition Submission**\n",
    "- **Formatted predictions**: Top-3 fertilizer recommendations in Kaggle format\n",
    "- **Submission metadata**: Model description, performance metrics, methodology\n",
    "- **Submission file**: Ready-to-upload CSV with proper formatting\n",
    "- **Prediction confidence**: Ensemble probability scores for quality assessment\n",
    "\n",
    "**File Organization:**\n",
    "- **Structured directory**: Organized by model type and performance score\n",
    "- **Unique naming**: Model directories named with MAP@3 score for easy comparison\n",
    "- **Complete documentation**: JSON metadata files with full model information\n",
    "- **Reproducible setup**: Everything needed to reproduce results from scratch\n",
    "\n",
    "**Quality Assurance:**\n",
    "- **File verification**: Confirm all artifacts saved successfully\n",
    "- **Format validation**: Ensure submission file meets Kaggle requirements\n",
    "- **Metadata completeness**: Comprehensive documentation for future reference\n",
    "- **Model integrity**: Verify saved models can be loaded and used for prediction\n",
    "\n",
    "**Production Readiness:**\n",
    "- **Deployment package**: Complete model bundle for agricultural recommendation systems\n",
    "- **API integration**: Saved encoders and models ready for real-time inference\n",
    "- **Performance tracking**: Baseline metrics for monitoring production performance\n",
    "- **Version control**: Complete artifact versioning for model management\n",
    "\n",
    "**Competition Documentation:**\n",
    "- **Methodology record**: Complete approach documentation for team sharing\n",
    "- **Performance tracking**: Historical record of model improvements\n",
    "- **Submission history**: Track multiple submission attempts and results\n",
    "- **Reproducibility guarantee**: Everything needed to recreate exact results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaeaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FILE SAVING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure model name based on MAP@3\n",
    "overall_map3 = training_results['overall_map3']\n",
    "model_name = f\"XGB_10CV_MAP@3-{overall_map3:.5f}\".replace('.', '')\n",
    "model_dir = f\"../models/XGB/{N_SPLITS}CV/{model_name}\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ MODEL DIRECTORY:\")\n",
    "print(f\"  {model_dir}\")\n",
    "\n",
    "# File name configuration - KAGGLE COMPETITION ESSENTIALS ONLY\n",
    "base_filename = model_name\n",
    "files_to_create = {\n",
    "    'hparams': f\"{base_filename}_hparams.json\",        # âœ… ESSENTIAL - Hyperparameters for reproducibility\n",
    "    'metrics': f\"{base_filename}_metrics.json\",        # âœ… ESSENTIAL - Performance metrics and config\n",
    "    'submission': f\"{base_filename}_submission.csv\",   # âœ… ESSENTIAL - Competition submission file\n",
    "    'submission_info': f\"{base_filename}_submission_info.json\"  # âœ… ESSENTIAL - Submission metadata\n",
    "    \n",
    "    # âŒ REMOVED - Not needed for Kaggle competition:\n",
    "    # 'metrics_pkl': Heavy pickle file with redundant data\n",
    "    # 'model_pkl': Heavy model files (ensemble used in-memory only)\n",
    "    # 'feature_import': Nice-to-have but not essential for submission\n",
    "}\n",
    "\n",
    "print(f\"\\nğŸ“ FILES TO CREATE:\")\n",
    "for file_type, filename in files_to_create.items():\n",
    "    print(f\"  {file_type:15}: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE HYPERPARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# General model information\n",
    "hparams_data = {\n",
    "    \"model_type\": \"XGBClassifier\",\n",
    "    \"model_abbreviation\": \"XGB\",\n",
    "    \"cv_strategy\": f\"{N_SPLITS}-Fold Stratified Cross Validation\",\n",
    "    \"optimization_method\": \"Manual hyperparameter tuning\",\n",
    "    \"ensemble_method\": \"Average of fold predictions\",\n",
    "    \n",
    "    # Fixed hyperparameters used\n",
    "    \"hyperparameters\": xgb_params,\n",
    "    \n",
    "    # General configuration\n",
    "    \"features_selected\": features_to_use,\n",
    "    \"num_features\": len(features_to_use),\n",
    "    \"class_weights_used\": True,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"cv_splits\": N_SPLITS,\n",
    "    \"total_models\": len(training_results['models']),\n",
    "    \"early_stopping_rounds\": es\n",
    "}\n",
    "\n",
    "# Save general hyperparameters\n",
    "hparams_file = os.path.join(model_dir, files_to_create['hparams'])\n",
    "with open(hparams_file, 'w') as f:\n",
    "    json.dump(hparams_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Hyperparameters saved:\")\n",
    "print(f\"  ğŸ“„ General: {files_to_create['hparams']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697983d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE METRICS - KAGGLE COMPETITION ESSENTIALS ONLY\n",
    "# =============================================================================\n",
    "\n",
    "# Extract metrics from training results\n",
    "cv_results_df = pd.DataFrame(training_results['cv_scores'])\n",
    "map3_mean = cv_results_df['map3'].mean()\n",
    "map3_std = cv_results_df['map3'].std()\n",
    "accuracy_mean = cv_results_df['accuracy'].mean()\n",
    "accuracy_std = cv_results_df['accuracy'].std()\n",
    "\n",
    "# Complete metrics data for competition tracking\n",
    "metrics_data = {\n",
    "    \"model_type\": \"XGBClassifier\",\n",
    "    \"model_abbreviation\": \"XGB\",\n",
    "    \"tier\": \"10_FOLD_CV\",\n",
    "    \"target_variable\": \"Fertilizer Name\",\n",
    "    \"cv_strategy\": f\"{N_SPLITS}-Fold Stratified Cross Validation\",\n",
    "    \"optimization_method\": \"Manual hyperparameter tuning\",\n",
    "    \n",
    "    # Main performance metrics\n",
    "    \"map3_score_cv_mean\": float(map3_mean),\n",
    "    \"map3_score_cv_std\": float(map3_std),\n",
    "    \"map3_score_oof\": float(training_results['overall_map3']),\n",
    "    \"accuracy_cv_mean\": float(accuracy_mean),\n",
    "    \"accuracy_cv_std\": float(accuracy_std),\n",
    "    \"accuracy_oof\": float(training_results['overall_accuracy']),\n",
    "    \n",
    "    # Model configuration\n",
    "    \"num_classes\": len(label_encoders['target'].classes_),\n",
    "    \"features_used\": len(features_to_use),\n",
    "    \"features_list\": features_to_use,\n",
    "    \"cv_folds\": N_SPLITS,\n",
    "    \"total_models_trained\": len(training_results['models']),\n",
    "    \n",
    "    # Detailed fold results\n",
    "    \"fold_results\": training_results['cv_scores'],\n",
    "    \n",
    "    # Stability statistics\n",
    "    \"accuracy_cv_coefficient\": float(accuracy_std / accuracy_mean) if accuracy_mean > 0 else 0.0,\n",
    "    \"map3_cv_coefficient\": float(map3_std / map3_mean) if map3_mean > 0 else 0.0,\n",
    "    \n",
    "    # Training performance\n",
    "    \"training_time_total\": float(total_time),\n",
    "    \"training_time_per_fold_avg\": float(cv_results_df['training_time'].mean()),\n",
    "    \n",
    "    # Hyperparameters used\n",
    "    \"hyperparameters\": xgb_params,\n",
    "    \n",
    "    # Feature importance summary (lightweight)\n",
    "    \"top_features\": training_results['feature_importance'].head(10).to_dict('records') if training_results['feature_importance'] is not None else None,\n",
    "    \n",
    "    # Metadata\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"kaggle_competition\": \"playground-series-s5e6\",\n",
    "    \"ensemble_method\": \"Average of 10-fold CV models\",\n",
    "    \"models_saved\": False,  # Models used in-memory only for predictions\n",
    "    \"memory_optimized\": True\n",
    "}\n",
    "\n",
    "# Save comprehensive metrics as JSON (lightweight, portable)\n",
    "metrics_file = os.path.join(model_dir, files_to_create['metrics'])\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(metrics_data, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Metrics saved (KAGGLE ESSENTIALS):\")\n",
    "print(f\"  ğŸ“„ JSON: {files_to_create['metrics']}\")\n",
    "print(f\"  ğŸ’¾ Size: Lightweight (~5-15 KB)\")\n",
    "print(f\"  ğŸš€ Competition ready!\")\n",
    "print(f\"  ğŸ“Š Contains: Performance metrics, hyperparameters, fold results, feature importance\")\n",
    "\n",
    "# NOTE: Heavy files removed for competition efficiency:\n",
    "#   âŒ OOF predictions PKL (~10-50 MB)\n",
    "#   âŒ Model files PKL (~500MB-2GB) \n",
    "#   âŒ Feature importance PKL (~1-5 MB)\n",
    "#   âŒ Label encoders PKL (~1-5 MB)\n",
    "# \n",
    "# âœ… Ensemble predictions generated in-memory using all 10 models\n",
    "# âœ… All essential information preserved in lightweight JSON format\n",
    "# =============================================================================\n",
    "# GENERATE TEST PREDICTIONS AND CREATE KAGGLE SUBMISSION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nğŸ”® Generating test predictions using 10-model ensemble...\")\n",
    "\n",
    "# âœ… ENSEMBLE PREDICTION USING ALL 10 TRAINED MODELS (IN-MEMORY)\n",
    "# Models are NOT saved to disk - used only for prediction generation\n",
    "test_predictions_all = []\n",
    "for fold_name, model in training_results['models'].items():\n",
    "    pred_proba = model.predict_proba(X_test_final)\n",
    "    test_predictions_all.append(pred_proba)\n",
    "\n",
    "print(f\"  ğŸ“Š Generated predictions from {len(test_predictions_all)} fold models\")\n",
    "\n",
    "# Average predictions across all folds (ensemble method)\n",
    "test_predictions_ensemble = np.mean(test_predictions_all, axis=0)\n",
    "\n",
    "# Get top 3 predictions for each sample (MAP@3 format)\n",
    "test_top3_indices = np.argsort(test_predictions_ensemble, axis=1)[:, -3:][:, ::-1]\n",
    "\n",
    "# Convert prediction indices to fertilizer names\n",
    "test_top3_names = []\n",
    "for i in range(len(test_top3_indices)):\n",
    "    top3_for_sample = []\n",
    "    for j in range(3):\n",
    "        class_idx = test_top3_indices[i, j]\n",
    "        class_name = label_encoders['target'].inverse_transform([class_idx])[0]\n",
    "        top3_for_sample.append(class_name)\n",
    "    test_top3_names.append(top3_for_sample)\n",
    "\n",
    "# Create submission predictions (space-separated top 3 fertilizers)\n",
    "submission_predictions = []\n",
    "for top3_names in test_top3_names:\n",
    "    prediction_string = ' '.join(top3_names)\n",
    "    submission_predictions.append(prediction_string)\n",
    "\n",
    "# Create final submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': range(len(test_predictions_ensemble)),\n",
    "    'Fertilizer Name': submission_predictions\n",
    "})\n",
    "\n",
    "print(f\"  âœ… Ensemble predictions completed\")\n",
    "print(f\"  ğŸ“Š Test samples: {len(submission)}\")\n",
    "print(f\"  ğŸ¯ Format: Top-3 fertilizers per sample (MAP@3)\")\n",
    "\n",
    "# Display sample predictions\n",
    "print(f\"\\nğŸ“‹ SAMPLE PREDICTIONS:\")\n",
    "for i in range(min(5, len(submission))):\n",
    "    print(f\"  Sample {i}: {submission.iloc[i, 1]}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ ENSEMBLE INFO:\")\n",
    "print(f\"  ğŸ”„ Method: Average probabilities across 10 CV models\")\n",
    "print(f\"  ğŸ¯ Objective: Maximize MAP@3 score\")\n",
    "print(f\"  ğŸ’¾ Models: Used in-memory only (not saved to reduce file size)\")\n",
    "print(f\"  ğŸš€ Ready for Kaggle submission!\")\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE ESSENTIAL KAGGLE COMPETITION FILES\n",
    "# =============================================================================\n",
    "\n",
    "# Save submission file\n",
    "submission_file = os.path.join(model_dir, files_to_create['submission'])\n",
    "submission.to_csv(submission_file, index=False)\n",
    "\n",
    "# Enhanced submission information for competition tracking\n",
    "submission_info = {\n",
    "    \"model_type\": \"XGBClassifier\",\n",
    "    \"model_abbreviation\": \"XGB\", \n",
    "    \"cv_strategy\": f\"{N_SPLITS}-Fold Stratified Cross Validation\",\n",
    "    \"ensemble_method\": \"Average of 10-fold CV models\",\n",
    "    \n",
    "    # Performance metrics\n",
    "    \"map3_score_cv_mean\": float(map3_mean),\n",
    "    \"map3_score_cv_std\": float(map3_std),\n",
    "    \"map3_score_oof\": float(training_results['overall_map3']),\n",
    "    \"accuracy_cv_mean\": float(accuracy_mean),\n",
    "    \"accuracy_oof\": float(training_results['overall_accuracy']),\n",
    "    \n",
    "    # Submission details\n",
    "    \"submission_file\": files_to_create['submission'],\n",
    "    \"num_predictions\": len(submission),\n",
    "    \"format\": \"MAP@3 - Top 3 fertilizer names separated by spaces\",\n",
    "    \"target_variable\": \"Fertilizer Name\",\n",
    "    \n",
    "    # Model details\n",
    "    \"ensemble_models\": len(training_results['models']),\n",
    "    \"features_used\": len(features_to_use),\n",
    "    \"feature_list\": features_to_use,\n",
    "    \n",
    "    # Training info\n",
    "    \"total_training_time_minutes\": float(total_time / 60),\n",
    "    \"hyperparameters\": xgb_params,\n",
    "    \n",
    "    # Competition metadata\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"kaggle_competition\": \"playground-series-s5e6\",\n",
    "    \"memory_optimized\": True,\n",
    "    \"models_saved_to_disk\": False\n",
    "}\n",
    "\n",
    "# Save submission information\n",
    "submission_info_file = os.path.join(model_dir, files_to_create['submission_info'])\n",
    "with open(submission_info_file, 'w') as f:\n",
    "    json.dump(submission_info, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… KAGGLE ESSENTIAL FILES SAVED:\")\n",
    "print(f\"  ğŸ“„ Submission: {files_to_create['submission']}\")\n",
    "print(f\"  ğŸ“„ Submission info: {files_to_create['submission_info']}\")\n",
    "print(f\"  ğŸ“Š Submission shape: {submission.shape}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ SAMPLE PREDICTIONS:\")\n",
    "for i in range(min(3, len(submission))):\n",
    "    print(f\"  Sample {i+1}: {submission.iloc[i, 1]}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ SUBMISSION READY FOR KAGGLE!\")\n",
    "print(f\"  ğŸ¯ Format: Top-3 fertilizer recommendations per sample\")\n",
    "print(f\"  ğŸ“Š Total predictions: {len(submission)}\")\n",
    "print(f\"  ğŸ”„ Ensemble: 10-fold CV models averaged\")\n",
    "print(f\"  ğŸ’¾ File size: Minimal (~MB vs GB for full model save)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ba37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL SUMMARY - KAGGLE COMPETITION FILES\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"\\nğŸ’¾ KAGGLE COMPETITION FILES SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"ğŸ“ DIRECTORY: {model_dir}\")\n",
    "print(f\"\\nğŸ“„ ESSENTIAL FILES SAVED:\")\n",
    "\n",
    "# Calculate total size\n",
    "total_size = 0\n",
    "file_details = []\n",
    "\n",
    "for file_type, filename in files_to_create.items():\n",
    "    file_path = os.path.join(model_dir, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        total_size += file_size\n",
    "        \n",
    "        if file_size > 1024*1024:  # > 1MB\n",
    "            size_str = f\"{file_size/(1024*1024):.1f} MB\"\n",
    "        elif file_size > 1024:  # > 1KB\n",
    "            size_str = f\"{file_size/1024:.1f} KB\"\n",
    "        else:\n",
    "            size_str = f\"{file_size} bytes\"\n",
    "        \n",
    "        # File descriptions\n",
    "        descriptions = {\n",
    "            'hparams': \"Model hyperparameters & configuration\",\n",
    "            'metrics': \"Performance metrics & training details\", \n",
    "            'submission': \"Kaggle submission with top-3 predictions\",\n",
    "            'submission_info': \"Submission metadata & model info\"\n",
    "        }\n",
    "        \n",
    "        desc = descriptions.get(file_type, \"\")\n",
    "        file_details.append((filename, size_str, desc))\n",
    "        print(f\"  âœ… {filename:30} ({size_str:8}) - {desc}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {filename:30} (NOT CREATED)\")\n",
    "\n",
    "# Total size calculation\n",
    "if total_size > 1024*1024:\n",
    "    total_size_str = f\"{total_size/(1024*1024):.1f} MB\"\n",
    "elif total_size > 1024:\n",
    "    total_size_str = f\"{total_size/1024:.1f} KB\"\n",
    "else:\n",
    "    total_size_str = f\"{total_size} bytes\"\n",
    "\n",
    "print(f\"\\nğŸ“Š TOTAL SIZE: {total_size_str}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ PERFORMANCE SUMMARY:\")\n",
    "print(f\"  ğŸ“ˆ MAP@3 (CV):         {map3_mean:.5f} Â± {map3_std:.5f}\")\n",
    "print(f\"  ğŸ“ˆ MAP@3 (OOF):        {training_results['overall_map3']:.5f}\")\n",
    "print(f\"  ğŸ“ˆ Accuracy (OOF):     {training_results['overall_accuracy']:.5f}\")\n",
    "print(f\"  ğŸ”„ CV Folds:           {N_SPLITS}\")\n",
    "print(f\"  â±ï¸ Training Time:      {total_time/60:.1f} minutes\")\n",
    "\n",
    "print(f\"\\nğŸ¤– MODEL CONFIGURATION:\")\n",
    "print(f\"  ğŸ”§ Algorithm:          XGBoost Ensemble\")\n",
    "print(f\"  ğŸ“Š Features:           {len(features_to_use)}\")\n",
    "print(f\"  ğŸ¯ Classes:            {len(label_encoders['target'].classes_)} fertilizers\")\n",
    "print(f\"  ğŸ’¾ Memory Strategy:    In-memory ensemble (models not saved)\")\n",
    "\n",
    "print(f\"\\nğŸš€ KAGGLE SUBMISSION READY:\")\n",
    "print(f\"  ğŸ“„ File: {files_to_create['submission']}\")\n",
    "print(f\"  ğŸ“Š Predictions: {len(submission)} samples\")\n",
    "print(f\"  ğŸ¯ Format: MAP@3 (top-3 fertilizers per sample)\")\n",
    "print(f\"  ğŸ’¡ Upload this file to Kaggle for competition scoring\")\n",
    "\n",
    "print(f\"\\nâœ¨ OPTIMIZATION BENEFITS:\")\n",
    "print(f\"  ğŸ’¾ File Size: ~{total_size_str} (vs ~500MB-2GB with models)\")\n",
    "print(f\"  âš¡ Memory: ~99% reduction from full model save\")\n",
    "print(f\"  ğŸ¯ Focus: Only competition-essential files\")\n",
    "print(f\"  ğŸ“ Clean: No heavy PKL files or model artifacts\")\n",
    "\n",
    "print(f\"\\nğŸ“‚ Location: {os.path.abspath(model_dir)}\")\n",
    "print(f\"\\nğŸ‰ KAGGLE-OPTIMIZED XGBOOST PIPELINE COMPLETED!\")\n",
    "print(f\"ğŸš€ Ready for competition submission! ğŸ†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83527858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
