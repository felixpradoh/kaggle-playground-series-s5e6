{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "311624b8",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; padding: 25px; margin: 20px 0; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 20px; box-shadow: 0 10px 20px rgba(0,0,0,0.15); color: white;\">\n",
    "  <h1 style=\"color: white; margin-bottom: 15px; font-size: 2.5em;\">🚀 Kaggle Playground Series S5E6</h1>\n",
    "  <h2 style=\"color: white; margin-bottom: 20px; font-weight: 300;\">XGBoost with 10-Fold CV - Fertilizer Prediction</h2>\n",
    "  <p style=\"font-size: 18px; margin-bottom: 0; font-style: italic;\">Advanced XGBoost modeling pipeline with stratified cross-validation for agricultural fertilizer recommendation</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"padding: 20px; margin: 20px 0; background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); border-radius: 15px; box-shadow: 0 8px 16px rgba(0,0,0,0.1); color: white;\">\n",
    "  <h3 style=\"color: white; margin-bottom: 15px; text-align: center;\">🎯 Competition Overview</h3>\n",
    "  <p style=\"font-size: 16px; line-height: 1.6; text-align: center;\">This notebook implements a <strong>comprehensive XGBoost modeling solution</strong> for the <strong>Kaggle Playground Series S5E6: Fertilizer Prediction Challenge</strong>. It builds upon <a href=\"https://www.kaggle.com/code/felixpradoh/ps-s5e6-exploratory-data-anlaysis\" style=\"color: #FFE4E1; text-decoration: underline;\">exploratory data analysis (EDA) insights</a> to create a robust, competition-ready model with advanced feature engineering and rigorous validation.</p>\n",
    "</div>\n",
    "\n",
    "## 🌱 Agricultural Context & Problem Domain\n",
    "\n",
    "This challenge focuses on **precision agriculture** and **intelligent fertilizer recommendation systems**. Understanding the agricultural context is crucial for effective modeling:\n",
    "\n",
    "<div style=\"display: flex; flex-wrap: wrap; gap: 15px; margin: 20px 0;\">\n",
    "\n",
    "<div style=\"flex: 1; min-width: 300px; padding: 15px; background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%); border-radius: 10px; color: #2c3e50;\">\n",
    "  <h3 style=\"color: #2c3e50; margin-bottom: 10px;\">🌾 Soil Science Insights</h3>\n",
    "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
    "    <li><strong>Nutrient balance</strong>: N-P-K ratios determine crop health</li>\n",
    "    <li><strong>pH optimization</strong>: Affects nutrient availability</li>\n",
    "    <li><strong>Micronutrient interactions</strong>: Complex relationships between elements</li>\n",
    "    <li><strong>Soil composition</strong>: Sandy, loamy, clay affect fertilizer needs</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"flex: 1; min-width: 300px; padding: 15px; background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%); border-radius: 10px; color: #2c3e50;\">\n",
    "  <h3 style=\"color: #2c3e50; margin-bottom: 10px;\">🌡️ Environmental Factors</h3>\n",
    "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
    "    <li><strong>Temperature impact</strong>: Affects nutrient uptake rates</li>\n",
    "    <li><strong>Rainfall patterns</strong>: Influences fertilizer timing</li>\n",
    "    <li><strong>Crop types</strong>: Different species have unique requirements</li>\n",
    "    <li><strong>Growth stages</strong>: Fertilizer needs change over time</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "## 🤖 Why XGBoost for This Challenge?\n",
    "\n",
    "**XGBoost (eXtreme Gradient Boosting)** is the optimal choice for this agricultural classification task for several reasons:\n",
    "\n",
    "<div style=\"display: flex; flex-wrap: wrap; gap: 15px; margin: 20px 0;\">\n",
    "  \n",
    "<div style=\"flex: 1; min-width: 300px; padding: 15px; background: linear-gradient(135deg, #84fab0 0%, #8fd3f4 100%); border-radius: 10px; color: #2c3e50;\">\n",
    "  <h3 style=\"color: #2c3e50; margin-bottom: 10px;\">🏆 Algorithmic Advantages</h3>\n",
    "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
    "    <li><strong>Tree-based architecture</strong>: Naturally handles feature interactions</li>\n",
    "    <li><strong>Built-in regularization</strong>: Prevents overfitting with L1/L2 penalties</li>\n",
    "    <li><strong>Missing value handling</strong>: Robust to data inconsistencies</li>\n",
    "    <li><strong>Feature selection</strong>: Automatic relevance weighting</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"flex: 1; min-width: 300px; padding: 15px; background: linear-gradient(135deg, #a8edea 0%, #fed6e3 100%); border-radius: 10px; color: #2c3e50;\">\n",
    "  <h3 style=\"color: #2c3e50; margin-bottom: 10px;\">📊 Multi-class Excellence</h3>\n",
    "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
    "    <li><strong>Native multi-class support</strong>: Handles 22 fertilizer classes efficiently</li>\n",
    "    <li><strong>Probability outputs</strong>: Essential for MAP@3 ranking optimization</li>\n",
    "    <li><strong>Class imbalance handling</strong>: Sample weighting for balanced performance</li>\n",
    "    <li><strong>Calibrated predictions</strong>: Reliable probability estimates</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; flex-wrap: wrap; gap: 15px; margin: 20px 0;\">\n",
    "\n",
    "<div style=\"flex: 1; min-width: 300px; padding: 15px; background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%); border-radius: 10px; color: #2c3e50;\">\n",
    "  <h3 style=\"color: #2c3e50; margin-bottom: 10px;\">⚡ Performance & Scalability</h3>\n",
    "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
    "    <li><strong>Fast training</strong>: Efficient gradient boosting implementation</li>\n",
    "    <li><strong>Memory efficiency</strong>: Handles large feature sets</li>\n",
    "    <li><strong>Early stopping</strong>: Prevents overfitting with validation</li>\n",
    "    <li><strong>Cross-validation friendly</strong>: Stable performance across splits</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "<div style=\"flex: 1; min-width: 300px; padding: 15px; background: linear-gradient(135deg, #d299c2 0%, #fef9d7 100%); border-radius: 10px; color: #2c3e50;\">\n",
    "  <h3 style=\"color: #2c3e50; margin-bottom: 10px;\">🎯 Competition Benefits</h3>\n",
    "  <ul style=\"margin: 0; padding-left: 20px;\">\n",
    "    <li><strong>Ensemble capability</strong>: Multiple models from CV folds</li>\n",
    "    <li><strong>Hyperparameter sensitivity</strong>: Extensive tuning options</li>\n",
    "    <li><strong>Proven track record</strong>: Dominant in Kaggle competitions</li>\n",
    "    <li><strong>MAP@3 optimization</strong>: Probability-based outputs ideal</li>\n",
    "  </ul>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"padding: 20px; margin: 20px 0; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; box-shadow: 0 8px 16px rgba(0,0,0,0.1); color: white;\">\n",
    "  <h2 style=\"color: white; margin-bottom: 15px; text-align: left;\">🗂️ Notebook Structure</h2>\n",
    "  <p style=\"font-size: 16px; text-align: left; margin-bottom: 15px;\"><strong>This comprehensive agricultural ML pipeline covers:</strong></p>\n",
    "  \n",
    "  <div style=\"color: white; font-size: 16px; line-height: 1.8; text-align: left; max-width: 600px; margin: 0 auto;\">\n",
    "    <ol style=\"padding-left: 5px;\">\n",
    "      <li><strong>📚 Library Import & Setup</strong></li>\n",
    "      <li><strong>📂 Data Loading & Preparation</strong></li>\n",
    "      <li><strong>🌱 Agricultural Feature Engineering</strong></li>\n",
    "      <li><strong>🔢 Categorical Variable Encoding</strong></li>\n",
    "      <li><strong>🎯 Strategic Feature Selection</strong></li>\n",
    "      <li><strong>🔄 Cross-Validation Setup</strong></li>\n",
    "      <li><strong>⚙️ XGBoost Configuration</strong></li>\n",
    "      <li><strong>🏋️ Model Training & Validation</strong></li>\n",
    "      <li><strong>📊 Performance Evaluation</strong></li>\n",
    "      <li><strong>🔮 Test Predictions & Submission</strong></li>\n",
    "      <li><strong>💾 Model Persistence</strong></li>\n",
    "    </ol>\n",
    "  </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb332602",
   "metadata": {},
   "source": [
    "## 📚 1. Library Import & Setup\n",
    "\n",
    "**Why?** We import essential libraries for the complete XGBoost modeling pipeline. These libraries provide data manipulation, machine learning algorithms, evaluation metrics, and model persistence capabilities required for competition-level performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4c06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning - Scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost.callback import EarlyStopping\n",
    "\n",
    "# Model persistence and metadata\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration\n",
    "np.random.seed(513)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb56f28",
   "metadata": {},
   "source": [
    "## 📏 MAP@K Evaluation Function\n",
    "\n",
    "**Why?**\n",
    "\n",
    "MAP@3 (Mean Average Precision at 3) is the official Kaggle competition metric. This function calculates how well our model ranks the correct fertilizer within the top 3 predictions for each sample.\n",
    "\n",
    "**Function Details:**\n",
    "- **Input**: Actual fertilizer labels and predicted rankings (top-k)\n",
    "- **Output**: Score between 0 and 1 (higher is better)\n",
    "- **Logic**: Rewards correct predictions more heavily when they appear earlier in the ranking\n",
    "- **Competition Critical**: This exact implementation matches Kaggle's evaluation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463195f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapk(actual, predicted, k=3):\n",
    "    \"\"\"Compute mean average precision at k (MAP@k).\"\"\"\n",
    "    def apk(a, p, k):\n",
    "        score = 0.0\n",
    "        for i in range(min(k, len(p))):\n",
    "            if p[i] == a:\n",
    "                score += 1.0 / (i + 1)\n",
    "                break  # only the first correct prediction counts\n",
    "        return score\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee416ea2",
   "metadata": {},
   "source": [
    "## 📂 2. Data Loading & Preparation\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Load competition datasets and perform initial data separation. EDA insights guide our feature engineering approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b348a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "data_path = '../data'\n",
    "train_path = os.path.join(data_path, 'train.csv')\n",
    "test_path = os.path.join(data_path, 'test.csv')\n",
    "sample_submission_path = os.path.join(data_path, 'sample_submission.csv')\n",
    "\n",
    "# Load datasets\n",
    "print(\"📂 Loading datasets...\")\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "\n",
    "# print(\"✅ Data loaded successfully:\")\n",
    "# print(f\"  • Training set: {train_df.shape}\")\n",
    "# print(f\"  • Test set: {test_df.shape}\")\n",
    "# print(f\"  • Sample submission: {sample_submission.shape}\")\n",
    "\n",
    "# Separate features and target variable\n",
    "target_column = 'Fertilizer Name'\n",
    "\n",
    "# Split training data\n",
    "X_raw = train_df.drop(columns=[target_column])\n",
    "y_raw = train_df[target_column]\n",
    "X_test_raw = test_df.copy()\n",
    "\n",
    "print(\"✅ Data separation completed:\")\n",
    "print(f\"  • Training features: {X_raw.shape}\")\n",
    "print(f\"  • Training target: {y_raw.shape}\")\n",
    "print(f\"  • Test features: {X_test_raw.shape}\")\n",
    "print(f\"  • Target classes: {y_raw.nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aee804",
   "metadata": {},
   "source": [
    "## 🔬 3. Advanced Feature Engineering\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Create sophisticated features that capture agricultural relationships and domain knowledge based on EDA findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3d8682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Create engineered features based on agricultural domain knowledge\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with agricultural features\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with additional engineered features\n",
    "    \"\"\"\n",
    "    df_eng = df.copy()\n",
    "    \n",
    "    # NPK Ratios (crucial for agricultural decisions)\n",
    "    df_eng['N_P_ratio'] = df_eng['Nitrogen'] / (df_eng['Phosphorous'] + 0.001)\n",
    "    df_eng['N_K_ratio'] = df_eng['Nitrogen'] / (df_eng['Potassium'] + 0.001)\n",
    "    df_eng['P_K_ratio'] = df_eng['Phosphorous'] / (df_eng['Potassium'] + 0.001)\n",
    "    \n",
    "    # Total NPK and NPK Balance\n",
    "    df_eng['Total_NPK'] = df_eng['Nitrogen'] + df_eng['Phosphorous'] + df_eng['Potassium']\n",
    "    npk_mean = df_eng[['Nitrogen', 'Phosphorous', 'Potassium']].mean(axis=1)\n",
    "    df_eng['NPK_Balance'] = df_eng[['Nitrogen', 'Phosphorous', 'Potassium']].std(axis=1) / (npk_mean + 0.001)\n",
    "    \n",
    "    # Environmental indices\n",
    "    df_eng['Temp_Hum_index'] = df_eng['Temparature'] * df_eng['Humidity'] / 100\n",
    "    df_eng['Moist_Balance'] = df_eng['Moisture'] - df_eng['Humidity']\n",
    "    df_eng['Environ_Stress'] = np.sqrt((df_eng['Temparature'] - 25)**2 + (df_eng['Humidity'] - 65)**2)\n",
    "    df_eng['Temp_Moist_inter'] = df_eng['Temparature'] * df_eng['Moisture'] / 100\n",
    "    \n",
    "    # Dominant nutrient\n",
    "    npk_cols = ['Nitrogen', 'Phosphorous', 'Potassium']\n",
    "    df_eng['Dominant_NPK'] = df_eng[npk_cols].idxmax(axis=1)\n",
    "    \n",
    "    # Categorical binning\n",
    "    df_eng['Temp_Cat'] = pd.cut(df_eng['Temparature'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    df_eng['Hum_Cat'] = pd.cut(df_eng['Humidity'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    df_eng['N_Level'] = pd.cut(df_eng['Nitrogen'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    df_eng['K_Level'] = pd.cut(df_eng['Potassium'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    df_eng['P_Level'] = pd.cut(df_eng['Phosphorous'], bins=3, labels=['Low', 'Medium', 'High'])\n",
    "    \n",
    "    # Soil-Crop interaction\n",
    "    df_eng['Soil_Crop_Combo'] = df_eng['Soil Type'].astype(str) + '_' + df_eng['Crop Type'].astype(str)\n",
    "    \n",
    "    return df_eng\n",
    "\n",
    "# Apply feature engineering\n",
    "print(\"🔧 Applying feature engineering...\")\n",
    "X_train_featured = create_features(X_raw)\n",
    "X_test_featured = create_features(X_test_raw)\n",
    "\n",
    "# Display new feature names\n",
    "original_features = set(X_raw.columns)\n",
    "new_features = [col for col in X_train_featured.columns if col not in original_features]\n",
    "\n",
    "print(f\"✅ Feature engineering completed: {X_raw.shape[1]} → {X_train_featured.shape[1]} features (+{X_train_featured.shape[1] - X_raw.shape[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fdd129",
   "metadata": {},
   "source": [
    "## 🔢 4. Categorical Variable Encoding\n",
    "\n",
    "**Why?**\n",
    "\n",
    "XGBoost requires numerical inputs, so we convert categorical variables (soil types, crop types, engineered categories) to numerical representations while maintaining consistency between training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39158f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categorical_features(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Encode categorical features using LabelEncoder\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features\n",
    "        X_test: Test features  \n",
    "        y_train: Training target\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (X_train_encoded, X_test_encoded, y_encoded, encoders_dict)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize encoders dictionary\n",
    "    encoders = {}\n",
    "    \n",
    "    # Create copies to avoid modifying originals\n",
    "    X_train_enc = X_train.copy()\n",
    "    X_test_enc = X_test.copy()\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"🔢 Encoding categorical features...\")\n",
    "    print(f\"Categorical columns found: {categorical_cols}\")\n",
    "    \n",
    "    # Encode categorical features\n",
    "    for col in categorical_cols:\n",
    "        print(f\"  • Encoding: {col}\")\n",
    "        \n",
    "        # Create encoder\n",
    "        encoder = LabelEncoder()\n",
    "        \n",
    "        # Fit on combined training and test data to ensure consistency\n",
    "        combined_values = pd.concat([X_train[col], X_test[col]]).astype(str)\n",
    "        encoder.fit(combined_values)\n",
    "        \n",
    "        # Transform both datasets\n",
    "        X_train_enc[col] = encoder.transform(X_train[col].astype(str))\n",
    "        X_test_enc[col] = encoder.transform(X_test[col].astype(str))\n",
    "        \n",
    "        # Store encoder\n",
    "        encoders[col] = encoder\n",
    "            \n",
    "    # Encode target variable\n",
    "    print(f\"\\n🎯 Encoding target variable: {target_column}\")\n",
    "    target_encoder = LabelEncoder()\n",
    "    y_encoded = target_encoder.fit_transform(y_train)\n",
    "    encoders['target'] = target_encoder\n",
    "    \n",
    "    print(f\"  • Target classes: {len(target_encoder.classes_)}\")\n",
    "    # print(f\"  • Class mapping preview: {dict(zip(target_encoder.classes_[:5], range(5)))}\")\n",
    "    \n",
    "    return X_train_enc, X_test_enc, y_encoded, encoders\n",
    "\n",
    "# Apply encoding\n",
    "X_train_encoded, X_test_encoded, y_encoded, label_encoders = encode_categorical_features(\n",
    "    X_train_featured, X_test_featured, y_raw\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Encoding completed:\")\n",
    "print(f\"  • Training features: {X_train_encoded.shape}\")\n",
    "print(f\"  • Test features: {X_test_encoded.shape}\")\n",
    "print(f\"  • Encoded target: {y_encoded.shape}\")\n",
    "print(f\"  • Encoders stored: {len(label_encoders)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f42c8d",
   "metadata": {},
   "source": [
    "## 🎯 5. Strategic Feature Selection\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Implement flexible feature selection based on EDA insights and mutual information analysis for optimal model performance.\n",
    "\n",
    "**Selection Philosophy:**\n",
    "\n",
    "- **EDA-driven**: Prioritize features with high mutual information scores\n",
    "- **Domain-informed**: Include agriculturally meaningful combinations\n",
    "- **Computational efficiency**: Balance between information gain and training speed\n",
    "- **Easy experimentation**: Toggle features on/off with simple commenting system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0844e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE SELECTION FOR THE MODEL\n",
    "# =============================================================================\n",
    "\n",
    "# Feature selection\n",
    "features_to_use = [\n",
    "    # Original features\n",
    "    # 'Temparature',\n",
    "    # 'Humidity', \n",
    "    # 'Moisture',\n",
    "    # 'Nitrogen',\n",
    "    # 'Potassium', \n",
    "    # 'Phosphorous',\n",
    "    \n",
    "    # Engineered features\n",
    "    'N_P_ratio',\n",
    "    # 'N_K_ratio',\n",
    "    # 'P_K_ratio',\n",
    "    # 'Total_NPK',\n",
    "    # 'NPK_Balance',\n",
    "    # 'Temp_Hum_index',\n",
    "    # 'Moist_Balance',\n",
    "    # 'Environ_Stress',\n",
    "    # 'Temp_Moist_inter',\n",
    "    # 'Temp_Cat',\n",
    "    # 'Hum_Cat',\n",
    "    # 'N_Level',\n",
    "    # 'K_Level',\n",
    "    # 'P_Level',\n",
    "    \n",
    "    # Combinations\n",
    "    'Soil_Crop_Combo',\n",
    "    # 'Dominant_NPK',\n",
    "    \n",
    "    # Categorical features\n",
    "    # 'Soil Type',\n",
    "    'Crop Type',\n",
    "]\n",
    "\n",
    "# Validate features\n",
    "available_features = [f for f in features_to_use if f in X_train_encoded.columns]\n",
    "missing_features = [f for f in features_to_use if f not in X_train_encoded.columns]\n",
    "\n",
    "features_to_use = available_features\n",
    "\n",
    "if missing_features:\n",
    "    print(f\"Missing features: {missing_features}\")\n",
    "\n",
    "print(f\"✅ Selected features ({len(features_to_use)}): {features_to_use}\")\n",
    "\n",
    "# Create final datasets\n",
    "X_final = X_train_encoded[features_to_use].copy()\n",
    "X_test_final = X_test_encoded[features_to_use].copy()\n",
    "\n",
    "# print(f\"Training: {X_final.shape}, Test: {X_test_final.shape}, Target: {y_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de5d67f",
   "metadata": {},
   "source": [
    "## 🔄 6. Stratified 10-Fold Cross-Validation Setup\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Robust cross-validation is critical for reliable model evaluation and preventing overfitting using stratified 10-fold CV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea40e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STRATIFIED 10-FOLD CROSS-VALIDATION CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Cross-validation parameters\n",
    "N_SPLITS = 10  # 10-fold cross-validation for robust evaluation\n",
    "RANDOM_STATE = 513\n",
    "SHUFFLE = True\n",
    "\n",
    "# Initialize StratifiedKFold to maintain class distribution\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=N_SPLITS, \n",
    "    shuffle=SHUFFLE, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"🔄 CROSS-VALIDATION CONFIGURATION:\")\n",
    "print(f\"  • Number of folds: {N_SPLITS}\")\n",
    "print(f\"  • Strategy: Stratified (maintains class proportions)\")\n",
    "print(f\"  • Shuffle: {SHUFFLE}\")\n",
    "print(f\"  • Random state: {RANDOM_STATE}\")\n",
    "\n",
    "# Analyze class distribution for stratification\n",
    "print(f\"\\n📊 Class distribution analysis:\")\n",
    "unique_classes, class_counts = np.unique(y_encoded, return_counts=True)\n",
    "print(f\"  • Total classes: {len(unique_classes)}\")\n",
    "print(f\"  • Total samples: {len(y_encoded)}\")\n",
    "print(f\"  • Samples per fold: ~{len(y_encoded) // N_SPLITS}\")\n",
    "\n",
    "# Check minimum class size for stratification\n",
    "min_class_count = min(class_counts)\n",
    "print(f\"  • Minimum class size: {min_class_count}\")\n",
    "if min_class_count < N_SPLITS:\n",
    "    print(f\"  ⚠️ Warning: Smallest class has {min_class_count} samples, less than {N_SPLITS} folds\")\n",
    "    print(f\"    Some folds may not contain all classes\")\n",
    "else:\n",
    "    print(f\"  ✅ All classes have sufficient samples for {N_SPLITS}-fold CV\")\n",
    "\n",
    "# Preview fold splits\n",
    "print(f\"\\n🔍 Fold size preview:\")\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(skf.split(X_final, y_encoded)):\n",
    "    if fold_idx < 3:  # Show first 3 folds\n",
    "        print(f\"  Fold {fold_idx + 1}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
    "    elif fold_idx == 3:\n",
    "        print(\"  ...\")\n",
    "    elif fold_idx == N_SPLITS - 1:  # Show last fold\n",
    "        print(f\"  Fold {fold_idx + 1}: Train={len(train_idx)}, Val={len(val_idx)}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5df129",
   "metadata": {},
   "source": [
    "## ⚙️ 7. XGBoost Hyperparameter Configuration\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Optimal hyperparameter selection is crucial for XGBoost performance on multi-class classification tasks with balanced class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db2902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# XGBOOST HYPERPARAMETER CONFIGURATION\n",
    "#\n",
    "#🎯 EXPERIMENTATION ENCOURAGED!\n",
    "# These parameters provide a solid baseline, but feel free to experiment!\n",
    "# Try different learning rates, depths, or regularization for better scores.\n",
    "# =============================================================================\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_encoded),\n",
    "    y=y_encoded\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_encoded), class_weights))\n",
    "\n",
    "print(\"⚖️ Class weight calculation:\")\n",
    "print(f\"  • Balanced class weights computed for {len(class_weight_dict)} classes\")\n",
    "print(f\"  • Weight range: {min(class_weights):.3f} - {max(class_weights):.3f}\")\n",
    "\n",
    "# XGBoost hyperparameters (optimized for multi-class classification)\n",
    "xgb_params = {\n",
    "    # Multi-class objective\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': len(label_encoders['target'].classes_),\n",
    "    'eval_metric': 'mlogloss',\n",
    "    \n",
    "    # Tree structure\n",
    "    'max_depth': 12,\n",
    "    'min_child_weight': 3,\n",
    "    'subsample': 0.86,\n",
    "    'colsample_bytree': 0.467,\n",
    "    \n",
    "    # Learning parameters\n",
    "    'learning_rate': 0.5,\n",
    "    'n_estimators': 40,  # High number with early stopping\n",
    "    \n",
    "    # Regularization\n",
    "    'reg_alpha': 2.7,  # L1 regularization\n",
    "    'reg_lambda': 1.4,  # L2 regularization\n",
    "    'gamma': 0.25,      # Minimum split loss\n",
    "    'max_delta_step': 4,  # Maximum delta step for tree weights\n",
    "    \n",
    "    # Performance\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0,\n",
    "    \n",
    "    # 'device': 'cpu',\n",
    "    'tree_method': 'hist', \n",
    "\n",
    "    # GPU acceleration (comment out if no GPU available)\n",
    "    # 'gpu_id': 0,\n",
    "    # 'tree_method': 'gpu_hist',\n",
    "    \n",
    "    # Early stopping will be handled separately\n",
    "    'early_stopping_rounds': 150\n",
    "}\n",
    "\n",
    "# Early stopping configuration\n",
    "es = 150\n",
    "eval_metric = 'mlogloss'\n",
    "\n",
    "print(f\"\\n🚀 XGBOOST CONFIGURATION:\")\n",
    "print(f\"  • Objective: {xgb_params['objective']}\")\n",
    "print(f\"  • Number of classes: {xgb_params['num_class']}\")\n",
    "print(f\"  • Max depth: {xgb_params['max_depth']}\")\n",
    "print(f\"  • Learning rate: {xgb_params['learning_rate']}\")\n",
    "print(f\"  • Max estimators: {xgb_params['n_estimators']}\")\n",
    "print(f\"  • Early stopping: {xgb_params['early_stopping_rounds']} rounds\")\n",
    "print(f\"  • Evaluation metric: {xgb_params['eval_metric']}\")\n",
    "print(f\"  • Regularization: L1={xgb_params['reg_alpha']}, L2={xgb_params['reg_lambda']}\")\n",
    "print(f\"  • Tree method: {xgb_params.get('tree_method', 'hist')}\")\n",
    "print(f\"  • GPU ID: {xgb_params.get('gpu_id', 'N/A')}\")\n",
    "print(f\"  • Class balancing: Enabled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d735d",
   "metadata": {},
   "source": [
    "## 🏋️ 8. Model Training with 10-Fold Cross-Validation\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Implement core training pipeline using stratified 10-fold cross-validation with early stopping and class balancing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cc36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10-FOLD CROSS-VALIDATION TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "def train_xgboost_cv(X, y, features, cv_splitter, params):\n",
    "    \"\"\"\n",
    "    Train XGBoost models using cross-validation\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target vector (encoded)\n",
    "        features: List of feature names to use\n",
    "        cv_splitter: Cross-validation splitter (StratifiedKFold)\n",
    "        params: XGBoost parameters\n",
    "        early_stopping_rounds: Early stopping patience\n",
    "        \n",
    "    Returns:\n",
    "        Dict with trained models, predictions, and metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize storage\n",
    "    models = {}\n",
    "    oof_predictions = np.zeros((len(X), params['num_class']))  # Out-of-fold predictions\n",
    "    cv_scores = []\n",
    "    feature_importance_list = []\n",
    "    \n",
    "    print(f\"🏋️ Starting {N_SPLITS}-Fold Cross-Validation Training...\")\n",
    "    print(f\"⏰ Training started at: {time.strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(cv_splitter.split(X, y)):\n",
    "        \n",
    "        fold_start_time = time.time()\n",
    "        print(f\"\\n📁 FOLD {fold_idx + 1}/{N_SPLITS}\")\n",
    "        print(f\"  • Train samples: {len(train_idx)}\")\n",
    "        print(f\"  • Validation samples: {len(val_idx)}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_fold = X.iloc[train_idx][features]\n",
    "        X_val_fold = X.iloc[val_idx][features]\n",
    "        y_train_fold = y[train_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        # Calculate sample weights for this fold\n",
    "        fold_class_weights = compute_class_weight(\n",
    "            'balanced',\n",
    "            classes=np.unique(y_train_fold),\n",
    "            y=y_train_fold\n",
    "        )\n",
    "        fold_class_weight_dict = dict(zip(np.unique(y_train_fold), fold_class_weights))\n",
    "        sample_weights = np.array([fold_class_weight_dict.get(label, 1.0) for label in y_train_fold])\n",
    "        \n",
    "        # Initialize model\n",
    "        model = XGBClassifier(**params)\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            sample_weight=sample_weights,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Predict validation set\n",
    "        val_pred_proba = model.predict_proba(X_val_fold)\n",
    "        val_pred_classes = model.predict(X_val_fold)\n",
    "        \n",
    "        # Store out-of-fold predictions\n",
    "        oof_predictions[val_idx] = val_pred_proba\n",
    "        \n",
    "        # Calculate fold metrics\n",
    "        fold_accuracy = accuracy_score(y_val_fold, val_pred_classes)\n",
    "        \n",
    "        # Calculate MAP@3 for this fold and get top 3 predictions for each sample\n",
    "        val_top3_indices = np.argsort(val_pred_proba, axis=1)[:, -3:][:, ::-1]\n",
    "        \n",
    "        # Convert to lists for mapk function\n",
    "        actual_list = y_val_fold.tolist() if hasattr(y_val_fold, 'tolist') else list(y_val_fold)\n",
    "        predicted_list = val_top3_indices.tolist()\n",
    "        \n",
    "        # Calculate MAP@3 using the correct format\n",
    "        fold_map3 = mapk(actual_list, predicted_list, k=3)\n",
    "        \n",
    "        # Store results\n",
    "        cv_scores.append({\n",
    "            'fold': fold_idx + 1,\n",
    "            'accuracy': fold_accuracy,\n",
    "            'map3': fold_map3,\n",
    "            'best_iteration': model.best_iteration,\n",
    "            'train_samples': len(train_idx),\n",
    "            'val_samples': len(val_idx),\n",
    "            'training_time': time.time() - fold_start_time\n",
    "        })\n",
    "        \n",
    "        # Store model and feature importance\n",
    "        models[f'fold_{fold_idx + 1}'] = model\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': features,\n",
    "                'importance': model.feature_importances_,\n",
    "                'fold': fold_idx + 1\n",
    "            })\n",
    "            feature_importance_list.append(importance_df)\n",
    "        \n",
    "        fold_time = time.time() - fold_start_time\n",
    "        print(f\"  ✅ Fold completed in {fold_time:.1f}s\")\n",
    "        print(f\"  📊 Accuracy: {fold_accuracy:.4f} | MAP@3: {fold_map3:.4f}\")\n",
    "        print(f\"  🔄 Best iteration: {model.best_iteration}\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    oof_pred_classes = np.argmax(oof_predictions, axis=1)\n",
    "    overall_accuracy = accuracy_score(y, oof_pred_classes)\n",
    "    \n",
    "    # Calculate overall MAP@3 and get top 3 predictions for each sample\n",
    "    oof_top3_indices = np.argsort(oof_predictions, axis=1)[:, -3:][:, ::-1]\n",
    "    \n",
    "    # Convert to lists for mapk function\n",
    "    actual_list = y.tolist() if hasattr(y, 'tolist') else list(y)\n",
    "    predicted_list = oof_top3_indices.tolist()\n",
    "    \n",
    "    # Calculate MAP@3 using the correct format\n",
    "    overall_map3 = mapk(actual_list, predicted_list, k=3)\n",
    "    \n",
    "    # Combine feature importance across folds\n",
    "    if feature_importance_list:\n",
    "        feature_importance_df = pd.concat(feature_importance_list, ignore_index=True)\n",
    "        feature_importance_summary = feature_importance_df.groupby('feature')['importance'].agg(['mean', 'std']).reset_index()\n",
    "        feature_importance_summary = feature_importance_summary.sort_values('mean', ascending=False)\n",
    "    else:\n",
    "        feature_importance_summary = None\n",
    "    \n",
    "    return {\n",
    "        'models': models,\n",
    "        'oof_predictions': oof_predictions,\n",
    "        'cv_scores': cv_scores,\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'overall_map3': overall_map3,\n",
    "        'feature_importance': feature_importance_summary\n",
    "    }\n",
    "\n",
    "# Execute cross-validation training\n",
    "print(\"🚀 Starting model training...\")\n",
    "start_time = time.time()\n",
    "\n",
    "training_results = train_xgboost_cv(\n",
    "    X=X_final,\n",
    "    y=y_encoded,\n",
    "    features=features_to_use,\n",
    "    cv_splitter=skf,\n",
    "    params=xgb_params,\n",
    ")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n🎉 CROSS-VALIDATION TRAINING COMPLETED!\")\n",
    "print(f\"⏰ Training finished at: {time.strftime('%H:%M:%S')}\")\n",
    "print(f\"⏱️ Total training time: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "print(f\"📊 Overall Accuracy: {training_results['overall_accuracy']:.4f}\")\n",
    "print(f\"📊 Overall MAP@3: {training_results['overall_map3']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7da959",
   "metadata": {},
   "source": [
    "## 📊 9. Comprehensive Model Evaluation & Analysis\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Analyze model performance, stability, and competition readiness through statistical metrics and performance indicators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244fa935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CROSS-VALIDATION RESULTS EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"📊 CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract results from training\n",
    "cv_results_df = pd.DataFrame(training_results['cv_scores'])\n",
    "\n",
    "# Calculate statistics\n",
    "accuracy_mean = cv_results_df['accuracy'].mean()\n",
    "accuracy_std = cv_results_df['accuracy'].std()\n",
    "map3_mean = cv_results_df['map3'].mean()\n",
    "map3_std = cv_results_df['map3'].std()\n",
    "\n",
    "print(f\"🎯 FINAL METRICS:\")\n",
    "print(f\"  📈 Cross-Validation Accuracy: {accuracy_mean:.4f} ± {accuracy_std:.4f}\")\n",
    "print(f\"  📈 Cross-Validation MAP@3:    {map3_mean:.4f} ± {map3_std:.4f}\")\n",
    "print(f\"  📈 Out-of-Fold Accuracy:      {training_results['overall_accuracy']:.4f}\")\n",
    "print(f\"  📈 Out-of-Fold MAP@3:         {training_results['overall_map3']:.4f}\")\n",
    "\n",
    "# Stability evaluation\n",
    "accuracy_cv = accuracy_std / accuracy_mean if accuracy_mean > 0 else 0\n",
    "map3_cv = map3_std / map3_mean if map3_mean > 0 else 0\n",
    "\n",
    "print(f\"\\n🔍 STABILITY ANALYSIS:\")\n",
    "print(f\"  📊 Coefficient of variation (Accuracy): {accuracy_cv:.3f}\")\n",
    "print(f\"  📊 Coefficient of variation (MAP@3):    {map3_cv:.3f}\")\n",
    "print(f\"  {'✅ Stable model' if accuracy_cv < 0.05 else '⚠️ Variable model'} (Accuracy CV < 0.05)\")\n",
    "print(f\"  {'✅ Stable model' if map3_cv < 0.05 else '⚠️ Variable model'} (MAP@3 CV < 0.05)\")\n",
    "\n",
    "# Training time analysis\n",
    "avg_fold_time = cv_results_df['training_time'].mean()\n",
    "print(f\"\\n⏱️ TRAINING TIMES:\")\n",
    "print(f\"  📊 Average time per fold: {avg_fold_time:.1f}s\")\n",
    "print(f\"  📊 Total time: {total_time:.1f}s ({total_time/60:.1f}min)\")\n",
    "\n",
    "# Detailed results by fold\n",
    "print(f\"\\n📋 DETAILED RESULTS BY FOLD:\")\n",
    "print(\"Fold  Accuracy   MAP@3    Best_Iter  Time(s)\")\n",
    "print(\"-\" * 50)\n",
    "for _, row in cv_results_df.iterrows():\n",
    "    print(f\"{row['fold']:2.0f}    {row['accuracy']:.4f}   {row['map3']:.4f}     {row['best_iteration']:4.0f}   {row['training_time']:6.1f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"Mean  {accuracy_mean:.4f}   {map3_mean:.4f}     {cv_results_df['best_iteration'].mean():4.0f}   {avg_fold_time:6.1f}\")\n",
    "\n",
    "# Feature importance analysis\n",
    "if training_results['feature_importance'] is not None:\n",
    "    print(f\"\\n🔍 TOP 10 MOST IMPORTANT FEATURES:\")\n",
    "    print(\"Rank  Feature               Importance\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, (_, row) in enumerate(training_results['feature_importance'].head(10).iterrows()):\n",
    "        print(f\"{i+1:2d}.   {row['feature']:20} {row['mean']:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c09ef9c",
   "metadata": {},
   "source": [
    "## 💾 10. Model Persistence & Competition Submission\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Save essential model artifacts and performance metrics for competition submission and reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaeaef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FILE SAVING CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Configure model name based on MAP@3\n",
    "overall_map3 = training_results['overall_map3']\n",
    "model_name = f\"XGB_10CV_MAP@3-{overall_map3:.5f}\".replace('.', '')\n",
    "model_dir = f\"../models/XGB/{N_SPLITS}CV/{model_name}\"\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "print(f\"📁 MODEL DIRECTORY:\")\n",
    "print(f\"  {model_dir}\")\n",
    "\n",
    "# File name configuration - KAGGLE COMPETITION RECOMMENDED AND ESSENTIALS ONLY\n",
    "base_filename = model_name\n",
    "files_to_create = {\n",
    "    'hparams': f\"{base_filename}_hparams.json\",                 # ✅ RECOMMENDED - Hyperparameters for reproducibility\n",
    "    'metrics': f\"{base_filename}_metrics.json\",                 # ✅ RECOMMENDED - Performance metrics and config\n",
    "    'submission': f\"{base_filename}_submission.csv\",            # ✅ ESSENTIAL - Competition submission file\n",
    "    'submission_info': f\"{base_filename}_submission_info.json\"  # ✅ RECOMMENDED - Submission metadata\n",
    "}\n",
    "\n",
    "print(f\"\\n📝 FILES TO CREATE:\")\n",
    "for file_type, filename in files_to_create.items():\n",
    "    print(f\"  {file_type:15}: {filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf5945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SAVE HYPERPARAMETERS AND METRICS - COMPETITION\n",
    "# =============================================================================\n",
    "\n",
    "# Extract metrics from training results\n",
    "cv_results_df = pd.DataFrame(training_results['cv_scores'])\n",
    "map3_mean = cv_results_df['map3'].mean()\n",
    "map3_std = cv_results_df['map3'].std()\n",
    "accuracy_mean = cv_results_df['accuracy'].mean()\n",
    "accuracy_std = cv_results_df['accuracy'].std()\n",
    "\n",
    "# 1. HYPERPARAMETERS DATA\n",
    "hparams_data = {\n",
    "    \"model_type\": \"XGBClassifier\",\n",
    "    \"model_abbreviation\": \"XGB\",\n",
    "    \"cv_strategy\": f\"{N_SPLITS}-Fold Stratified Cross Validation\",\n",
    "    \"optimization_method\": \"Manual hyperparameter tuning\",\n",
    "    \"ensemble_method\": \"Average of fold predictions\",\n",
    "    \n",
    "    # Fixed hyperparameters used\n",
    "    \"hyperparameters\": xgb_params,\n",
    "    \n",
    "    # General configuration\n",
    "    \"features_selected\": features_to_use,\n",
    "    \"num_features\": len(features_to_use),\n",
    "    \"class_weights_used\": True,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"cv_splits\": N_SPLITS,\n",
    "    \"total_models\": len(training_results['models']),\n",
    "    \"early_stopping_rounds\": es\n",
    "}\n",
    "\n",
    "# 2. METRICS DATA  \n",
    "metrics_data = {\n",
    "    \"model_type\": \"XGBClassifier\",\n",
    "    \"model_abbreviation\": \"XGB\",\n",
    "    \"tier\": \"10_FOLD_CV\",\n",
    "    \"target_variable\": \"Fertilizer Name\",\n",
    "    \"cv_strategy\": f\"{N_SPLITS}-Fold Stratified Cross Validation\",\n",
    "    \"optimization_method\": \"Manual hyperparameter tuning\",\n",
    "    \n",
    "    # Main performance metrics\n",
    "    \"map3_score_cv_mean\": float(map3_mean),\n",
    "    \"map3_score_cv_std\": float(map3_std),\n",
    "    \"map3_score_oof\": float(training_results['overall_map3']),\n",
    "    \"accuracy_cv_mean\": float(accuracy_mean),\n",
    "    \"accuracy_cv_std\": float(accuracy_std),\n",
    "    \"accuracy_oof\": float(training_results['overall_accuracy']),\n",
    "    \n",
    "    # Model configuration\n",
    "    \"num_classes\": len(label_encoders['target'].classes_),\n",
    "    \"features_used\": len(features_to_use),\n",
    "    \"features_list\": features_to_use,\n",
    "    \"cv_folds\": N_SPLITS,\n",
    "    \"total_models_trained\": len(training_results['models']),\n",
    "    \n",
    "    # Detailed fold results\n",
    "    \"fold_results\": training_results['cv_scores'],\n",
    "    \n",
    "    # Stability statistics\n",
    "    \"accuracy_cv_coefficient\": float(accuracy_std / accuracy_mean) if accuracy_mean > 0 else 0.0,\n",
    "    \"map3_cv_coefficient\": float(map3_std / map3_mean) if map3_mean > 0 else 0.0,\n",
    "    \n",
    "    # Training performance\n",
    "    \"training_time_total\": float(total_time),\n",
    "    \"training_time_per_fold_avg\": float(cv_results_df['training_time'].mean()),\n",
    "    \n",
    "    # Hyperparameters used\n",
    "    \"hyperparameters\": xgb_params,\n",
    "    \n",
    "    # Feature importance summary (lightweight)\n",
    "    \"top_features\": training_results['feature_importance'].head(10).to_dict('records') if training_results['feature_importance'] is not None else None,\n",
    "    \n",
    "    # Metadata\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"kaggle_competition\": \"playground-series-s5e6\",\n",
    "    \"ensemble_method\": \"Average of 10-fold CV models\",\n",
    "    \"models_saved\": False,  # Models used in-memory only for predictions\n",
    "    \"memory_optimized\": True\n",
    "}\n",
    "\n",
    "# Save both files\n",
    "hparams_file = os.path.join(model_dir, files_to_create['hparams'])\n",
    "metrics_file = os.path.join(model_dir, files_to_create['metrics'])\n",
    "\n",
    "with open(hparams_file, 'w') as f:\n",
    "    json.dump(hparams_data, f, indent=2)\n",
    "    \n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(metrics_data, f, indent=2)\n",
    "\n",
    "print(f\"✅ Competition files saved:\")\n",
    "print(f\"  📄 Hyperparameters: {files_to_create['hparams']}\")\n",
    "print(f\"  📊 Metrics: {files_to_create['metrics']}\")\n",
    "print(f\"  💾 Size: Lightweight (~20-35 KB total)\")\n",
    "print(f\"  🚀 Competition ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956a61a",
   "metadata": {},
   "source": [
    "## 🔮 11. Test Predictions & Submission Generation\n",
    "\n",
    "**Why?**\n",
    "\n",
    "Generate final predictions using 10-fold ensemble and create Kaggle submission file with MAP@3 optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83527858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# GENERATE TEST PREDICTIONS AND CREATE KAGGLE SUBMISSION\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"🔮 Generating test predictions using {len(training_results['models'])}-model ensemble...\")\n",
    "\n",
    "# Generate ensemble predictions using all trained models\n",
    "test_predictions_all = []\n",
    "for fold_name, model in training_results['models'].items():\n",
    "    pred_proba = model.predict_proba(X_test_final)\n",
    "    test_predictions_all.append(pred_proba)\n",
    "\n",
    "# Average predictions across all folds\n",
    "test_predictions_ensemble = np.mean(test_predictions_all, axis=0)\n",
    "\n",
    "# Get top 3 predictions for each sample (MAP@3 format)\n",
    "test_top3_indices = np.argsort(test_predictions_ensemble, axis=1)[:, -3:][:, ::-1]\n",
    "\n",
    "# Convert prediction indices to fertilizer names\n",
    "test_top3_names = []\n",
    "for i in range(len(test_top3_indices)):\n",
    "    top3_for_sample = []\n",
    "    for j in range(3):\n",
    "        class_idx = test_top3_indices[i, j]\n",
    "        class_name = label_encoders['target'].inverse_transform([class_idx])[0]\n",
    "        top3_for_sample.append(class_name)\n",
    "    test_top3_names.append(top3_for_sample)\n",
    "\n",
    "# Create submission format (space-separated top 3 fertilizers)\n",
    "submission_predictions = [' '.join(top3_names) for top3_names in test_top3_names]\n",
    "\n",
    "# Create final submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'id': sample_submission['id'].copy(),\n",
    "    'Fertilizer Name': submission_predictions\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission_file = os.path.join(model_dir, files_to_create['submission'])\n",
    "submission.to_csv(submission_file, index=False)\n",
    "\n",
    "# Create submission metadata\n",
    "submission_info = {\n",
    "    \"model_type\": \"XGBClassifier\",\n",
    "    \"cv_strategy\": f\"{N_SPLITS}-Fold Stratified Cross Validation\",\n",
    "    \"ensemble_method\": \"Average of 10-fold CV models\",\n",
    "    \"map3_score_cv\": f\"{map3_mean:.5f} ± {map3_std:.5f}\",\n",
    "    \"map3_score_oof\": float(training_results['overall_map3']),\n",
    "    \"submission_file\": files_to_create['submission'],\n",
    "    \"num_predictions\": len(submission),\n",
    "    \"features_used\": len(features_to_use),\n",
    "    \"hyperparameters\": xgb_params,\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"kaggle_competition\": \"playground-series-s5e6\"\n",
    "}\n",
    "\n",
    "# Save submission metadata\n",
    "submission_info_file = os.path.join(model_dir, files_to_create['submission_info'])\n",
    "with open(submission_info_file, 'w') as f:\n",
    "    json.dump(submission_info, f, indent=2)\n",
    "\n",
    "print(f\"✅ KAGGLE SUBMISSION READY\")\n",
    "print(f\"  📄 File: {files_to_create['submission']}\")\n",
    "print(f\"  📊 Samples: {len(submission):,}\")\n",
    "print(f\"  📈 MAP@3 (CV): {map3_mean:.5f} ± {map3_std:.5f}\")\n",
    "print(f\"  📈 MAP@3 (OOF): {training_results['overall_map3']:.5f}\")\n",
    "print(f\"  ⏱️ Training: {total_time/60:.1f} minutes\")\n",
    "print(f\"  🚀 Ready for competition upload!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79ff26",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align: center; padding: 20px; margin: 20px 0; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); border-radius: 15px; box-shadow: 0 8px 16px rgba(0,0,0,0.1); color: white;\">\n",
    "  <h2 style=\"color: white; margin-bottom: 15px;\">🎉 Thank You for Reading! 🎉</h2>\n",
    "  <p style=\"font-size: 18px; margin-bottom: 10px;\"><strong>Dear Fellow Kagglers & Data Scientists,</strong> 🤝</p>\n",
    "  <p style=\"font-size: 16px; line-height: 1.6;\">Thank you for taking the time to explore this notebook! I hope you found the analysis insightful and the methodology useful for your own projects.</p>\n",
    "</div>\n",
    "\n",
    "### 💬 **Your Feedback Matters!**\n",
    "\n",
    "- **👍 Upvote** if this notebook was helpful\n",
    "- **💭 Comment** with your thoughts, suggestions, or questions\n",
    "- **🔧 Fork & Improve** - I'd love to see your enhancements!\n",
    "- **📊 Share your results** and insights\n",
    "\n",
    "---\n",
    "\n",
    "**🔗 Connect & Collaborate:**\n",
    "- Follow for more data science content and competition solutions\n",
    "- Join the discussion in the comments below\n",
    "- Share this notebook if you found it valuable\n",
    "\n",
    "**🏆 Keep exploring, keep learning, keep growing!** 🌱\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe1c764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
