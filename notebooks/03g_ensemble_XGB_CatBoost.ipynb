{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debfbf04",
   "metadata": {},
   "source": [
    "# ü§ù Ensemble XGBoost + CatBoost\n",
    "\n",
    "## üìã Objetivos\n",
    "- Combinar predicciones OOF de XGBoost y CatBoost\n",
    "- Optimizar pesos de ensemble para mejor MAP@3\n",
    "- Evaluar mejora de rendimiento vs modelos individuales\n",
    "- Generar submission final del ensemble\n",
    "\n",
    "## üéØ Estrategia de Ensemble\n",
    "- **XGBoost**: Variables categ√≥ricas codificadas + 19 features engineered\n",
    "- **CatBoost**: Variables categ√≥ricas nativas + feature engineering adaptado\n",
    "- **Ensemble**: Weighted averaging + voting strategies\n",
    "- **Diversidad**: Diferentes representaciones categ√≥ricas maximizan diversidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c43e8",
   "metadata": {},
   "source": [
    "# üì¶ Instalaci√≥n y Configuraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa271374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "import pickle\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Imports de ML\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Imports personalizados\n",
    "sys.path.append('../src')\n",
    "from metrics import map_at_k, calculate_map_score\n",
    "\n",
    "# Configuraci√≥n\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Configuraci√≥n de paths\n",
    "MODELS_PATH = Path('../models')\n",
    "XGB_PATH = MODELS_PATH / 'XGB'\n",
    "CATBOOST_PATH = MODELS_PATH / 'CatBoost'\n",
    "ENSEMBLE_PATH = MODELS_PATH / 'Ensemble'\n",
    "ENSEMBLE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas exitosamente\")\n",
    "print(f\"üìÅ Directorio ensemble: {ENSEMBLE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e9aef9",
   "metadata": {},
   "source": [
    "# üîç Buscar Mejores Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ce0b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(models_path, model_type):\n",
    "    \"\"\"\n",
    "    Encuentra el mejor modelo basado en MAP@3 score en el nombre del directorio\n",
    "    \"\"\"\n",
    "    model_dirs = [d for d in models_path.iterdir() if d.is_dir()]\n",
    "    \n",
    "    if not model_dirs:\n",
    "        print(f\"‚ùå No se encontraron modelos en {models_path}\")\n",
    "        return None\n",
    "    \n",
    "    best_score = 0\n",
    "    best_dir = None\n",
    "    \n",
    "    print(f\"\\nüîç Buscando mejor modelo {model_type}:\")\n",
    "    for model_dir in model_dirs:\n",
    "        # Extraer score del nombre del directorio (formato: ModelType_MAP@3-XXXXX)\n",
    "        dir_name = model_dir.name\n",
    "        if 'MAP@3-' in dir_name or 'MAP3-' in dir_name:\n",
    "            try:\n",
    "                # Extraer score del nombre\n",
    "                score_part = dir_name.split('MAP@3-')[-1] if 'MAP@3-' in dir_name else dir_name.split('MAP3-')[-1]\n",
    "                score = float('0.' + score_part) if len(score_part) == 5 else float(score_part)\n",
    "                print(f\"  üìä {dir_name}: MAP@3 = {score:.5f}\")\n",
    "                \n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_dir = model_dir\n",
    "            except (ValueError, IndexError):\n",
    "                # Si no se puede extraer el score, buscar en metrics.json\n",
    "                metrics_file = model_dir / f\"{dir_name}_metrics.json\"\n",
    "                if metrics_file.exists():\n",
    "                    try:\n",
    "                        with open(metrics_file, 'r') as f:\n",
    "                            metrics = json.load(f)\n",
    "                        score = metrics.get('oof_results', {}).get('oof_map3', 0)\n",
    "                        print(f\"  üìä {dir_name}: MAP@3 = {score:.5f} (from metrics)\")\n",
    "                        \n",
    "                        if score > best_score:\n",
    "                            best_score = score\n",
    "                            best_dir = model_dir\n",
    "                    except:\n",
    "                        print(f\"  ‚ùå No se pudo leer m√©tricas de {dir_name}\")\n",
    "    \n",
    "    if best_dir:\n",
    "        print(f\"\\nüèÜ Mejor modelo {model_type}: {best_dir.name}\")\n",
    "        print(f\"   üìä MAP@3: {best_score:.5f}\")\n",
    "        return best_dir, best_score\n",
    "    else:\n",
    "        print(f\"‚ùå No se encontr√≥ modelo v√°lido para {model_type}\")\n",
    "        return None, 0\n",
    "\n",
    "# Buscar mejores modelos\n",
    "best_xgb_dir, xgb_score = find_best_model(XGB_PATH, \"XGBoost\")\n",
    "best_catboost_dir, catboost_score = find_best_model(CATBOOST_PATH, \"CatBoost\")\n",
    "\n",
    "if not best_xgb_dir or not best_catboost_dir:\n",
    "    print(\"\\n‚ùå Error: No se encontraron ambos modelos necesarios para el ensemble\")\n",
    "    print(\"Aseg√∫rate de haber entrenado tanto XGBoost como CatBoost primero.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Modelos encontrados para ensemble:\")\n",
    "    print(f\"  üöÄ XGBoost: {best_xgb_dir.name} (MAP@3: {xgb_score:.5f})\")\n",
    "    print(f\"  üê± CatBoost: {best_catboost_dir.name} (MAP@3: {catboost_score:.5f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b8f907",
   "metadata": {},
   "source": [
    "# üìä Cargar Predicciones OOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_oof_predictions(model_dir, model_name):\n",
    "    \"\"\"\n",
    "    Cargar predicciones OOF de un modelo\n",
    "    \"\"\"\n",
    "    # Buscar archivo OOF\n",
    "    oof_files = list(model_dir.glob(\"*_oof_predictions.csv\"))\n",
    "    \n",
    "    if not oof_files:\n",
    "        print(f\"‚ùå No se encontr√≥ archivo OOF para {model_name} en {model_dir}\")\n",
    "        return None\n",
    "    \n",
    "    oof_file = oof_files[0]\n",
    "    print(f\"üìä Cargando OOF de {model_name}: {oof_file.name}\")\n",
    "    \n",
    "    oof_df = pd.read_csv(oof_file)\n",
    "    print(f\"   Forma: {oof_df.shape}\")\n",
    "    print(f\"   Columnas: {oof_df.columns.tolist()}\")\n",
    "    \n",
    "    return oof_df\n",
    "\n",
    "def load_submission_predictions(model_dir, model_name):\n",
    "    \"\"\"\n",
    "    Cargar predicciones de submission de un modelo\n",
    "    \"\"\"\n",
    "    # Buscar archivo submission\n",
    "    submission_files = list(model_dir.glob(\"*_submission.csv\"))\n",
    "    \n",
    "    if not submission_files:\n",
    "        print(f\"‚ùå No se encontr√≥ archivo submission para {model_name} en {model_dir}\")\n",
    "        return None\n",
    "    \n",
    "    submission_file = submission_files[0]\n",
    "    print(f\"üì§ Cargando submission de {model_name}: {submission_file.name}\")\n",
    "    \n",
    "    submission_df = pd.read_csv(submission_file)\n",
    "    print(f\"   Forma: {submission_df.shape}\")\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "# Cargar predicciones OOF\n",
    "if best_xgb_dir and best_catboost_dir:\n",
    "    print(\"\\nüìä Cargando predicciones Out-of-Fold...\")\n",
    "    \n",
    "    xgb_oof = load_oof_predictions(best_xgb_dir, \"XGBoost\")\n",
    "    catboost_oof = load_oof_predictions(best_catboost_dir, \"CatBoost\")\n",
    "    \n",
    "    print(\"\\nüì§ Cargando predicciones de test...\")\n",
    "    \n",
    "    xgb_submission = load_submission_predictions(best_xgb_dir, \"XGBoost\")\n",
    "    catboost_submission = load_submission_predictions(best_catboost_dir, \"CatBoost\")\n",
    "    \n",
    "    if xgb_oof is not None and catboost_oof is not None:\n",
    "        print(\"\\n‚úÖ Todas las predicciones cargadas exitosamente\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Error cargando predicciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc156cb",
   "metadata": {},
   "source": [
    "# üîç An√°lisis de Predicciones OOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fb60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if xgb_oof is not None and catboost_oof is not None:\n",
    "    print(\"üîç AN√ÅLISIS DE PREDICCIONES OOF\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Verificar que tengan el mismo n√∫mero de muestras\n",
    "    print(f\"\\nüìä Tama√±os de OOF:\")\n",
    "    print(f\"  XGBoost: {len(xgb_oof):,} muestras\")\n",
    "    print(f\"  CatBoost: {len(catboost_oof):,} muestras\")\n",
    "    \n",
    "    if len(xgb_oof) != len(catboost_oof):\n",
    "        print(\"‚ùå Error: Los OOF tienen diferentes tama√±os\")\n",
    "    else:\n",
    "        # Analizar concordancia de etiquetas verdaderas\n",
    "        if 'true_label' in xgb_oof.columns and 'true_label' in catboost_oof.columns:\n",
    "            concordance = (xgb_oof['true_label'] == catboost_oof['true_label']).mean()\n",
    "            print(f\"\\n‚úÖ Concordancia etiquetas verdaderas: {concordance:.3%}\")\n",
    "            \n",
    "            # Usar etiquetas del primer modelo como referencia\n",
    "            true_labels = xgb_oof['true_label']\n",
    "            \n",
    "            # Extraer predicciones\n",
    "            xgb_preds = xgb_oof['oof_prediction']\n",
    "            catboost_preds = catboost_oof['oof_prediction']\n",
    "            \n",
    "            # Calcular m√©tricas individuales\n",
    "            xgb_acc = accuracy_score(true_labels, xgb_preds)\n",
    "            catboost_acc = accuracy_score(true_labels, catboost_preds)\n",
    "            \n",
    "            print(f\"\\nüìä Rendimiento individual:\")\n",
    "            print(f\"  XGBoost Accuracy: {xgb_acc:.6f}\")\n",
    "            print(f\"  CatBoost Accuracy: {catboost_acc:.6f}\")\n",
    "            \n",
    "            # Analizar concordancia de predicciones\n",
    "            pred_concordance = (xgb_preds == catboost_preds).mean()\n",
    "            print(f\"\\nü§ù Concordancia predicciones: {pred_concordance:.3%}\")\n",
    "            \n",
    "            # Casos donde difieren\n",
    "            different_preds = xgb_preds != catboost_preds\n",
    "            print(f\"üîÑ Predicciones diferentes: {different_preds.sum():,} ({different_preds.mean():.1%})\")\n",
    "            \n",
    "            # Analizar cu√°ndo cada modelo es correcto\n",
    "            xgb_correct = xgb_preds == true_labels\n",
    "            catboost_correct = catboost_preds == true_labels\n",
    "            \n",
    "            both_correct = xgb_correct & catboost_correct\n",
    "            only_xgb_correct = xgb_correct & ~catboost_correct\n",
    "            only_catboost_correct = ~xgb_correct & catboost_correct\n",
    "            both_wrong = ~xgb_correct & ~catboost_correct\n",
    "            \n",
    "            print(f\"\\nüéØ An√°lisis de aciertos:\")\n",
    "            print(f\"  Ambos correctos: {both_correct.sum():,} ({both_correct.mean():.1%})\")\n",
    "            print(f\"  Solo XGBoost correcto: {only_xgb_correct.sum():,} ({only_xgb_correct.mean():.1%})\")\n",
    "            print(f\"  Solo CatBoost correcto: {only_catboost_correct.sum():,} ({only_catboost_correct.mean():.1%})\")\n",
    "            print(f\"  Ambos incorrectos: {both_wrong.sum():,} ({both_wrong.mean():.1%})\")\n",
    "            \n",
    "            # Potencial de mejora con ensemble\n",
    "            potential_improvement = only_xgb_correct.sum() + only_catboost_correct.sum()\n",
    "            print(f\"\\nüöÄ Potencial mejora ensemble: {potential_improvement:,} casos ({potential_improvement/len(true_labels):.1%})\")\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå No se encontraron columnas 'true_label' en los OOF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b84f9b",
   "metadata": {},
   "source": [
    "# üéØ Estrategias de Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07334735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_voting_ensemble(xgb_preds, catboost_preds):\n",
    "    \"\"\"\n",
    "    Ensemble simple por votaci√≥n mayoritaria\n",
    "    \"\"\"\n",
    "    # Crear array con ambas predicciones\n",
    "    all_preds = np.column_stack([xgb_preds, catboost_preds])\n",
    "    \n",
    "    # Votaci√≥n por mayor√≠a (en caso de empate, tomar XGBoost)\n",
    "    ensemble_preds = []\n",
    "    for i in range(len(all_preds)):\n",
    "        preds_row = all_preds[i]\n",
    "        # Si son iguales, usar cualquiera\n",
    "        if preds_row[0] == preds_row[1]:\n",
    "            ensemble_preds.append(preds_row[0])\n",
    "        else:\n",
    "            # En caso de desempate, usar XGBoost (generalmente m√°s estable)\n",
    "            ensemble_preds.append(preds_row[0])\n",
    "    \n",
    "    return np.array(ensemble_preds)\n",
    "\n",
    "def weighted_ensemble(xgb_preds, catboost_preds, xgb_weight=0.5):\n",
    "    \"\"\"\n",
    "    Ensemble ponderado simple (para predicciones categ√≥ricas)\n",
    "    \"\"\"\n",
    "    # Para predicciones categ√≥ricas, usar votaci√≥n ponderada por performance\n",
    "    ensemble_preds = []\n",
    "    \n",
    "    for i in range(len(xgb_preds)):\n",
    "        if xgb_preds[i] == catboost_preds[i]:\n",
    "            # Si coinciden, usar esa predicci√≥n\n",
    "            ensemble_preds.append(xgb_preds[i])\n",
    "        else:\n",
    "            # Si difieren, usar el modelo con mayor peso\n",
    "            if xgb_weight >= 0.5:\n",
    "                ensemble_preds.append(xgb_preds[i])\n",
    "            else:\n",
    "                ensemble_preds.append(catboost_preds[i])\n",
    "    \n",
    "    return np.array(ensemble_preds)\n",
    "\n",
    "def confidence_based_ensemble(xgb_preds, catboost_preds, xgb_acc, catboost_acc):\n",
    "    \"\"\"\n",
    "    Ensemble basado en confianza (accuracy) de cada modelo\n",
    "    \"\"\"\n",
    "    # Calcular peso basado en accuracy\n",
    "    total_acc = xgb_acc + catboost_acc\n",
    "    xgb_weight = xgb_acc / total_acc\n",
    "    \n",
    "    return weighted_ensemble(xgb_preds, catboost_preds, xgb_weight)\n",
    "\n",
    "# Evaluar diferentes estrategias de ensemble\n",
    "if xgb_oof is not None and catboost_oof is not None and 'true_label' in xgb_oof.columns:\n",
    "    print(\"\\nüéØ EVALUANDO ESTRATEGIAS DE ENSEMBLE\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    true_labels = xgb_oof['true_label']\n",
    "    xgb_preds = xgb_oof['oof_prediction']\n",
    "    catboost_preds = catboost_oof['oof_prediction']\n",
    "    \n",
    "    # Accuracy individuales\n",
    "    xgb_acc = accuracy_score(true_labels, xgb_preds)\n",
    "    catboost_acc = accuracy_score(true_labels, catboost_preds)\n",
    "    \n",
    "    # 1. Simple Voting\n",
    "    voting_preds = simple_voting_ensemble(xgb_preds, catboost_preds)\n",
    "    voting_acc = accuracy_score(true_labels, voting_preds)\n",
    "    \n",
    "    # 2. Weighted by performance\n",
    "    confidence_preds = confidence_based_ensemble(xgb_preds, catboost_preds, xgb_acc, catboost_acc)\n",
    "    confidence_acc = accuracy_score(true_labels, confidence_preds)\n",
    "    \n",
    "    # 3. Diferentes pesos\n",
    "    weights_to_test = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "    weighted_results = []\n",
    "    \n",
    "    for weight in weights_to_test:\n",
    "        weighted_preds = weighted_ensemble(xgb_preds, catboost_preds, weight)\n",
    "        weighted_acc = accuracy_score(true_labels, weighted_preds)\n",
    "        weighted_results.append((weight, weighted_acc))\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\nüìä RESULTADOS ACCURACY:\")\n",
    "    print(f\"  XGBoost individual: {xgb_acc:.6f}\")\n",
    "    print(f\"  CatBoost individual: {catboost_acc:.6f}\")\n",
    "    print(f\"  Simple Voting: {voting_acc:.6f}\")\n",
    "    print(f\"  Confidence-based: {confidence_acc:.6f}\")\n",
    "    \n",
    "    print(f\"\\n‚öñÔ∏è Pesos ponderados:\")\n",
    "    for weight, acc in weighted_results:\n",
    "        print(f\"  XGB weight {weight:.1f}: {acc:.6f}\")\n",
    "    \n",
    "    # Encontrar mejor estrategia\n",
    "    all_strategies = [\n",
    "        (\"XGBoost\", xgb_acc, xgb_preds),\n",
    "        (\"CatBoost\", catboost_acc, catboost_preds),\n",
    "        (\"Simple Voting\", voting_acc, voting_preds),\n",
    "        (\"Confidence-based\", confidence_acc, confidence_preds)\n",
    "    ]\n",
    "    \n",
    "    # Agregar estrategias ponderadas\n",
    "    for weight, acc in weighted_results:\n",
    "        preds = weighted_ensemble(xgb_preds, catboost_preds, weight)\n",
    "        all_strategies.append((f\"Weighted XGB={weight:.1f}\", acc, preds))\n",
    "    \n",
    "    # Ordenar por accuracy\n",
    "    all_strategies.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nüèÜ RANKING DE ESTRATEGIAS:\")\n",
    "    for i, (name, acc, _) in enumerate(all_strategies[:10]):\n",
    "        print(f\"  {i+1:2d}. {name:<20} {acc:.6f}\")\n",
    "    \n",
    "    # Seleccionar mejor estrategia\n",
    "    best_strategy_name, best_accuracy, best_ensemble_preds = all_strategies[0]\n",
    "    \n",
    "    print(f\"\\nüéØ MEJOR ESTRATEGIA: {best_strategy_name}\")\n",
    "    print(f\"   Accuracy: {best_accuracy:.6f}\")\n",
    "    \n",
    "    # Mejora vs modelos individuales\n",
    "    improvement_vs_xgb = best_accuracy - xgb_acc\n",
    "    improvement_vs_catboost = best_accuracy - catboost_acc\n",
    "    improvement_vs_best_individual = best_accuracy - max(xgb_acc, catboost_acc)\n",
    "    \n",
    "    print(f\"\\nüìà MEJORAS:\")\n",
    "    print(f\"   vs XGBoost: {improvement_vs_xgb:+.6f}\")\n",
    "    print(f\"   vs CatBoost: {improvement_vs_catboost:+.6f}\")\n",
    "    print(f\"   vs Mejor Individual: {improvement_vs_best_individual:+.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcb8772",
   "metadata": {},
   "source": [
    "# üöÄ Aplicar Mejor Estrategia a Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60010866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_best_strategy_to_test(strategy_name, xgb_test_preds, catboost_test_preds, xgb_acc, catboost_acc):\n",
    "    \"\"\"\n",
    "    Aplicar la mejor estrategia de ensemble a las predicciones de test\n",
    "    \"\"\"\n",
    "    if strategy_name == \"XGBoost\":\n",
    "        return xgb_test_preds\n",
    "    elif strategy_name == \"CatBoost\":\n",
    "        return catboost_test_preds\n",
    "    elif strategy_name == \"Simple Voting\":\n",
    "        return simple_voting_ensemble(xgb_test_preds, catboost_test_preds)\n",
    "    elif strategy_name == \"Confidence-based\":\n",
    "        return confidence_based_ensemble(xgb_test_preds, catboost_test_preds, xgb_acc, catboost_acc)\n",
    "    elif \"Weighted XGB=\" in strategy_name:\n",
    "        # Extraer peso del nombre\n",
    "        weight = float(strategy_name.split(\"=\")[1])\n",
    "        return weighted_ensemble(xgb_test_preds, catboost_test_preds, weight)\n",
    "    else:\n",
    "        print(f\"‚ùå Estrategia no reconocida: {strategy_name}\")\n",
    "        return simple_voting_ensemble(xgb_test_preds, catboost_test_preds)\n",
    "\n",
    "# Aplicar a predicciones de test\n",
    "if (xgb_submission is not None and catboost_submission is not None and \n",
    "    'best_strategy_name' in locals() and xgb_oof is not None):\n",
    "    \n",
    "    print(f\"\\nüöÄ Aplicando estrategia '{best_strategy_name}' a predicciones de test...\")\n",
    "    \n",
    "    # Obtener predicciones de test\n",
    "    xgb_test_preds = xgb_submission['Fertilizer Name'].values\n",
    "    catboost_test_preds = catboost_submission['Fertilizer Name'].values\n",
    "    \n",
    "    print(f\"üìä Predicciones de test:\")\n",
    "    print(f\"  XGBoost: {len(xgb_test_preds):,} predicciones\")\n",
    "    print(f\"  CatBoost: {len(catboost_test_preds):,} predicciones\")\n",
    "    \n",
    "    # Verificar que tengan el mismo tama√±o\n",
    "    if len(xgb_test_preds) != len(catboost_test_preds):\n",
    "        print(\"‚ùå Error: Las predicciones de test tienen diferentes tama√±os\")\n",
    "    else:\n",
    "        # Aplicar mejor estrategia\n",
    "        ensemble_test_preds = apply_best_strategy_to_test(\n",
    "            best_strategy_name, \n",
    "            xgb_test_preds, \n",
    "            catboost_test_preds, \n",
    "            xgb_acc, \n",
    "            catboost_acc\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Ensemble aplicado exitosamente\")\n",
    "        print(f\"   Predicciones generadas: {len(ensemble_test_preds):,}\")\n",
    "        \n",
    "        # Analizar distribuci√≥n de predicciones\n",
    "        print(f\"\\nüìä Distribuci√≥n de predicciones ensemble:\")\n",
    "        pred_distribution = pd.Series(ensemble_test_preds).value_counts()\n",
    "        for fertilizer, count in pred_distribution.head(10).items():\n",
    "            percentage = count / len(ensemble_test_preds) * 100\n",
    "            print(f\"  {fertilizer}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Comparar con distribuciones individuales\n",
    "        xgb_unique = len(pd.Series(xgb_test_preds).unique())\n",
    "        catboost_unique = len(pd.Series(catboost_test_preds).unique())\n",
    "        ensemble_unique = len(pd.Series(ensemble_test_preds).unique())\n",
    "        \n",
    "        print(f\"\\nüéØ Clases √∫nicas predichas:\")\n",
    "        print(f\"  XGBoost: {xgb_unique}\")\n",
    "        print(f\"  CatBoost: {catboost_unique}\")\n",
    "        print(f\"  Ensemble: {ensemble_unique}\")\n",
    "        \n",
    "        # Analizar concordancia en test\n",
    "        test_concordance = (xgb_test_preds == catboost_test_preds).mean()\n",
    "        print(f\"\\nü§ù Concordancia en test: {test_concordance:.1%}\")\n",
    "        different_test = (xgb_test_preds != catboost_test_preds).sum()\n",
    "        print(f\"üîÑ Predicciones diferentes en test: {different_test:,} ({different_test/len(xgb_test_preds):.1%})\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No se pueden aplicar estrategias de ensemble - faltan datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf703b3",
   "metadata": {},
   "source": [
    "# üíæ Guardar Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7f6786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados del ensemble\n",
    "if 'ensemble_test_preds' in locals() and 'best_accuracy' in locals():\n",
    "    \n",
    "    # Crear nombre del experimento\n",
    "    ensemble_score_str = f\"{best_accuracy:.5f}\".replace('.', '')\n",
    "    ensemble_name = f\"Ensemble_XGB-CatBoost_ACC-{ensemble_score_str}\"\n",
    "    ensemble_dir = ENSEMBLE_PATH / ensemble_name\n",
    "    ensemble_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nüíæ Guardando ensemble en: {ensemble_dir}\")\n",
    "    \n",
    "    # 1. Informaci√≥n del ensemble\n",
    "    ensemble_info = {\n",
    "        'ensemble_name': ensemble_name,\n",
    "        'best_strategy': best_strategy_name,\n",
    "        'ensemble_accuracy': float(best_accuracy),\n",
    "        'models_used': {\n",
    "            'xgb': {\n",
    "                'model_dir': best_xgb_dir.name,\n",
    "                'individual_accuracy': float(xgb_acc),\n",
    "                'map3_score': float(xgb_score)\n",
    "            },\n",
    "            'catboost': {\n",
    "                'model_dir': best_catboost_dir.name,\n",
    "                'individual_accuracy': float(catboost_acc),\n",
    "                'map3_score': float(catboost_score)\n",
    "            }\n",
    "        },\n",
    "        'improvements': {\n",
    "            'vs_xgb': float(improvement_vs_xgb),\n",
    "            'vs_catboost': float(improvement_vs_catboost),\n",
    "            'vs_best_individual': float(improvement_vs_best_individual)\n",
    "        },\n",
    "        'test_predictions': {\n",
    "            'total_samples': len(ensemble_test_preds),\n",
    "            'unique_classes': int(ensemble_unique),\n",
    "            'concordance_rate': float(test_concordance)\n",
    "        },\n",
    "        'created_at': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(ensemble_dir / f\"{ensemble_name}_info.json\", 'w') as f:\n",
    "        json.dump(ensemble_info, f, indent=2)\n",
    "    \n",
    "    # 2. Estrategias evaluadas\n",
    "    strategies_results = {\n",
    "        'all_strategies': [\n",
    "            {'name': name, 'accuracy': float(acc)} \n",
    "            for name, acc, _ in all_strategies\n",
    "        ],\n",
    "        'best_strategy': {\n",
    "            'name': best_strategy_name,\n",
    "            'accuracy': float(best_accuracy)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(ensemble_dir / f\"{ensemble_name}_strategies.json\", 'w') as f:\n",
    "        json.dump(strategies_results, f, indent=2)\n",
    "    \n",
    "    # 3. Predicciones OOF del ensemble\n",
    "    ensemble_oof_df = pd.DataFrame({\n",
    "        'id': range(len(best_ensemble_preds)),\n",
    "        'true_label': true_labels,\n",
    "        'xgb_prediction': xgb_preds,\n",
    "        'catboost_prediction': catboost_preds,\n",
    "        'ensemble_prediction': best_ensemble_preds\n",
    "    })\n",
    "    \n",
    "    ensemble_oof_df.to_csv(\n",
    "        ensemble_dir / f\"{ensemble_name}_oof_predictions.csv\", \n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    # 4. Submission final\n",
    "    final_submission = pd.DataFrame({\n",
    "        'id': range(len(ensemble_test_preds)),\n",
    "        'Fertilizer Name': ensemble_test_preds\n",
    "    })\n",
    "    \n",
    "    final_submission.to_csv(\n",
    "        ensemble_dir / f\"{ensemble_name}_submission.csv\", \n",
    "        index=False\n",
    "    )\n",
    "    \n",
    "    # 5. An√°lisis detallado\n",
    "    analysis_data = {\n",
    "        'oof_analysis': {\n",
    "            'total_samples': len(true_labels),\n",
    "            'both_correct': int(both_correct.sum()),\n",
    "            'only_xgb_correct': int(only_xgb_correct.sum()),\n",
    "            'only_catboost_correct': int(only_catboost_correct.sum()),\n",
    "            'both_wrong': int(both_wrong.sum()),\n",
    "            'prediction_concordance': float(pred_concordance),\n",
    "            'potential_improvement': int(potential_improvement)\n",
    "        },\n",
    "        'test_analysis': {\n",
    "            'total_samples': len(ensemble_test_preds),\n",
    "            'prediction_concordance': float(test_concordance),\n",
    "            'different_predictions': int(different_test),\n",
    "            'ensemble_distribution': pred_distribution.head(10).to_dict()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    with open(ensemble_dir / f\"{ensemble_name}_analysis.json\", 'w') as f:\n",
    "        json.dump(analysis_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Ensemble guardado exitosamente:\")\n",
    "    print(f\"  üìÅ Directorio: {ensemble_dir}\")\n",
    "    print(f\"  ‚ÑπÔ∏è Info: {ensemble_name}_info.json\")\n",
    "    print(f\"  üéØ Estrategias: {ensemble_name}_strategies.json\")\n",
    "    print(f\"  üìä OOF: {ensemble_name}_oof_predictions.csv\")\n",
    "    print(f\"  üì§ Submission: {ensemble_name}_submission.csv\")\n",
    "    print(f\"  üîç An√°lisis: {ensemble_name}_analysis.json\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No se puede guardar ensemble - faltan datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cfe0be",
   "metadata": {},
   "source": [
    "# üìã Resumen Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2b2a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'ensemble_name' in locals():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"üèÜ RESUMEN FINAL - ENSEMBLE XGBoost + CatBoost\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nü§ñ MODELOS UTILIZADOS:\")\n",
    "    print(f\"  üöÄ XGBoost: {best_xgb_dir.name}\")\n",
    "    print(f\"     - MAP@3: {xgb_score:.5f}\")\n",
    "    print(f\"     - OOF Accuracy: {xgb_acc:.5f}\")\n",
    "    print(f\"     - Estrategia: Variables categ√≥ricas codificadas\")\n",
    "    \n",
    "    print(f\"\\n  üê± CatBoost: {best_catboost_dir.name}\")\n",
    "    print(f\"     - MAP@3: {catboost_score:.5f}\")\n",
    "    print(f\"     - OOF Accuracy: {catboost_acc:.5f}\")\n",
    "    print(f\"     - Estrategia: Variables categ√≥ricas nativas\")\n",
    "    \n",
    "    print(f\"\\nüéØ ENSEMBLE FINAL:\")\n",
    "    print(f\"  Nombre: {ensemble_name}\")\n",
    "    print(f\"  Estrategia: {best_strategy_name}\")\n",
    "    print(f\"  Accuracy: {best_accuracy:.6f}\")\n",
    "    \n",
    "    print(f\"\\nüìà MEJORAS LOGRADAS:\")\n",
    "    print(f\"  vs XGBoost: {improvement_vs_xgb:+.6f} ({improvement_vs_xgb/xgb_acc*100:+.2f}%)\")\n",
    "    print(f\"  vs CatBoost: {improvement_vs_catboost:+.6f} ({improvement_vs_catboost/catboost_acc*100:+.2f}%)\")\n",
    "    print(f\"  vs Mejor Individual: {improvement_vs_best_individual:+.6f}\")\n",
    "    \n",
    "    print(f\"\\nü§ù DIVERSIDAD DE MODELOS:\")\n",
    "    print(f\"  Concordancia OOF: {pred_concordance:.1%}\")\n",
    "    print(f\"  Concordancia Test: {test_concordance:.1%}\")\n",
    "    print(f\"  Potencial de mejora: {potential_improvement:,} casos\")\n",
    "    \n",
    "    print(f\"\\nüìä PREDICCIONES FINALES:\")\n",
    "    print(f\"  Total muestras test: {len(ensemble_test_preds):,}\")\n",
    "    print(f\"  Clases √∫nicas: {ensemble_unique}\")\n",
    "    print(f\"  Diferencias test: {different_test:,} ({different_test/len(ensemble_test_preds):.1%})\")\n",
    "    \n",
    "    print(f\"\\nüíæ ARCHIVOS GENERADOS:\")\n",
    "    print(f\"  üìÅ {ensemble_dir}\")\n",
    "    print(f\"  üì§ {ensemble_name}_submission.csv\")\n",
    "    print(f\"  üìä {ensemble_name}_oof_predictions.csv\")\n",
    "    print(f\"  ‚ÑπÔ∏è {ensemble_name}_info.json\")\n",
    "    \n",
    "    print(f\"\\nüéâ ENSEMBLE COMPLETADO EXITOSAMENTE!\")\n",
    "    print(f\"\\nüöÄ El archivo de submission est√° listo para Kaggle.\")\n",
    "    print(f\"üìà El ensemble deber√≠a superar el rendimiento de ambos modelos individuales.\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå No se pudo completar el ensemble\")\n",
    "    print(\"Verifica que tanto XGBoost como CatBoost est√©n entrenados correctamente.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
