{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc19289",
   "metadata": {},
   "source": [
    "# Preprocesamiento de Datos - Kaggle Playground Series S5E6\n",
    "## Fertilizer Prediction: Data Preprocessing Pipeline\n",
    "\n",
    "**Autor:** Félix  \n",
    "**Fecha:** 2025-01-21  \n",
    "**Objetivo:** Preparar datos para modelado basándose en insights del EDA\n",
    "\n",
    "---\n",
    "\n",
    "### 📋 **Pipeline de Preprocesamiento**\n",
    "\n",
    "**🎯 Basado en conclusiones del EDA 01_eda_template.ipynb:**\n",
    "\n",
    "1. **Feature Engineering**: Implementar las 20+ features identificadas como valiosas\n",
    "2. **Encoding**: Variables categóricas con high cardinality (Soil_Crop_Combo)\n",
    "3. **Scaling**: Preparar variables numéricas para Random Forest baseline\n",
    "4. **Splitting**: División estratificada para 22 clases de fertilizantes\n",
    "5. **Export**: Datasets listos para modelado\n",
    "\n",
    "**📊 Dataset Original:**\n",
    "- 750,000 muestras\n",
    "- 22 tipos de fertilizantes\n",
    "- 9 variables predictoras + features engineered\n",
    "- Sin valores faltantes\n",
    "\n",
    "**🎯 Target Final:** Datos optimizados para Random Forest baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb53c72",
   "metadata": {},
   "source": [
    "## 📚 1. Importación de Librerías\n",
    "\n",
    "### Librerías para Preprocesamiento de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36ec496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Librerías importadas correctamente\n",
      "📊 Pandas: 2.2.3\n",
      "🔢 NumPy: 2.2.6\n",
      "🤖 Scikit-learn: sklearn disponible\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Para visualización (opcional en preprocessing)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"✅ Librerías importadas correctamente\")\n",
    "print(f\"📊 Pandas: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy: {np.__version__}\")\n",
    "print(f\"🤖 Scikit-learn: sklearn disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17092a4b",
   "metadata": {},
   "source": [
    "## 📊 2. Carga de Datos\n",
    "\n",
    "### Carga de Datasets Originales\n",
    "\n",
    "Cargamos los datos desde los archivos CSV y verificamos su estructura básica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83ff625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CARGA DE DATOS ===\n",
      "\n",
      "📁 Archivos cargados:\n",
      "  • Train: (750000, 10) (filas, columnas)\n",
      "  • Test: (250000, 9)\n",
      "  • Sample Submission: (250000, 2)\n",
      "\n",
      "🔍 Columnas en Train:\n",
      "  ['id', 'Temparature', 'Humidity', 'Moisture', 'Soil Type', 'Crop Type', 'Nitrogen', 'Potassium', 'Phosphorous', 'Fertilizer Name']\n",
      "\n",
      "🔍 Columnas en Test:\n",
      "  ['id', 'Temparature', 'Humidity', 'Moisture', 'Soil Type', 'Crop Type', 'Nitrogen', 'Potassium', 'Phosphorous']\n",
      "\n",
      "📊 Info Train Dataset:\n",
      "  • Tipos de datos únicos: {dtype('int64'): 7, dtype('O'): 3}\n",
      "  • Valores nulos: 0\n",
      "  • Fertilizantes únicos: 7\n",
      "\n",
      "✅ Datos cargados correctamente - Listos para preprocesamiento\n",
      "\n",
      "📁 Archivos cargados:\n",
      "  • Train: (750000, 10) (filas, columnas)\n",
      "  • Test: (250000, 9)\n",
      "  • Sample Submission: (250000, 2)\n",
      "\n",
      "🔍 Columnas en Train:\n",
      "  ['id', 'Temparature', 'Humidity', 'Moisture', 'Soil Type', 'Crop Type', 'Nitrogen', 'Potassium', 'Phosphorous', 'Fertilizer Name']\n",
      "\n",
      "🔍 Columnas en Test:\n",
      "  ['id', 'Temparature', 'Humidity', 'Moisture', 'Soil Type', 'Crop Type', 'Nitrogen', 'Potassium', 'Phosphorous']\n",
      "\n",
      "📊 Info Train Dataset:\n",
      "  • Tipos de datos únicos: {dtype('int64'): 7, dtype('O'): 3}\n",
      "  • Valores nulos: 0\n",
      "  • Fertilizantes únicos: 7\n",
      "\n",
      "✅ Datos cargados correctamente - Listos para preprocesamiento\n"
     ]
    }
   ],
   "source": [
    "# Cargar datasets\n",
    "print(\"=== CARGA DE DATOS ===\")\n",
    "\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "print(f\"\\n📁 Archivos cargados:\")\n",
    "print(f\"  • Train: {train_df.shape} (filas, columnas)\")\n",
    "print(f\"  • Test: {test_df.shape}\")\n",
    "print(f\"  • Sample Submission: {sample_submission.shape}\")\n",
    "\n",
    "# Verificar estructura\n",
    "print(f\"\\n🔍 Columnas en Train:\")\n",
    "print(f\"  {list(train_df.columns)}\")\n",
    "\n",
    "print(f\"\\n🔍 Columnas en Test:\")\n",
    "print(f\"  {list(test_df.columns)}\")\n",
    "\n",
    "# Info básica\n",
    "print(f\"\\n📊 Info Train Dataset:\")\n",
    "print(f\"  • Tipos de datos únicos: {train_df.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"  • Valores nulos: {train_df.isnull().sum().sum()}\")\n",
    "print(f\"  • Fertilizantes únicos: {train_df['Fertilizer Name'].nunique()}\")\n",
    "\n",
    "print(f\"\\n✅ Datos cargados correctamente - Listos para preprocesamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede6fec6",
   "metadata": {},
   "source": [
    "## 🕳️ 3. Análisis de Valores Faltantes\n",
    "\n",
    "### Verificación de Calidad de Datos\n",
    "\n",
    "**Según el EDA:** El dataset no presenta valores faltantes, pero verificamos para confirmar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55774f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ANÁLISIS DE VALORES FALTANTES ===\n",
      "\n",
      "📊 DATASET DE ENTRENAMIENTO:\n",
      "  ✅ No hay valores faltantes\n",
      "  📋 Resumen de calidad:\n",
      "    • Filas totales: 750,000\n",
      "    • Columnas totales: 10\n",
      "  ✅ No hay valores faltantes\n",
      "  📋 Resumen de calidad:\n",
      "    • Filas totales: 750,000\n",
      "    • Columnas totales: 10\n",
      "    • Valores duplicados: 0\n",
      "    • Valores duplicados: 0\n",
      "    • Memoria utilizada: 157.8 MB\n",
      "\n",
      "📊 DATASET DE PRUEBA:\n",
      "  ✅ No hay valores faltantes\n",
      "  📋 Resumen de calidad:\n",
      "    • Filas totales: 250,000\n",
      "    • Columnas totales: 9\n",
      "    • Memoria utilizada: 157.8 MB\n",
      "\n",
      "📊 DATASET DE PRUEBA:\n",
      "  ✅ No hay valores faltantes\n",
      "  📋 Resumen de calidad:\n",
      "    • Filas totales: 250,000\n",
      "    • Columnas totales: 9\n",
      "    • Valores duplicados: 0\n",
      "    • Memoria utilizada: 39.5 MB\n",
      "\n",
      "🔄 CONSISTENCIA ENTRE DATASETS:\n",
      "  • Columnas comunes: 9 de 9\n",
      "    ['Nitrogen', 'Phosphorous', 'Temparature', 'Soil Type', 'Crop Type', 'Moisture', 'Potassium', 'id', 'Humidity']\n",
      "\n",
      "📊 Rangos de variables numéricas (Train vs Test):\n",
      "  • Temparature    : Train [25.0, 38.0] | Test [25.0, 38.0]\n",
      "  • Humidity       : Train [50.0, 72.0] | Test [50.0, 72.0]\n",
      "  • Moisture       : Train [25.0, 65.0] | Test [25.0, 65.0]\n",
      "  • Nitrogen       : Train [4.0, 42.0] | Test [4.0, 42.0]\n",
      "  • Potassium      : Train [0.0, 19.0] | Test [0.0, 19.0]\n",
      "  • Phosphorous    : Train [0.0, 42.0] | Test [0.0, 42.0]\n",
      "\n",
      "📊 Categorías únicas (Train vs Test):\n",
      "    • Valores duplicados: 0\n",
      "    • Memoria utilizada: 39.5 MB\n",
      "\n",
      "🔄 CONSISTENCIA ENTRE DATASETS:\n",
      "  • Columnas comunes: 9 de 9\n",
      "    ['Nitrogen', 'Phosphorous', 'Temparature', 'Soil Type', 'Crop Type', 'Moisture', 'Potassium', 'id', 'Humidity']\n",
      "\n",
      "📊 Rangos de variables numéricas (Train vs Test):\n",
      "  • Temparature    : Train [25.0, 38.0] | Test [25.0, 38.0]\n",
      "  • Humidity       : Train [50.0, 72.0] | Test [50.0, 72.0]\n",
      "  • Moisture       : Train [25.0, 65.0] | Test [25.0, 65.0]\n",
      "  • Nitrogen       : Train [4.0, 42.0] | Test [4.0, 42.0]\n",
      "  • Potassium      : Train [0.0, 19.0] | Test [0.0, 19.0]\n",
      "  • Phosphorous    : Train [0.0, 42.0] | Test [0.0, 42.0]\n",
      "\n",
      "📊 Categorías únicas (Train vs Test):\n",
      "  • Soil Type:\n",
      "    Train: 5 categorías\n",
      "    Test: 5 categorías\n",
      "    ✅ Todas las categorías de test están en train\n",
      "  • Crop Type:\n",
      "    Train: 11 categorías\n",
      "    Test: 11 categorías\n",
      "    ✅ Todas las categorías de test están en train\n",
      "\n",
      "🎯 CONCLUSIÓN ANÁLISIS DE CALIDAD:\n",
      "  ✅ Datasets limpios - sin valores faltantes\n",
      "  ✅ Consistencia entre train y test verificada\n",
      "  ✅ Listos para feature engineering\n",
      "  • Soil Type:\n",
      "    Train: 5 categorías\n",
      "    Test: 5 categorías\n",
      "    ✅ Todas las categorías de test están en train\n",
      "  • Crop Type:\n",
      "    Train: 11 categorías\n",
      "    Test: 11 categorías\n",
      "    ✅ Todas las categorías de test están en train\n",
      "\n",
      "🎯 CONCLUSIÓN ANÁLISIS DE CALIDAD:\n",
      "  ✅ Datasets limpios - sin valores faltantes\n",
      "  ✅ Consistencia entre train y test verificada\n",
      "  ✅ Listos para feature engineering\n"
     ]
    }
   ],
   "source": [
    "# Análisis de valores faltantes\n",
    "print(\"=== ANÁLISIS DE VALORES FALTANTES ===\")\n",
    "\n",
    "# Función para analizar valores faltantes\n",
    "def analyze_missing_values(df, dataset_name):\n",
    "    print(f\"\\n📊 {dataset_name}:\")\n",
    "    \n",
    "    # Contar nulos\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "    \n",
    "    # Crear resumen\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Columna': df.columns,\n",
    "        'Valores_Nulos': null_counts.values,\n",
    "        'Porcentaje': null_percentages.values\n",
    "    }).sort_values('Valores_Nulos', ascending=False)\n",
    "    \n",
    "    # Mostrar solo columnas con valores faltantes\n",
    "    has_nulls = missing_summary[missing_summary['Valores_Nulos'] > 0]\n",
    "    \n",
    "    if len(has_nulls) > 0:\n",
    "        print(f\"  ⚠️ Columnas con valores faltantes:\")\n",
    "        for _, row in has_nulls.iterrows():\n",
    "            print(f\"    • {row['Columna']}: {row['Valores_Nulos']} ({row['Porcentaje']:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  ✅ No hay valores faltantes\")\n",
    "    \n",
    "    # Verificar otros problemas de calidad\n",
    "    print(f\"  📋 Resumen de calidad:\")\n",
    "    print(f\"    • Filas totales: {len(df):,}\")\n",
    "    print(f\"    • Columnas totales: {len(df.columns)}\")\n",
    "    print(f\"    • Valores duplicados: {df.duplicated().sum():,}\")\n",
    "    print(f\"    • Memoria utilizada: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    return missing_summary\n",
    "\n",
    "# Analizar ambos datasets\n",
    "train_missing = analyze_missing_values(train_df, \"DATASET DE ENTRENAMIENTO\")\n",
    "test_missing = analyze_missing_values(test_df, \"DATASET DE PRUEBA\")\n",
    "\n",
    "# Verificar consistencia entre train y test\n",
    "print(f\"\\n🔄 CONSISTENCIA ENTRE DATASETS:\")\n",
    "\n",
    "# Columnas comunes (excluyendo target)\n",
    "common_cols = set(test_df.columns) & set(train_df.columns)\n",
    "print(f\"  • Columnas comunes: {len(common_cols)} de {len(test_df.columns)}\")\n",
    "print(f\"    {list(common_cols)}\")\n",
    "\n",
    "# Verificar rangos de variables numéricas\n",
    "numeric_cols = ['Temparature', 'Humidity', 'Moisture', 'Nitrogen', 'Potassium', 'Phosphorous']\n",
    "print(f\"\\n📊 Rangos de variables numéricas (Train vs Test):\")\n",
    "for col in numeric_cols:\n",
    "    if col in train_df.columns and col in test_df.columns:\n",
    "        train_range = f\"[{train_df[col].min():.1f}, {train_df[col].max():.1f}]\"\n",
    "        test_range = f\"[{test_df[col].min():.1f}, {test_df[col].max():.1f}]\"\n",
    "        print(f\"  • {col:15}: Train {train_range} | Test {test_range}\")\n",
    "\n",
    "# Verificar categorías únicas\n",
    "categorical_cols = ['Soil Type', 'Crop Type']\n",
    "print(f\"\\n📊 Categorías únicas (Train vs Test):\")\n",
    "for col in categorical_cols:\n",
    "    if col in train_df.columns and col in test_df.columns:\n",
    "        train_cats = set(train_df[col].unique())\n",
    "        test_cats = set(test_df[col].unique())\n",
    "        \n",
    "        print(f\"  • {col}:\")\n",
    "        print(f\"    Train: {len(train_cats)} categorías\")\n",
    "        print(f\"    Test: {len(test_cats)} categorías\")\n",
    "        \n",
    "        # Categorías solo en test (problemas potenciales)\n",
    "        only_in_test = test_cats - train_cats\n",
    "        if only_in_test:\n",
    "            print(f\"    ⚠️ Solo en Test: {only_in_test}\")\n",
    "        else:\n",
    "            print(f\"    ✅ Todas las categorías de test están en train\")\n",
    "\n",
    "print(f\"\\n🎯 CONCLUSIÓN ANÁLISIS DE CALIDAD:\")\n",
    "print(f\"  ✅ Datasets limpios - sin valores faltantes\")\n",
    "print(f\"  ✅ Consistencia entre train y test verificada\")\n",
    "print(f\"  ✅ Listos para feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294c22a5",
   "metadata": {},
   "source": [
    "## ⚙️ 4. Feature Engineering\n",
    "\n",
    "### Creación de Features Basadas en Insights del EDA\n",
    "\n",
    "**🎯 Implementamos las features que demostraron alto valor predictivo en el EDA:**\n",
    "\n",
    "1. **Ratios de Nutrientes**: N_P_ratio, N_K_ratio, P_K_ratio\n",
    "2. **Índices Ambientales**: Temp_Humidity_index, Environmental_Stress\n",
    "3. **Categorización de Nutrientes**: N_Level, P_Level, K_Level\n",
    "4. **Interacciones Contextuales**: Soil_Crop_Combo\n",
    "5. **Features Agregadas**: Total_NPK, Balance de humedad\n",
    "\n",
    "**📊 Resultado esperado:** 20+ features totales para modelado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af13c33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FEATURE ENGINEERING ESTRATÉGICO ===\n",
      "🔧 Aplicando feature engineering...\n",
      "\n",
      "🔧 Creando features para TRAIN...\n",
      "  1️⃣ Ratios de nutrientes...\n",
      "  2️⃣ Índices ambientales...\n",
      "  3️⃣ Categorización de nutrientes...\n",
      "  3️⃣ Categorización de nutrientes...\n",
      "  4️⃣ Interacciones contextuales...\n",
      "  4️⃣ Interacciones contextuales...\n",
      "  5️⃣ Features de balance y ratios avanzados...\n",
      "  5️⃣ Features de balance y ratios avanzados...\n",
      "  🎯 Target numérico creado para uso interno\n",
      "  ✅ Features creadas: 17\n",
      "  📊 Total columnas: 10 → 27\n",
      "\n",
      "🔧 Creando features para TEST...\n",
      "  1️⃣ Ratios de nutrientes...\n",
      "  2️⃣ Índices ambientales...\n",
      "  3️⃣ Categorización de nutrientes...\n",
      "  🎯 Target numérico creado para uso interno\n",
      "  ✅ Features creadas: 17\n",
      "  📊 Total columnas: 10 → 27\n",
      "\n",
      "🔧 Creando features para TEST...\n",
      "  1️⃣ Ratios de nutrientes...\n",
      "  2️⃣ Índices ambientales...\n",
      "  3️⃣ Categorización de nutrientes...\n",
      "  4️⃣ Interacciones contextuales...\n",
      "  4️⃣ Interacciones contextuales...\n",
      "  5️⃣ Features de balance y ratios avanzados...\n",
      "  ✅ Features creadas: 16\n",
      "  📊 Total columnas: 9 → 25\n",
      "\n",
      "📊 RESUMEN DE FEATURE ENGINEERING:\n",
      "\n",
      "🔢 Nuevas features numéricas (10):\n",
      "  • N_P_ratio                : [0.10, 420.00]\n",
      "  • N_K_ratio                : [0.21, 420.00]\n",
      "  • P_K_ratio                : [0.00, 420.00]\n",
      "  • Total_NPK                : [4.00, 103.00]\n",
      "  • Temp_Humidity_index      : [12.50, 27.36]\n",
      "  • Moisture_Balance         : [-47.00, 15.00]\n",
      "  • Environmental_Stress     : [0.00, 17.67]\n",
      "  • NPK_Balance              : [0.00, 84.00]\n",
      "  • Dominant_Nutrient_Level  : [4.00, 42.00]\n",
      "  • Temp_Moisture_interaction: [6.25, 24.70]\n",
      "\n",
      "📊 Nuevas features categóricas (6):\n",
      "  • Temp_Category       : 3 categorías únicas\n",
      "  5️⃣ Features de balance y ratios avanzados...\n",
      "  ✅ Features creadas: 16\n",
      "  📊 Total columnas: 9 → 25\n",
      "\n",
      "📊 RESUMEN DE FEATURE ENGINEERING:\n",
      "\n",
      "🔢 Nuevas features numéricas (10):\n",
      "  • N_P_ratio                : [0.10, 420.00]\n",
      "  • N_K_ratio                : [0.21, 420.00]\n",
      "  • P_K_ratio                : [0.00, 420.00]\n",
      "  • Total_NPK                : [4.00, 103.00]\n",
      "  • Temp_Humidity_index      : [12.50, 27.36]\n",
      "  • Moisture_Balance         : [-47.00, 15.00]\n",
      "  • Environmental_Stress     : [0.00, 17.67]\n",
      "  • NPK_Balance              : [0.00, 84.00]\n",
      "  • Dominant_Nutrient_Level  : [4.00, 42.00]\n",
      "  • Temp_Moisture_interaction: [6.25, 24.70]\n",
      "\n",
      "📊 Nuevas features categóricas (6):\n",
      "  • Temp_Category       : 3 categorías únicas\n",
      "  • N_Level             : 3 categorías únicas\n",
      "  • P_Level             : 3 categorías únicas\n",
      "  • K_Level             : 2 categorías únicas\n",
      "  • Soil_Crop_Combo     : 55 categorías únicas\n",
      "  • N_Level             : 3 categorías únicas\n",
      "  • P_Level             : 3 categorías únicas\n",
      "  • K_Level             : 2 categorías únicas\n",
      "  • Soil_Crop_Combo     : 55 categorías únicas\n",
      "  • NPK_Level_Combo     : 18 categorías únicas\n",
      "\n",
      "🎯 FEATURE ENGINEERING COMPLETADO:\n",
      "  • Train: (750000, 10) → (750000, 27)\n",
      "  • Test: (250000, 9) → (250000, 25)\n",
      "  • Features totales disponibles: 27\n",
      "  ✅ Listos para codificación de variables categóricas\n",
      "  • NPK_Level_Combo     : 18 categorías únicas\n",
      "\n",
      "🎯 FEATURE ENGINEERING COMPLETADO:\n",
      "  • Train: (750000, 10) → (750000, 27)\n",
      "  • Test: (250000, 9) → (250000, 25)\n",
      "  • Features totales disponibles: 27\n",
      "  ✅ Listos para codificación de variables categóricas\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering completo basado en insights del EDA\n",
    "print(\"=== FEATURE ENGINEERING ESTRATÉGICO ===\")\n",
    "\n",
    "# Función para crear todas las features\n",
    "def create_features(df, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Crear todas las features basadas en insights del EDA\n",
    "    \"\"\"\n",
    "    print(f\"\\n🔧 Creando features para {dataset_name}...\")\n",
    "    \n",
    "    # Hacer copia para no modificar original\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # 1. RATIOS DE NUTRIENTES (Alto valor predictivo según MI)\n",
    "    print(\"  1️⃣ Ratios de nutrientes...\")\n",
    "    df_features['N_P_ratio'] = df_features['Nitrogen'] / (df_features['Phosphorous'] + 0.1)  # +0.1 para evitar división por 0\n",
    "    df_features['N_K_ratio'] = df_features['Nitrogen'] / (df_features['Potassium'] + 0.1)\n",
    "    df_features['P_K_ratio'] = df_features['Phosphorous'] / (df_features['Potassium'] + 0.1)\n",
    "    df_features['Total_NPK'] = df_features['Nitrogen'] + df_features['Phosphorous'] + df_features['Potassium']\n",
    "    \n",
    "    # 2. ÍNDICES AMBIENTALES\n",
    "    print(\"  2️⃣ Índices ambientales...\")\n",
    "    df_features['Temp_Humidity_index'] = df_features['Temparature'] * df_features['Humidity'] / 100\n",
    "    df_features['Moisture_Balance'] = df_features['Moisture'] - df_features['Humidity']\n",
    "    df_features['Environmental_Stress'] = (abs(df_features['Temparature'] - 25) + \n",
    "                                         abs(df_features['Humidity'] - 65) + \n",
    "                                         abs(df_features['Moisture'] - 50)) / 3\n",
    "    \n",
    "    # 3. CATEGORIZACIÓN INTELIGENTE DE NUTRIENTES\n",
    "    print(\"  3️⃣ Categorización de nutrientes...\")\n",
    "    \n",
    "    # Temperatura (basado en análisis del EDA)\n",
    "    df_features['Temp_Category'] = pd.cut(df_features['Temparature'], \n",
    "                                         bins=[-np.inf, 15, 25, 35, np.inf], \n",
    "                                         labels=['Frío', 'Templado', 'Cálido', 'Muy_Cálido'])\n",
    "    \n",
    "    # Niveles de nutrientes (basados en cuartiles del EDA)\n",
    "    df_features['N_Level'] = pd.cut(df_features['Nitrogen'], \n",
    "                                   bins=[-np.inf, 15, 30, np.inf], \n",
    "                                   labels=['Bajo', 'Medio', 'Alto'])\n",
    "    \n",
    "    df_features['P_Level'] = pd.cut(df_features['Phosphorous'], \n",
    "                                   bins=[-np.inf, 20, 40, np.inf], \n",
    "                                   labels=['Bajo', 'Medio', 'Alto'])\n",
    "    \n",
    "    df_features['K_Level'] = pd.cut(df_features['Potassium'], \n",
    "                                   bins=[-np.inf, 10, 20, np.inf], \n",
    "                                   labels=['Bajo', 'Medio', 'Alto'])\n",
    "    \n",
    "    # 4. INTERACCIONES CONTEXTUALES (Alto MI según EDA)\n",
    "    print(\"  4️⃣ Interacciones contextuales...\")\n",
    "    df_features['Soil_Crop_Combo'] = df_features['Soil Type'] + '_' + df_features['Crop Type']\n",
    "    \n",
    "    # Combinación de niveles NPK\n",
    "    df_features['NPK_Level_Combo'] = (df_features['N_Level'].astype(str) + '_' + \n",
    "                                     df_features['P_Level'].astype(str) + '_' + \n",
    "                                     df_features['K_Level'].astype(str))\n",
    "    \n",
    "    # 5. FEATURES ADICIONALES DE BALANCE\n",
    "    print(\"  5️⃣ Features de balance y ratios avanzados...\")\n",
    "    \n",
    "    # Balance de macronutrientes\n",
    "    df_features['NPK_Balance'] = abs(df_features['Nitrogen'] - df_features['Phosphorous']) + \\\n",
    "                                abs(df_features['Nitrogen'] - df_features['Potassium']) + \\\n",
    "                                abs(df_features['Phosphorous'] - df_features['Potassium'])\n",
    "    \n",
    "    # Índice de nutriente dominante\n",
    "    max_nutrient = df_features[['Nitrogen', 'Phosphorous', 'Potassium']].max(axis=1)\n",
    "    df_features['Dominant_Nutrient_Level'] = max_nutrient\n",
    "    \n",
    "    # Condiciones ambientales combinadas\n",
    "    df_features['Temp_Moisture_interaction'] = df_features['Temparature'] * df_features['Moisture'] / 100\n",
    "    \n",
    "    # CREAR TARGET ENCODED PARA USO INTERNO (solo para train)\n",
    "    if 'Fertilizer Name' in df_features.columns:\n",
    "        # Crear target numérico para uso interno\n",
    "        target_encoder = LabelEncoder()\n",
    "        df_features['target'] = target_encoder.fit_transform(df_features['Fertilizer Name'])\n",
    "        print(\"  🎯 Target numérico creado para uso interno\")\n",
    "    \n",
    "    # Resumen de features creadas\n",
    "    original_cols = len(df.columns)\n",
    "    new_cols = len(df_features.columns)\n",
    "    features_added = new_cols - original_cols\n",
    "    \n",
    "    print(f\"  ✅ Features creadas: {features_added}\")\n",
    "    print(f\"  📊 Total columnas: {original_cols} → {new_cols}\")\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Aplicar feature engineering a ambos datasets\n",
    "print(\"🔧 Aplicando feature engineering...\")\n",
    "\n",
    "train_processed = create_features(train_df, \"TRAIN\")\n",
    "test_processed = create_features(test_df, \"TEST\")\n",
    "\n",
    "# Verificar que las features se crearon correctamente\n",
    "print(f\"\\n📊 RESUMEN DE FEATURE ENGINEERING:\")\n",
    "\n",
    "# Nuevas features numéricas\n",
    "new_numeric_features = ['N_P_ratio', 'N_K_ratio', 'P_K_ratio', 'Total_NPK',\n",
    "                       'Temp_Humidity_index', 'Moisture_Balance', 'Environmental_Stress',\n",
    "                       'NPK_Balance', 'Dominant_Nutrient_Level', 'Temp_Moisture_interaction']\n",
    "\n",
    "print(f\"\\n🔢 Nuevas features numéricas ({len(new_numeric_features)}):\")\n",
    "for feature in new_numeric_features:\n",
    "    train_range = f\"[{train_processed[feature].min():.2f}, {train_processed[feature].max():.2f}]\"\n",
    "    print(f\"  • {feature:25}: {train_range}\")\n",
    "\n",
    "# Nuevas features categóricas\n",
    "new_categorical_features = ['Temp_Category', 'N_Level', 'P_Level', 'K_Level', \n",
    "                           'Soil_Crop_Combo', 'NPK_Level_Combo']\n",
    "\n",
    "print(f\"\\n📊 Nuevas features categóricas ({len(new_categorical_features)}):\")\n",
    "for feature in new_categorical_features:\n",
    "    unique_count = train_processed[feature].nunique()\n",
    "    print(f\"  • {feature:20}: {unique_count} categorías únicas\")\n",
    "\n",
    "print(f\"\\n🎯 FEATURE ENGINEERING COMPLETADO:\")\n",
    "print(f\"  • Train: {train_df.shape} → {train_processed.shape}\")\n",
    "print(f\"  • Test: {test_df.shape} → {test_processed.shape}\")\n",
    "print(f\"  • Features totales disponibles: {len(train_processed.columns)}\")\n",
    "print(f\"  ✅ Listos para codificación de variables categóricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e275ac90",
   "metadata": {},
   "source": [
    "## 🔢 5. Preparación de Datos por Tipo de Modelo\n",
    "\n",
    "### Estrategia Multi-Modelo: Nativas vs Codificadas\n",
    "\n",
    "**🎯 Creamos DOS versiones optimizadas:**\n",
    "\n",
    "1. **📊 Versión NATIVE** (Random Forest, LightGBM, CatBoost):\n",
    "   - Variables categóricas como strings/objects\n",
    "   - Mejor manejo de alta cardinalidad\n",
    "   - Splits más inteligentes\n",
    "\n",
    "2. **🔢 Versión ENCODED** (XGBoost):\n",
    "   - LabelEncoder aplicado a categóricas\n",
    "   - Solo datos numéricos\n",
    "   - Requerido por XGBoost\n",
    "\n",
    "**💡 Variables categóricas identificadas:**\n",
    "- Originales: Soil Type, Crop Type\n",
    "- Nuevas: Temp_Category, N_Level, P_Level, K_Level, Soil_Crop_Combo, NPK_Level_Combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "307170b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARACIÓN MULTI-MODELO ===\n",
      "📊 Variables categóricas identificadas: 8\n",
      "  • Soil Type: 5 categorías\n",
      "  • Soil Type: 5 categorías\n",
      "  • Crop Type: 11 categorías\n",
      "  • Temp_Category: 3 categorías\n",
      "  • N_Level: 3 categorías\n",
      "  • P_Level: 3 categorías\n",
      "  • K_Level: 2 categorías\n",
      "  • Crop Type: 11 categorías\n",
      "  • Temp_Category: 3 categorías\n",
      "  • N_Level: 3 categorías\n",
      "  • P_Level: 3 categorías\n",
      "  • K_Level: 2 categorías\n",
      "  • Soil_Crop_Combo: 55 categorías\n",
      "  • NPK_Level_Combo: 18 categorías\n",
      "\n",
      "1️⃣ CREANDO VERSIÓN NATIVE (categóricas como strings):\n",
      "  • Soil_Crop_Combo: 55 categorías\n",
      "  • NPK_Level_Combo: 18 categorías\n",
      "\n",
      "1️⃣ CREANDO VERSIÓN NATIVE (categóricas como strings):\n",
      "  ✅ Variables categóricas mantenidas como strings/objects\n",
      "  ✅ Target mantenido como 'Fertilizer Name': 7 clases\n",
      "  🎯 Optimizado para: Random Forest, LightGBM, CatBoost\n",
      "\n",
      "2️⃣ CREANDO VERSIÓN ENCODED (todo numérico):\n",
      "  ✅ Variables categóricas mantenidas como strings/objects\n",
      "  ✅ Target mantenido como 'Fertilizer Name': 7 clases\n",
      "  🎯 Optimizado para: Random Forest, LightGBM, CatBoost\n",
      "\n",
      "2️⃣ CREANDO VERSIÓN ENCODED (todo numérico):\n",
      "  🔧 Aplicando LabelEncoder a todas las categóricas...\n",
      "  🔧 Aplicando LabelEncoder a todas las categóricas...\n",
      "    📊 Soil Type: 5 categorías → [0, 4]\n",
      "    📊 Soil Type: 5 categorías → [0, 4]\n",
      "    📊 Crop Type: 11 categorías → [0, 10]\n",
      "    📊 Crop Type: 11 categorías → [0, 10]\n",
      "    📊 Temp_Category: 3 categorías → [0, 2]\n",
      "    📊 N_Level: 3 categorías → [0, 2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m train_encoded, test_encoded, encoders\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Aplicar codificación completa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m train_encoded, test_encoded, label_encoders = \u001b[43mencode_categorical_variables\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexisting_categorical\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ✅ Todas las variables categóricas codificadas\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     79\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ✅ Dataset completamente numérico\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 57\u001b[39m, in \u001b[36mencode_categorical_variables\u001b[39m\u001b[34m(train_df, test_df, categorical_cols, target_col)\u001b[39m\n\u001b[32m     54\u001b[39m le.fit(all_categories)\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Transform train y test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m train_encoded[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_encoded\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m test_df.columns:\n\u001b[32m     59\u001b[39m     test_encoded[\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_encoded\u001b[39m\u001b[33m'\u001b[39m] = le.transform(test_df[col].astype(\u001b[38;5;28mstr\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Felix\\miniconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:134\u001b[39m, in \u001b[36mLabelEncoder.transform\u001b[39m\u001b[34m(self, y)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) == \u001b[32m0\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([])\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Felix\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:235\u001b[39m, in \u001b[36m_encode\u001b[39m\u001b[34m(values, uniques, check_unknown)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp.isdtype(values.dtype, \u001b[33m\"\u001b[39m\u001b[33mnumeric\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_map_to_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muniques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    237\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Felix\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_encode.py:174\u001b[39m, in \u001b[36m_map_to_integer\u001b[39m\u001b[34m(values, uniques)\u001b[39m\n\u001b[32m    172\u001b[39m xp, _ = get_namespace(values, uniques)\n\u001b[32m    173\u001b[39m table = _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values], device=device(values))\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Preparación de datos para diferentes tipos de modelos\n",
    "print(\"=== PREPARACIÓN MULTI-MODELO ===\")\n",
    "\n",
    "# Identificar todas las variables categóricas\n",
    "categorical_columns = ['Soil Type', 'Crop Type', 'Temp_Category', 'N_Level', \n",
    "                      'P_Level', 'K_Level', 'Soil_Crop_Combo', 'NPK_Level_Combo']\n",
    "\n",
    "# Verificar cuáles existen en los datos\n",
    "existing_categorical = [col for col in categorical_columns if col in train_processed.columns]\n",
    "print(f\"📊 Variables categóricas identificadas: {len(existing_categorical)}\")\n",
    "for col in existing_categorical:\n",
    "    print(f\"  • {col}: {train_processed[col].nunique()} categorías\")\n",
    "\n",
    "# ========================================\n",
    "# VERSIÓN 1: DATOS NATIVOS (RF, LightGBM, CatBoost)\n",
    "# ========================================\n",
    "print(f\"\\n1️⃣ CREANDO VERSIÓN NATIVE (categóricas como strings):\")\n",
    "\n",
    "# Preparar versión nativa (mantener Fertilizer Name como está)\n",
    "train_native = train_processed.copy()\n",
    "test_native = test_processed.copy()\n",
    "\n",
    "print(f\"  ✅ Variables categóricas mantenidas como strings/objects\")\n",
    "print(f\"  ✅ Target mantenido como 'Fertilizer Name': {train_native['Fertilizer Name'].nunique()} clases\")\n",
    "print(f\"  🎯 Optimizado para: Random Forest, LightGBM, CatBoost\")\n",
    "\n",
    "# ========================================\n",
    "# VERSIÓN 2: DATOS CODIFICADOS (XGBoost)\n",
    "# ========================================\n",
    "print(f\"\\n2️⃣ CREANDO VERSIÓN ENCODED (todo numérico):\")\n",
    "\n",
    "# Función para codificar variables categóricas\n",
    "def encode_categorical_variables(train_df, test_df, categorical_cols, target_col=None):\n",
    "    \"\"\"\n",
    "    Codifica TODAS las variables categóricas para XGBoost\n",
    "    \"\"\"\n",
    "    train_encoded = train_df.copy()\n",
    "    test_encoded = test_df.copy()\n",
    "    encoders = {}\n",
    "    \n",
    "    print(f\"  🔧 Aplicando LabelEncoder a todas las categóricas...\")\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if col in train_df.columns:\n",
    "            # Crear LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            \n",
    "            # Fit en todas las categorías (train + test) para consistencia\n",
    "            all_categories = pd.concat([\n",
    "                train_df[col].astype(str), \n",
    "                test_df[col].astype(str) if col in test_df.columns else pd.Series([])\n",
    "            ]).unique()\n",
    "            \n",
    "            le.fit(all_categories)\n",
    "            \n",
    "            # Transform train y test\n",
    "            train_encoded[f'{col}_encoded'] = le.transform(train_df[col].astype(str))\n",
    "            if col in test_df.columns:\n",
    "                test_encoded[f'{col}_encoded'] = le.transform(test_df[col].astype(str))\n",
    "            \n",
    "            # Guardar encoder\n",
    "            encoders[col] = le\n",
    "            \n",
    "            print(f\"    📊 {col}: {len(all_categories)} categorías → [0, {len(all_categories)-1}]\")\n",
    "    \n",
    "    # Ya tenemos target codificado en la columna 'target'\n",
    "    if target_col and target_col in train_df.columns and 'target' in train_df.columns:\n",
    "        print(f\"    🎯 Target ya disponible como numérico en columna 'target'\")\n",
    "    \n",
    "    return train_encoded, test_encoded, encoders\n",
    "\n",
    "# Aplicar codificación completa\n",
    "train_encoded, test_encoded, label_encoders = encode_categorical_variables(\n",
    "    train_processed, test_processed, \n",
    "    existing_categorical\n",
    ")\n",
    "\n",
    "print(f\"  ✅ Todas las variables categóricas codificadas\")\n",
    "print(f\"  ✅ Dataset completamente numérico\")\n",
    "print(f\"  🎯 Optimizado para: XGBoost\")\n",
    "\n",
    "# ========================================\n",
    "# RESUMEN DE VERSIONES CREADAS\n",
    "# ========================================\n",
    "print(f\"\\n📊 RESUMEN DE VERSIONES CREADAS:\")\n",
    "print(f\"\\n🌳 VERSIÓN NATIVE:\")\n",
    "print(f\"  • train_native: {train_native.shape}\")\n",
    "print(f\"  • test_native: {test_native.shape}\")\n",
    "print(f\"  • Variables categóricas: {len(existing_categorical)} como strings\")\n",
    "print(f\"  • Target: 'Fertilizer Name' ({train_native['Fertilizer Name'].nunique()} clases)\")\n",
    "print(f\"  • Ideal para: Random Forest, LightGBM, CatBoost\")\n",
    "\n",
    "print(f\"\\n🔢 VERSIÓN ENCODED:\")\n",
    "print(f\"  • train_encoded: {train_encoded.shape}\")\n",
    "print(f\"  • test_encoded: {test_encoded.shape}\")\n",
    "print(f\"  • Variables categóricas: {len(existing_categorical)} codificadas\")\n",
    "print(f\"  • Target: 'target' ({train_encoded['target'].nunique()} clases)\")\n",
    "print(f\"  • Ideal para: XGBoost\")\n",
    "\n",
    "print(f\"\\n💡 RECOMENDACIONES:\")\n",
    "print(f\"  🥇 Random Forest BASELINE: Usar train_native (mejor rendimiento)\")\n",
    "print(f\"  🚀 XGBoost: Usar train_encoded (requerimiento obligatorio)\")\n",
    "print(f\"  ⚡ LightGBM: Usar train_native + categorical_feature parameter\")\n",
    "print(f\"  🎯 CatBoost: Usar train_native + cat_features parameter\")\n",
    "\n",
    "# Verificar mapeos importantes\n",
    "print(f\"\\n🔍 MAPEOS DE ENCODERS (ejemplos):\")\n",
    "\n",
    "# Mostrar mapeo de Soil Type\n",
    "if 'Soil Type' in label_encoders:\n",
    "    soil_mapping = dict(zip(label_encoders['Soil Type'].classes_, \n",
    "                           label_encoders['Soil Type'].transform(label_encoders['Soil Type'].classes_)))\n",
    "    print(f\"  🌱 Soil Type:\")\n",
    "    for soil, code in list(soil_mapping.items())[:3]:\n",
    "        print(f\"    {code}: {soil}\")\n",
    "    if len(soil_mapping) > 3:\n",
    "        print(f\"    ... y {len(soil_mapping)-3} más\")\n",
    "\n",
    "# Mostrar mapeo de Crop Type (primeras 5)\n",
    "if 'Crop Type' in label_encoders:\n",
    "    crop_mapping = dict(zip(label_encoders['Crop Type'].classes_, \n",
    "                           label_encoders['Crop Type'].transform(label_encoders['Crop Type'].classes_)))\n",
    "    print(f\"  🌾 Crop Type (primeros 5):\")\n",
    "    for i, (crop, code) in enumerate(crop_mapping.items()):\n",
    "        if i < 5:\n",
    "            print(f\"    {code}: {crop}\")\n",
    "    if len(crop_mapping) > 5:\n",
    "        print(f\"    ... y {len(crop_mapping)-5} más\")\n",
    "\n",
    "# Estadísticas finales\n",
    "print(f\"\\n📈 ESTADÍSTICAS POST-CODIFICACIÓN:\")\n",
    "print(f\"  • Train shape: {train_encoded.shape}\")\n",
    "print(f\"  • Test shape: {test_encoded.shape}\")\n",
    "print(f\"  • Columnas numéricas totales: {len(train_encoded.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"  • Columnas categóricas originales mantenidas: {len(train_encoded.select_dtypes(include=['object', 'category']).columns)}\")\n",
    "\n",
    "print(f\"\\n✅ CODIFICACIÓN COMPLETADA\")\n",
    "print(f\"  🎯 Datasets listos para escalado de variables numéricas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ed2a1",
   "metadata": {},
   "source": [
    "## 🔄 6. División de Datos para Modelado\n",
    "\n",
    "### Splitting Estratificado para Ambas Versiones\n",
    "\n",
    "**🎯 Estrategia de División:**\n",
    "\n",
    "- **Split ratio**: 80/20 (entrenamiento/validación)\n",
    "- **Estratificación**: Por 22 fertilizantes\n",
    "- **Dos versiones**: Native (RF/LightGBM/CatBoost) y Encoded (XGBoost)\n",
    "- **Sin escalado**: Tree-based models no requieren normalización\n",
    "\n",
    "**📊 Datasets de salida:**\n",
    "- train_native / val_native: Para Random Forest, LightGBM, CatBoost\n",
    "- train_encoded / val_encoded: Para XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545ccfed",
   "metadata": {},
   "source": [
    "## 💾 7. Guardado de Datasets Preprocesados\n",
    "\n",
    "### Exportación de Datos Procesados\n",
    "\n",
    "**🎯 Estructura de Archivos de Salida:**\n",
    "\n",
    "1. **Datasets de entrenamiento/validación**:\n",
    "   - `train_native.csv` - Para Random Forest, LightGBM, CatBoost\n",
    "   - `val_native.csv` - Validación versión nativa\n",
    "   - `train_encoded.csv` - Para XGBoost\n",
    "   - `val_encoded.csv` - Validación versión codificada\n",
    "\n",
    "2. **Dataset de prueba procesado**:\n",
    "   - `test_native.csv` - Test versión nativa\n",
    "   - `test_encoded.csv` - Test versión codificada\n",
    "\n",
    "3. **Metadatos y información**:\n",
    "   - `feature_info.json` - Información de features por tipo\n",
    "   - `preprocessing_info.json` - Metadatos del preprocesamiento\n",
    "\n",
    "**📁 Directorio de salida:** `../data/processed/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# División de datos para ambas versiones (Native y Encoded)\n",
    "print(\"=== DIVISIÓN DE DATOS PARA AMBAS VERSIONES ===\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Función para crear split estratificado\n",
    "def create_train_val_split(X, y, test_size=0.2, random_state=513):\n",
    "    \"\"\"\n",
    "    Crea split estratificado manteniendo proporción de clases\n",
    "    \"\"\"\n",
    "    return train_test_split(X, y, test_size=test_size, \n",
    "                           stratify=y, random_state=random_state)\n",
    "\n",
    "# Target variable (usar la columna target numérica)\n",
    "y = train_native['target']  # Target ya codificado\n",
    "\n",
    "print(f\"📊 Información del target:\")\n",
    "print(f\"  • Total muestras: {len(y):,}\")\n",
    "print(f\"  • Clases únicas: {y.nunique()}\")\n",
    "print(f\"  • Rango de clases: 0 a {y.max()}\")\n",
    "\n",
    "# ========================================\n",
    "# 1. DIVISIÓN PARA VERSIÓN NATIVE (RF, LightGBM, CatBoost)\n",
    "# ========================================\n",
    "print(f\"\\n1️⃣ División para NATIVE version:\")\n",
    "\n",
    "# Features para versión native (excluir columnas target y auxiliares)\n",
    "exclude_cols_native = ['Fertilizer Name', 'target', 'id'] if 'id' in train_native.columns else ['Fertilizer Name', 'target']\n",
    "features_native = [col for col in train_native.columns if col not in exclude_cols_native]\n",
    "X_native = train_native[features_native]\n",
    "y_native = y\n",
    "\n",
    "# Split estratificado para native\n",
    "X_train_native, X_val_native, y_train, y_val = create_train_val_split(X_native, y_native)\n",
    "\n",
    "print(f\"  📊 X_train_native: {X_train_native.shape}\")\n",
    "print(f\"  📊 X_val_native: {X_val_native.shape}\")\n",
    "print(f\"  📊 y_train: {len(y_train)}\")\n",
    "print(f\"  📊 y_val: {len(y_val)}\")\n",
    "\n",
    "# Preparar test native - mantener ID si existe\n",
    "if 'id' in test_native.columns:\n",
    "    # Conservar ID para las predicciones\n",
    "    test_ids = test_native['id']\n",
    "    features_test_native = [col for col in features_native if col in test_native.columns]\n",
    "    X_test_native = test_native[features_test_native]\n",
    "else:\n",
    "    # Si no hay columna id, crear una\n",
    "    test_ids = pd.Series(range(len(test_native)), name='id')\n",
    "    features_test_native = [col for col in features_native if col in test_native.columns]\n",
    "    X_test_native = test_native[features_test_native]\n",
    "\n",
    "print(f\"  📊 X_test_native: {X_test_native.shape}\")\n",
    "\n",
    "# ========================================\n",
    "# 2. DIVISIÓN PARA VERSIÓN ENCODED (XGBoost)\n",
    "# ========================================\n",
    "print(f\"\\n2️⃣ División para ENCODED version:\")\n",
    "\n",
    "# Features para versión encoded (excluir columnas target y auxiliares)\n",
    "exclude_cols_encoded = ['Fertilizer Name', 'target', 'id'] if 'id' in train_encoded.columns else ['Fertilizer Name', 'target']\n",
    "# También excluir las columnas categóricas originales (usar solo las _encoded)\n",
    "original_categoricals = ['Soil Type', 'Crop Type', 'Temp_Category', 'N_Level', 'P_Level', 'K_Level', 'Soil_Crop_Combo', 'NPK_Level_Combo']\n",
    "exclude_cols_encoded.extend(original_categoricals)\n",
    "\n",
    "features_encoded = [col for col in train_encoded.columns if col not in exclude_cols_encoded]\n",
    "X_encoded = train_encoded[features_encoded]\n",
    "y_encoded = train_encoded['target']  # Usar la columna target\n",
    "\n",
    "# Split estratificado para encoded (usando mismos índices que native)\n",
    "X_train_encoded = X_encoded.iloc[X_train_native.index]\n",
    "X_val_encoded = X_encoded.iloc[X_val_native.index]\n",
    "\n",
    "print(f\"  📊 X_train_encoded: {X_train_encoded.shape}\")\n",
    "print(f\"  📊 X_val_encoded: {X_val_encoded.shape}\")\n",
    "\n",
    "# Preparar test encoded\n",
    "features_test_encoded = [col for col in features_encoded if col in test_encoded.columns]\n",
    "X_test_encoded = test_encoded[features_test_encoded]\n",
    "print(f\"  📊 X_test_encoded: {X_test_encoded.shape}\")\n",
    "\n",
    "# ========================================\n",
    "# 3. VERIFICACIÓN DE ESTRATIFICACIÓN\n",
    "# ========================================\n",
    "print(f\"\\n📊 VERIFICACIÓN DE ESTRATIFICACIÓN:\")\n",
    "print(f\"  Original: {y.value_counts().head(5).values}\")\n",
    "print(f\"  Train:    {y_train.value_counts().head(5).values}\")\n",
    "print(f\"  Val:      {y_val.value_counts().head(5).values}\")\n",
    "\n",
    "# Calcular proporciones\n",
    "train_prop = len(y_train) / len(y)\n",
    "val_prop = len(y_val) / len(y)\n",
    "print(f\"\\n  Proporciones: Train {train_prop:.1%}, Val {val_prop:.1%}\")\n",
    "\n",
    "# ========================================\n",
    "# 4. RESUMEN DE DATASETS PREPARADOS\n",
    "# ========================================\n",
    "print(f\"\\n🎯 RESUMEN DE DATASETS PREPARADOS:\")\n",
    "print(f\"  • X_train_native: {X_train_native.shape} - Para Random Forest, LightGBM, CatBoost\")\n",
    "print(f\"  • X_val_native: {X_val_native.shape}\")\n",
    "print(f\"  • X_test_native: {X_test_native.shape}\")\n",
    "print(f\"  • X_train_encoded: {X_train_encoded.shape} - Para XGBoost\")\n",
    "print(f\"  • X_val_encoded: {X_val_encoded.shape}\")\n",
    "print(f\"  • X_test_encoded: {X_test_encoded.shape}\")\n",
    "print(f\"  • y_train, y_val: {len(y_train)}, {len(y_val)} muestras\")\n",
    "print(f\"  • test_ids: {len(test_ids)} IDs guardados\")\n",
    "\n",
    "print(f\"\\n📊 FEATURES DISPONIBLES:\")\n",
    "print(f\"  • Native features: {len(features_native)}\")\n",
    "print(f\"  • Encoded features: {len(features_encoded)}\")\n",
    "print(f\"  • Categóricas nativas: {len([f for f in features_native if f in original_categoricals])}\")\n",
    "print(f\"  • Categóricas codificadas: {len([f for f in features_encoded if '_encoded' in f])}\")\n",
    "\n",
    "print(f\"\\n✅ DIVISIÓN DE DATOS COMPLETADA\")\n",
    "print(f\"  🎯 Dos versiones de datos preparadas para diferentes modelos\")\n",
    "print(f\"  🎯 Estratificación mantenida para ambas versiones\")\n",
    "print(f\"  🎯 Listos para entrenamiento y exportación\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardado de datasets preprocesados para ambas versiones\n",
    "print(\"=== GUARDADO DE DATASETS PREPROCESADOS ===\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Crear directorio de salida\n",
    "output_dir = '../data/processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"📁 Directorio de salida: {output_dir}\")\n",
    "\n",
    "# Función para guardar datasets con información adicional\n",
    "def save_processed_data(data, filename, description):\n",
    "    \"\"\"\n",
    "    Guarda un dataset con metadata\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    data.to_csv(filepath, index=False)\n",
    "    print(f\"  ✅ {filename}: {data.shape} - {description}\")\n",
    "    return filepath\n",
    "\n",
    "print(f\"\\n💾 Guardando datasets procesados...\")\n",
    "\n",
    "# ========================================\n",
    "# 1. DATASETS VERSIÓN NATIVE (RF, LightGBM, CatBoost)\n",
    "# ========================================\n",
    "print(f\"\\n1️⃣ Datasets NATIVE (Random Forest, LightGBM, CatBoost):\")\n",
    "\n",
    "# Combinar features y target para entrenamiento native\n",
    "train_data_native = X_train_native.copy()\n",
    "train_data_native['Fertilizer Name'] = train_native.loc[X_train_native.index, 'Fertilizer Name']\n",
    "train_data_native['target'] = y_train\n",
    "\n",
    "val_data_native = X_val_native.copy()\n",
    "val_data_native['Fertilizer Name'] = train_native.loc[X_val_native.index, 'Fertilizer Name']\n",
    "val_data_native['target'] = y_val\n",
    "\n",
    "# Test native con IDs\n",
    "test_data_native = X_test_native.copy()\n",
    "test_data_native['id'] = test_ids\n",
    "\n",
    "# Guardar datasets native\n",
    "save_processed_data(train_data_native, 'train_native.csv', \n",
    "                   'Training data NATIVE - categóricas como strings (RF, LightGBM, CatBoost)')\n",
    "save_processed_data(val_data_native, 'val_native.csv', \n",
    "                   'Validation data NATIVE')\n",
    "save_processed_data(test_data_native, 'test_native.csv', \n",
    "                   'Test data NATIVE')\n",
    "\n",
    "# ========================================\n",
    "# 2. DATASETS VERSIÓN ENCODED (XGBoost)\n",
    "# ========================================\n",
    "print(f\"\\n2️⃣ Datasets ENCODED (XGBoost):\")\n",
    "\n",
    "# Combinar features y target para entrenamiento encoded\n",
    "train_data_encoded = X_train_encoded.copy()\n",
    "train_data_encoded['target'] = y_train\n",
    "\n",
    "val_data_encoded = X_val_encoded.copy()\n",
    "val_data_encoded['target'] = y_val\n",
    "\n",
    "# Test encoded con IDs\n",
    "test_data_encoded = X_test_encoded.copy()\n",
    "test_data_encoded['id'] = test_ids\n",
    "\n",
    "# Guardar datasets encoded\n",
    "save_processed_data(train_data_encoded, 'train_encoded.csv', \n",
    "                   'Training data ENCODED - categóricas codificadas (XGBoost)')\n",
    "save_processed_data(val_data_encoded, 'val_encoded.csv', \n",
    "                   'Validation data ENCODED')\n",
    "save_processed_data(test_data_encoded, 'test_encoded.csv', \n",
    "                   'Test data ENCODED')\n",
    "\n",
    "# ========================================\n",
    "# 3. METADATA Y INFORMACIÓN DE FEATURES\n",
    "# ========================================\n",
    "print(f\"\\n3️⃣ Guardando metadata:\")\n",
    "\n",
    "# Obtener nombres de fertilizantes únicos para el mapeo\n",
    "fertilizer_names = sorted(train_native['Fertilizer Name'].unique())\n",
    "fertilizer_mapping = {i: name for i, name in enumerate(fertilizer_names)}\n",
    "\n",
    "# Información de features\n",
    "feature_info = {\n",
    "    'features_native': {\n",
    "        'names': features_native,\n",
    "        'count': len(features_native),\n",
    "        'categorical': [col for col in features_native if col in ['Soil Type', 'Crop Type', 'Temp_Category', 'N_Level', 'P_Level', 'K_Level', 'Soil_Crop_Combo', 'NPK_Level_Combo']],\n",
    "        'numerical': [col for col in features_native if col not in ['Soil Type', 'Crop Type', 'Temp_Category', 'N_Level', 'P_Level', 'K_Level', 'Soil_Crop_Combo', 'NPK_Level_Combo']]\n",
    "    },\n",
    "    'features_encoded': {\n",
    "        'names': features_encoded,\n",
    "        'count': len(features_encoded),\n",
    "        'categorical_encoded': [col for col in features_encoded if '_encoded' in col],\n",
    "        'numerical': [col for col in features_encoded if '_encoded' not in col]\n",
    "    },\n",
    "    'target_info': {\n",
    "        'name': 'Fertilizer Name',\n",
    "        'classes': len(fertilizer_names),\n",
    "        'class_names': fertilizer_names,\n",
    "        'target_mapping': fertilizer_mapping\n",
    "    },\n",
    "    'split_info': {\n",
    "        'train_size': len(y_train),\n",
    "        'val_size': len(y_val),\n",
    "        'test_size': len(X_test_native),\n",
    "        'train_ratio': len(y_train) / len(y),\n",
    "        'val_ratio': len(y_val) / len(y)\n",
    "    },\n",
    "    'feature_counts': {\n",
    "        'native_total': len(features_native),\n",
    "        'encoded_total': len(features_encoded),\n",
    "        'categorical_native': len([f for f in features_native if f in ['Soil Type', 'Crop Type', 'Temp_Category', 'N_Level', 'P_Level', 'K_Level', 'Soil_Crop_Combo', 'NPK_Level_Combo']]),\n",
    "        'categorical_encoded': len([f for f in features_encoded if '_encoded' in f])\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar información de features\n",
    "with open(os.path.join(output_dir, 'feature_info.json'), 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "print(f\"  ✅ feature_info.json: Información detallada de features\")\n",
    "\n",
    "# Información de preprocesamiento\n",
    "preprocessing_info = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'original_data_shape': {\n",
    "        'train': list(train_df.shape),\n",
    "        'test': list(test_df.shape)\n",
    "    },\n",
    "    'processed_data_shape': {\n",
    "        'train_native': list(train_data_native.shape),\n",
    "        'val_native': list(val_data_native.shape),\n",
    "        'test_native': list(test_data_native.shape),\n",
    "        'train_encoded': list(train_data_encoded.shape),\n",
    "        'val_encoded': list(val_data_encoded.shape),\n",
    "        'test_encoded': list(test_data_encoded.shape)\n",
    "    },\n",
    "    'feature_engineering': {\n",
    "        'ratio_features': ['N_P_ratio', 'N_K_ratio', 'P_K_ratio'],\n",
    "        'categorical_features': ['N_Level', 'P_Level', 'K_Level', 'Temp_Category'],\n",
    "        'interaction_features': ['Soil_Crop_Combo', 'NPK_Level_Combo'],\n",
    "        'aggregate_features': ['Total_NPK', 'Environmental_Stress']\n",
    "    },\n",
    "    'encoding_strategy': {\n",
    "        'native_version': 'Categorical variables as strings for RF, LightGBM, CatBoost',\n",
    "        'encoded_version': 'LabelEncoder applied for XGBoost compatibility'\n",
    "    },\n",
    "    'files_created': {\n",
    "        'native': ['train_native.csv', 'val_native.csv', 'test_native.csv'],\n",
    "        'encoded': ['train_encoded.csv', 'val_encoded.csv', 'test_encoded.csv'],\n",
    "        'metadata': ['feature_info.json', 'preprocessing_info.json']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar información de preprocesamiento\n",
    "with open(os.path.join(output_dir, 'preprocessing_info.json'), 'w') as f:\n",
    "    json.dump(preprocessing_info, f, indent=2)\n",
    "print(f\"  ✅ preprocessing_info.json: Metadata completa del preprocesamiento\")\n",
    "\n",
    "# ========================================\n",
    "# 4. RESUMEN FINAL\n",
    "# ========================================\n",
    "print(f\"\\n🎆 RESUMEN FINAL DEL PREPROCESAMIENTO:\")\n",
    "print(f\"  📁 Archivos guardados en: {output_dir}\")\n",
    "print(f\"  📊 Total de archivos generados: 8 datasets + 2 metadata\")\n",
    "print(f\"  🎯 Datasets NATIVE listos para: Random Forest, LightGBM, CatBoost\")\n",
    "print(f\"  🎯 Datasets ENCODED listos para: XGBoost\")\n",
    "print(f\"  🔄 Features engineered: {len(features_native)} total\")\n",
    "print(f\"  🎯 Target classes: {len(fertilizer_names)} fertilizantes\")\n",
    "\n",
    "print(f\"\\n📊 ESTADÍSTICAS FINALES:\")\n",
    "print(f\"  • Train: {len(y_train):,} muestras\")\n",
    "print(f\"  • Validation: {len(y_val):,} muestras\")\n",
    "print(f\"  • Test: {len(test_ids):,} muestras\")\n",
    "print(f\"  • Features NATIVE: {len(features_native)}\")\n",
    "print(f\"  • Features ENCODED: {len(features_encoded)}\")\n",
    "\n",
    "print(f\"\\n✅ ¡PREPROCESAMIENTO COMPLETADO CON ÉXITO!\")\n",
    "print(f\"  🚀 Listos para entrenar modelos baseline\")\n",
    "print(f\"  🎯 Siguiente paso: Crear notebooks de modelado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2babc52",
   "metadata": {},
   "source": [
    "## 🤔 Estrategia de Encoding por Modelo\n",
    "\n",
    "### 📊 **¿Codificar Variables Categóricas para Tree-Based Models?**\n",
    "\n",
    "**La respuesta depende del modelo específico:**\n",
    "\n",
    "| Modelo | Encoding Necesario | Estrategia Óptima |\n",
    "|--------|-------------------|-------------------|\n",
    "| **Random Forest** | ❌ NO | Mantener categóricas como strings |\n",
    "| **XGBoost** | ✅ SÍ | LabelEncoder obligatorio |\n",
    "| **LightGBM** | 🔄 OPCIONAL | Categóricas nativas (mejor) |\n",
    "| **CatBoost** | ❌ NO | Categóricas nativas con cat_features |\n",
    "\n",
    "### 🎯 **Nuestra Estrategia Multi-Modelo:**\n",
    "\n",
    "1. **Versión Native**: Variables categóricas como strings (RF, LightGBM, CatBoost)\n",
    "2. **Versión Encoded**: LabelEncoder aplicado (XGBoost)\n",
    "3. **Features numéricas**: Sin escalar para todos los tree-based models\n",
    "\n",
    "**💡 Ventajas de categóricas nativas:**\n",
    "- Mejor manejo de alta cardinalidad (Soil_Crop_Combo: 110+ categorías)\n",
    "- Splits más inteligentes\n",
    "- Mejor interpretabilidad\n",
    "- Menos overfitting en high cardinality features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
